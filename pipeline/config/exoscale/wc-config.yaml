## Common kubernetes configuration options

## Define resources requests and limits for single Pods.
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## resources: {}

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
## nodeSelector: {}

## Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## tolerations: []

## Affinity for pod assignment
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

## Some common options used in various helm charts.
##
global:
  ## Compliantkubernetes-apps version.
  ## 'any', can be used when running on non-official release.
  ck8sVersion: any
  ## The cluster name.
  ## Used in logs and metrics as to separate these from other clusters.
  clusterName: pipeline-exoscale-wc
  ## Domain intended for ingress usage in the workload cluster
  ## and to reach user facing services such as kibana, grafana, and harbor.
  ## E.g. with 'prod.domain.com', kibana is reached via 'kibana.prod.domain.com'.
  baseDomain: pipeline-exoscale.elastisys.se
  ## Domain intended for ingress usage in the service cluster and to reach
  ## non-user facing services such as influxdb and elasticsearch.
  ## E.g. with 'ops.prod.domain.com', elasticsearch is reached via 'elastic.ops.prod.domain.com'.
  opsDomain: ops.pipeline-exoscale.elastisys.se
  ## Default cert-manager issuer to use for issuing certificates for ingresses.
  ## Normally one of 'letsencrypt-staging' or 'letsencrypt-prod'.
  issuer: letsencrypt-staging
  ## Verify ingress certificates
  verifyTls: false
  ## IP of the cluster DNS in kubernetes
  clusterDns: 10.233.0.3
## Configuration of storageclasses.
storageClasses:
  ## Name of the 'default' storageclass in kubernetes.
  ## Normally one of 'nfs-client', 'cinder-storage', 'local-storage', or 'ebs-gp2'.
  default: rook-ceph-block
  ## These enabled flags are 'null' but after first init
  ## they will be set to eihter 'true' or 'false'.
  ## On consecutive inits they will not be touched (unless they set to 'null').

  ## Enable deployment of nfsClientProvisioner
  ## Note, 'nfs-client' is installed as a deafult storagecalss only if 'storageClasses.default: nfs-client'.
  nfs:
    enabled: false # true | false
  ## Enableds installation of 'cinder-storage'  as a defaultstorageclass.
  cinder:
    enabled: false # true | false
  ## Enables deployment of local-volume-provisioner and installation
  ## of 'local-storage' storageclass.
  local:
    enabled: false # true | false
  ## Enables installation of 'ebs-gp2' as a default storageclass.
  ebs:
    enabled: false # true | false
## Nfs-client-provisioner configuration.
## Deployment is controlled via 'storageClasses.nfs.enabled'.
nfsProvisioner:
  server: not-used
  path: /nfs
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  tolerations: []
  nodeSelector: {}
  affinity: {}
## Object storage configuration for backups.
objectStorage:
  ## Options are 's3', 'gcs', or 'none'
  ## If 'none', remember to disable backups (velero)
  type: s3
  # gcs:
  #   project: "set-me"
  # s3:
  #   region: "set-me"
  #   regionEndpoint: "set-me"

  ## Buckets where each respctive application will store its backups.
  buckets:
    velero: pipeline-exoscale-velero
    harbor: pipeline-exoscale-harbor
    elasticsearch: pipeline-exoscale-es-backup
    influxDB: pipeline-exoscale-influxdb
    scFluentd: pipeline-exoscale-sc-logs
  s3:
    region: ch-gva-2
    regionEndpoint: https://sos-ch-gva-2.exo.io
    forcePathStyle: true
## User configuration.
user:
  ## This only controls if the namespaces should be created, user RBAC is always created.
  createNamespaces: true
  ## List of user namespaces to create.
  namespaces:
    - demo1
    - demo2
    - demo3
  ## List of users to create RBAC rules for.
  adminUsers:
    - admin@example.com

  ## List of groups to create RBAC rules for.
  adminGroups: []

  ## User controlled alertmanager configuration.
  alertmanager:
    enabled: false
    ## Namespace in which to install alertmanager
    namespace: monitoring
    ## Create basic-auth protected ingress to alertmanager
    ingress:
      enabled: false
## Falco configuration.
falco:
  enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 300Mi
    requests:
      cpu: 100m
      memory: 100Mi
  ## Run on master nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
  affinity: {}
  nodeSelector: {}
  ## Falco alerting configuration.
  alerts:
    enabled: true
    ## supported: 'alertmanager', 'slack'.
    type: alertmanager
    priority: notice
    hostPort: http://kube-prometheus-stack-alertmanager.monitoring:9093
  falcoSidekick:
    resources:
      limits:
        cpu: 20m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 25Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
  falcoExporter:
    resources: {}
    tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
    affinity: {}
    nodeSelector: {}
## Elasticsearch cluster topolgy.
## Used in prometheus alerts.
elasticsearch:
  masterNode:
    count: 1
  dataNode:
    count: 2
  clientNode:
    count: 1
## Prometheus configuration.
## Prometheus collects metrics and pushes it to InfluxDB.
prometheus:
  remoteWrite:
    ## User used when authentication against InfluxDB.
    user: wcWriter
  ## Persistence for prometheus to store metrics and wal.
  storage:
    enabled: false
  ## When prometheus should start to remove metrics from local storage.
  retention:
    size: 1GiB
    age: 3d
  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1" ## Must be a string (integers might be suported in newer versions)
  tolerations: []
  affinity: {}
  nodeSelector: {}
  ## Additional prometheus scrape config.
  ## ref: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
  additionalScrapeConfigs: []
## Open policy agent configuration
opa:
  # Disable to work around this issue: https://github.com/elastisys/compliantkubernetes-apps/issues/230
  enabled: false
  ## Enable rule that requires pods to come from
  ## the image registry defined by "URL".
  ## "enforcement" can assume either "dryrun" or "deny".
  imageRegistry:
    enabled: true
    enforcement: dryrun
    URL: harbor.pipeline-exoscale.elastisys.se
  ## Enable rule that requires pods to be targeted
  ## by at least one network policy.
  networkPolicies:
    enabled: true
    enforcement: dryrun
  ## Enable rule that requires pods to have resource requests.
  resourceRequests:
    enabled: true
    enforcement: dryrun
## Configuration for fluentd.
## Fluentd ships logs to elasticsearch.
## Consists of two different deployments, one for running on master nodes
## and and one for running on "user nodes".
fluentd:
  ## Tolerate master nodes.
  tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      value: ""
  ## Only run on master nodes.
  nodeSelector:
    node-role.kubernetes.io/master: ""
  resources:
    limits:
      cpu: 200m
      memory: 500Mi
    requests:
      cpu: 200m
      memory: 500Mi
  affinity: {}
  ## Extra fluentd config to mount.
  extraConfigMaps: {}
  ## User controllable fluentd deployment.
  ## These pods collect logs from nodes where the user can run pods.
  ## Users can specify additional plugins and config in the respective configmaps:
  ## 'fluentd-extra-plugins', and 'fluentd-extra-config'.
  user:
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 200m
        memory: 500Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
## Set external traffic policy to: "Local" to preserve source IP on
## providers supporting it
## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
externalTrafficPolicy:
  local: false
  ## Source IP range to allow.
  whitelistRange:
    global: 0.0.0.0/0
## Nginx ingress controller configuration
ingressNginx:
  controller:
    resources: {}
    tolerations: []
    affinity: {}
    nodeSelector: {}
    config:
      ## If 'true', use PROXY protocol
      ## ref: https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
      useProxyProtocol: false
    ## If 'true', nginx will use host ports 80 and 443
    useHostPort: true
    ## Kubernetes service configuration.
    service:
      enabled: false
      ## Type of service.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: set-me
      ## Annotations to add to service
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      annotations: set-me
    ## Additional configuration options for Nginx
    ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}
  defaultBackend:
    resources: {}
    tolerations: []
    affinity: {}
    nodeSelector: {}
## Configration for Velero and Restic.
## Check out https://compliantkubernetes.io/user-guide/backup/ to see what's included in backups.
velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi
  restic:
    tolerations: []
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi
## Configuration for cert-manager issuers.
issuers:
  ## Deploy let's encrypt ACME issuers
  ## "letsencrypt-prod" and "letsencrypt-staging".
  letsencrypt:
    enabled: false
  ## Additional issuers to create.
  ## ref: https://cert-manager.io/docs/configuration/
  extraIssuers: []
  # - apiVersion: cert-manager.io/v1
  #   kind: Issuer
  #   metadata:
  #     name: selfsigned-issuer
  #     namespace: sandbox
  #   spec:
  #     selfSigned: {}
## Configration for cert-manager and it's components.
certmanager:
  resources: {}
  nodeSelector: {}
  tolerations: {}
  affinity: {}
  webhook:
    resources: {}
    nodeSelector: {}
    tolerations: {}
    affinity: {}
  cainjector:
    resources: {}
    nodeSelector: {}
    tolerations: {}
    affinity: {}
metricsServer:
  enabled: true
calicoAccountant:
  enabled: true
calicoFelixMetrics:
  enabled: true
clusterAdmin:
  users: []
  groups: []
starboard:
  resources: {}
  tolerations: []
  affinity: {}
vulnerabilityExporter:
  resources: {}
  tolerations: []
  affinity: {}
monitoring:
  rook:
    enabled: true
