name: End-to-end

on:
  schedule:
  # Run at 4 and 10 GMT, basically "early morning" and "lunch"
  - cron: "0 4,10 * * *"

jobs:

  build-apps-image:
    runs-on: ubuntu-20.04
    steps:
    - name: Checkout compliantkubernetes-apps
      uses: actions/checkout@v2
      with:
        repository: elastisys/compliantkubernetes-apps
        ref: main
    - name: Build docker image
      uses: whoan/docker-build-with-cache-action@v3
      with:
        username: "${{ secrets.DOCKERHUB_USERNAME }}"
        password: "${{ secrets.DOCKERHUB_PASSWORD }}"
        image_name: elastisys/compliantkubernetes-apps-pipeline
        image_tag: ${{ github.sha }}
        context: pipeline

  apps:
    needs: [build-apps-image]
    runs-on: ubuntu-20.04
    container: elastisys/compliantkubernetes-apps-pipeline:${{ github.sha }}
    strategy:
      fail-fast: false
      matrix:
        cluster:
          - sc
          - wc
    env:
      EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
      EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
      CK8S_CONFIG_PATH: ./apps/pipeline/config/exoscale
      CK8S_AUTO_APPROVE: true
    steps:
    ## TODO: We cannot currently use ck8s-kubespray since the terraform code
    ## for exoscale is not in the version we have in the submodule
    - name: Checkout ck8s-kubespray
      uses: actions/checkout@v2
      with:
        repository: elastisys/compliantkubernetes-kubespray
        # TODO: Should we use a specific commit here?
        ref: main
        submodules: recursive
        path: compliantkubernetes-kubespray
    - name: Checkout compliantkubernetes-apps
      uses: actions/checkout@v2
      with:
        repository: elastisys/compliantkubernetes-apps
        ref: main
        path: apps
        fetch-depth: 0 # Fetches all tags

    - name: Generate and insert public SSH key
      run: |
        ssh-keygen -q -t rsa -N "" -f apps/pipeline/config/exoscale/id_rsa
        sed -i "s!PUBLIC_SSH_KEY_HERE!$(cat apps/pipeline/config/exoscale/id_rsa.pub)!g" apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars
    - name: Init terraform
      uses: docker://hashicorp/terraform:0.14.7
      with:
        args: init compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale
    # Note: This container doesn't run bash, so we cannot use env vars in the arguments
    - name: Run terraform
      uses: docker://hashicorp/terraform:0.14.7
      with:
        args: apply -auto-approve -var-file apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale

    - name: Prepare DNS
      if: matrix.cluster == 'sc'
      run: |
        ingress_controller_lb_ip_address="$(jq -r .outputs.ingress_controller_lb_ip_address.value $CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/terraform.tfstate)"
        sed -i "s/ip-address/${ingress_controller_lb_ip_address}/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
        sed -i "s/ACTION/CREATE/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
        cat apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
    - name: Create DNS records
      if: matrix.cluster == 'sc'
      uses: docker://amazon/aws-cli:2.1.26
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
        # See https://github.com/aws/aws-cli/issues/5262
        AWS_EC2_METADATA_DISABLED: true
      with:
        args: route53 change-resource-record-sets --hosted-zone-id Z2STJRQSJO5PZ0 --change-batch file://apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json

    - name: Run kubespray
      uses: docker://quay.io/kubespray/kubespray:v2.19.0
      env:
        ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
      with:
        args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray && ansible-playbook -i ../../$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/inventory.ini --private-key ../../$CK8S_CONFIG_PATH/id_rsa --become --become-user=root cluster.yml"

    - name: Import PGP key and configure GPG agent
      run: ./apps/pipeline/setup-pgp.bash
      env:
        PGP_KEY: ${{ secrets.PGP_KEY }}
        PGP_PASSPHRASE: ${{ secrets.PGP_PASSPHRASE }}

    - name: Prepare for apps
      env:
        S3_ACCESS_KEY: ${{ secrets.EXOSCALE_API_KEY }}
        S3_SECRET_KEY: ${{ secrets.EXOSCALE_API_SECRET }}
      run: |
        # Set public LB IP in the kubeconfig
        control_plane_lb_ip_address="$(jq -r .outputs.control_plane_lb_ip_address.value $CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/terraform.tfstate)"
        yq write --inplace "$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf" clusters[0].cluster.server https://${control_plane_lb_ip_address}:6443
        # Copy to correct location and encrypt
        mkdir "$CK8S_CONFIG_PATH/.state"
        cp "$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf" "$CK8S_CONFIG_PATH/.state/kube_config_${{ matrix.cluster }}.yaml"
        sops --config "$CK8S_CONFIG_PATH/.sops.yaml" -e -i "$CK8S_CONFIG_PATH/.state/kube_config_${{ matrix.cluster }}.yaml"
        # Encrypt and set necessary secrets
        sops --config "$CK8S_CONFIG_PATH/.sops.yaml" -e -i "$CK8S_CONFIG_PATH/secrets.yaml"
        sops --config "$CK8S_CONFIG_PATH/.sops.yaml" --set "[\"objectStorage\"] {\"s3\": {\"accessKey\": \"${S3_ACCESS_KEY}\", \"secretKey\": \"${S3_SECRET_KEY}\"}}" "$CK8S_CONFIG_PATH/secrets.yaml"

    - name: Initialize apps
      env:
        CK8S_ENVIRONMENT_NAME: pipeline-exoscale
        CK8S_CLOUD_PROVIDER: exoscale
        CK8S_FLAVOR: dev
      run: ./apps/bin/ck8s init
      id: initialize-apps

    - name: Create buckets
      if: matrix.cluster == 'sc'
      env:
        EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
        EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
      run: |
        ./apps/scripts/S3/generate-s3cfg.sh exoscale "${EXOSCALE_API_KEY}" "${EXOSCALE_API_SECRET}" sos-ch-gva-2.exo.io ch-gva-2 > s3cfg.ini
        ./apps/scripts/S3/entry.sh --s3cfg s3cfg.ini create

    - name: Install rook
      env:
        KUBECONFIG: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf
      run: |
        ./deploy-rook.sh
      working-directory: ./compliantkubernetes-kubespray/rook

    # TODO: Some kind of test of Rook before we waste time waiting for apps to fail if it is broken...
    # We may also want to move Rook to the apps bootstrap instead.

    - name: Gather configs
      if: always() && steps.initialize-apps.outcome == 'success' && matrix.cluster == 'sc'
      shell: bash
      working-directory: ./apps/pipeline/config/exoscale
      run: |
        cp defaults/common-config.yaml common-defaults.yaml
        cp defaults/sc-config.yaml sc-defaults.yaml
        cp defaults/wc-config.yaml wc-defaults.yaml
        tar -czf configs.tar.gz common-defaults.yaml common-config.yaml sc-defaults.yaml sc-config.yaml wc-defaults.yaml wc-config.yaml
      id: gather-configs

    - name: Upload configs
      if: always() && steps.gather-configs.outcome == 'success'
      uses: actions/upload-artifact@v2
      with:
        name: compliantkubernetes-apps-pipeline-config
        path: ./apps/pipeline/config/exoscale/configs.tar.gz

    - name: Install apps
      run: ./apps/pipeline/apply-${{ matrix.cluster }}.bash
      shell: bash
      id: install-apps

    - name: Wait for OpenSearch to be ready
      if: steps.install-apps.outcome == 'success' && matrix.cluster == 'wc'
      run: ./apps/pipeline/opensearch.bash
      continue-on-error: true
      shell: bash
      id: opensearch

    - name: Test apps
      if: always() && (steps.install-apps.outcome != 'cancelled' && steps.install-apps.outcome != 'skipped' && steps.install-apps.outcome != 'failure')
      run: ./apps/bin/ck8s test ${{ matrix.cluster }}
      env:
        PIPELINE: "true"

    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v2
      with:
        name: compliantkubernetes-apps-logs
        path: ./logs
    - name: Upload events
      if: always()
      uses: actions/upload-artifact@v2
      with:
        name: compliantkubernetes-apps-events
        path: ./events

    - name: Destroy terraform
      if: always()
      uses: docker://hashicorp/terraform:0.14.7
      with:
        args: destroy -auto-approve -var-file apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale

    - name: Prepare DNS cleanup
      if: always() && (matrix.cluster == 'sc')
      run: |
        sed -i "s/CREATE/DELETE/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
        cat apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
    - name: Delete DNS records
      if: always() && (matrix.cluster == 'sc')
      uses: docker://amazon/aws-cli:2.1.26
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
        AWS_EC2_METADATA_DISABLED: true
      with:
        args: route53 change-resource-record-sets --hosted-zone-id Z2STJRQSJO5PZ0 --change-batch file://apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json

    - name: Delete buckets
      if: always() && (matrix.cluster == 'sc')
      env:
        EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
        EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
      run: |
        ./apps/scripts/S3/entry.sh --s3cfg s3cfg.ini delete

  clean-apps-image:
    needs: apps
    if: always()
    runs-on: ubuntu-20.04
    steps:
    - name: Checkout compliantkubernetes-apps
      uses: actions/checkout@v2
      with:
        repository: elastisys/compliantkubernetes-apps
        ref: main
    - name: Cleanup docker image
      run: ./pipeline/cleanup-docker-image.bash
      env:
        DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
        DOCKERHUB_PASSWORD: ${{ secrets.DOCKERHUB_PASSWORD }}
        DOCKERHUB_REPOSITORY: elastisys/compliantkubernetes-apps-pipeline

  alert:
    runs-on: ubuntu-20.04
    needs: [clean-apps-image]
    if: always() && github.ref == 'refs/heads/main'  # Make sure this runs even if some jobs fail, and only on main
    steps:
    - id: get-sha
      run: |
          echo ::set-output name=sha::$( curl https://api.github.com/repos/elastisys/compliantkubernetes-apps/git/ref/heads/main | jq .object.sha | tr -d '"')
    - id: get-sha-short
      run: |
          echo ::set-output name=sha::$( echo ${{ steps.get-sha.outputs.sha }} | cut -c -8)
    - id: result
      run: |
        curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs \
        --header 'Authorization: token ${{ secrets.GITHUB_TOKEN }}' \
        --header 'Accept: application/vnd.github.v3+json' > response.json

        cat response.json | jq -r '.jobs[] | select(.conclusion=="failure") | .id' > jobs.failed
        if [ -s jobs.failed ]
        then
          while read i
          do
            cat response.json | jq --arg i $i -r '[.jobs[] | select(.id=='$i') | "The JOB: ", .name, " failed at STEPS: ", ([(.steps[] | select(.conclusion=="failure") | .name)] | join("; "))] | join("")' >> steps.failed
          done < jobs.failed
          export RESULT="$(cat steps.failed)"
        else
          export RESULT="All good."
        fi
        RESULT="${RESULT//'%'/'%25'}"
        RESULT="${RESULT//$'\n'/'%0A'}"
        RESULT="${RESULT//$'\r'/'%0D'}"
        echo "::set-output name=result::$(echo "$RESULT")"
    - uses: technote-space/workflow-conclusion-action@v1
    - name: Send Slack alert
      uses: elastisys/action-slack@v3
      with:
        status: custom
        fields: repo,message,commit,author,action,workflow,job,took,ref
        custom_payload: |
          {
            attachments: [{
              color: `${process.env.WORKFLOW_CONCLUSION}` === 'success' ? 'good' : `${process.env.WORKFLOW_CONCLUSION}` === 'failure' ? 'danger' : 'warning',
              text: `Pipeline: ${process.env.AS_WORKFLOW}\n\ncompliantkubernetes-apps commit: <https://github.com/elastisys/compliantkubernetes-apps/commit/${{ steps.get-sha.outputs.sha }}|${{ steps.get-sha-short.outputs.sha }}>\n\n Status: ${process.env.WORKFLOW_CONCLUSION}\nTook: ${process.env.AS_TOOK}\nDetails: ${{ steps.result.outputs.result }}`,
            }]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }} # required
      if: always()  # Notify for both successes and failures
