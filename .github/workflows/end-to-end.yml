name: End-to-end

on:
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'warning'
        type: choice
        options:
        - info
        - warning
        - debug
      tags:
        description: 'Test scenario tags'
        required: false
        type: boolean
      environment:
        description: 'Environment to run tests against'
        type: environment
        required: true

jobs:
  build-apps-image:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v3
        with:
          repository: elastisys/compliantkubernetes-apps
          ref: main
      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ghcr.io/elastisys/compliantkubernetes-apps-pipeline
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Build and push Docker images
        uses: docker/build-push-action@v3
        with:
          context: pipeline
          push: true
          tags: ghcr.io/elastisys/compliantkubernetes-apps-pipeline:${{ github.sha }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-to: type=gha,mode=max
          cache-from: type=gha

  apps:
    needs: [build-apps-image]
    runs-on: ubuntu-latest
    container: ghcr.io/elastisys/compliantkubernetes-apps-pipeline:${{ github.sha }}
    strategy:
      fail-fast: false
      matrix:
        cluster:
          - sc
          - wc
    env:
      EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
      EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
      CK8S_CONFIG_PATH: ./apps/pipeline/config/exoscale
      CK8S_AUTO_APPROVE: true
    steps:
      - name: Checkout ck8s-kubespray
        uses: actions/checkout@v3
        with:
          repository: elastisys/compliantkubernetes-kubespray
          # TODO: Should we use a specific commit here?
          ref: main
          submodules: recursive
          path: compliantkubernetes-kubespray
      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v3
        with:
          repository: elastisys/compliantkubernetes-apps
          ref: main
          path: apps
          fetch-depth: 0 # Fetches all tags

      - name: Generate and insert public SSH key
        run: |
          ssh-keygen -q -t rsa -N "" -f apps/pipeline/config/exoscale/id_rsa
          sed -i "s!PUBLIC_SSH_KEY_HERE!$(cat apps/pipeline/config/exoscale/id_rsa.pub)!g" apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars
      - name: Init terraform
        uses: docker://hashicorp/terraform:1.2.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale" init
      # Note: This container doesn't run bash, so we cannot use env vars in the arguments
      - name: Run terraform
        continue-on-error: true
        id: runterraform0
        uses: docker://hashicorp/terraform:1.2.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale" apply -auto-approve -var-file /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate

      - name: Run terraform - retry 1
        id: runterraform1
        if: steps.runterraform0.outcome=='failure'
        uses: docker://hashicorp/terraform:1.2.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale" apply -auto-approve -var-file /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate

      - name: Prepare DNS
        if: matrix.cluster == 'sc'
        run: |
          ingress_controller_lb_ip_address="$(jq -r .outputs.ingress_controller_lb_ip_address.value $CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/terraform.tfstate)"
          sed -i "s/ip-address/${ingress_controller_lb_ip_address}/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
          sed -i "s/ACTION/CREATE/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
          cat apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
      - name: Create DNS records
        if: matrix.cluster == 'sc'
        uses: docker://amazon/aws-cli:2.1.26
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          # See https://github.com/aws/aws-cli/issues/5262
          AWS_EC2_METADATA_DISABLED: true
        with:
          args: route53 change-resource-record-sets --hosted-zone-id Z2STJRQSJO5PZ0 --change-batch file://apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json

      - name: Run kubespray
        continue-on-error: true
        id: runkubespray0
        uses: docker://quay.io/kubespray/kubespray:v2.23.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray && ansible-playbook -i ../../$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/inventory.ini --private-key ../../$CK8S_CONFIG_PATH/id_rsa --become --become-user=root cluster.yml"

      - name: Run kubespray - retry 1
        id: runkubespray1
        if: steps.runkubespray0.outcome=='failure'
        uses: docker://quay.io/kubespray/kubespray:v2.23.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray && ansible-playbook -i ../../$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/inventory.ini --private-key ../../$CK8S_CONFIG_PATH/id_rsa --become --become-user=root cluster.yml"

      - name: Import PGP key and configure GPG agent
        run: ./apps/pipeline/setup-pgp.bash
        env:
          PGP_KEY: ${{ secrets.PGP_KEY }}
          PGP_PASSPHRASE: ${{ secrets.PGP_PASSPHRASE }}

      - name: Prepare for apps
        env:
          S3_ACCESS_KEY: ${{ secrets.EXOSCALE_API_KEY }}
          S3_SECRET_KEY: ${{ secrets.EXOSCALE_API_SECRET }}
        run: |
          # Set public LB IP in the kubeconfig
          control_plane_lb_ip_address="$(jq -r .outputs.control_plane_lb_ip_address.value $CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/terraform.tfstate)"
          yq write --inplace "$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf" clusters[0].cluster.server https://${control_plane_lb_ip_address}:6443
          # Copy to correct location and encrypt
          mkdir "$CK8S_CONFIG_PATH/.state"
          cp "$CK8S_CONFIG_PATH/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf" "$CK8S_CONFIG_PATH/.state/kube_config_${{ matrix.cluster }}.yaml"
          sops --config "$CK8S_CONFIG_PATH/.sops.yaml" -e -i "$CK8S_CONFIG_PATH/.state/kube_config_${{ matrix.cluster }}.yaml"
          # Encrypt and set necessary secrets
          sops --config "$CK8S_CONFIG_PATH/.sops.yaml" -e -i "$CK8S_CONFIG_PATH/secrets.yaml"
          sops --config "$CK8S_CONFIG_PATH/.sops.yaml" --set "[\"objectStorage\"] {\"s3\": {\"accessKey\": \"${S3_ACCESS_KEY}\", \"secretKey\": \"${S3_SECRET_KEY}\"}}" "$CK8S_CONFIG_PATH/secrets.yaml"

      - name: Initialize apps
        env:
          CK8S_ENVIRONMENT_NAME: pipeline-exoscale
          CK8S_CLOUD_PROVIDER: exoscale
          CK8S_FLAVOR: dev
        run: ./apps/bin/ck8s init both
        id: initialize-apps

      - name: Create buckets
        if: matrix.cluster == 'sc'
        env:
          EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
          EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
        run: |
          ./apps/scripts/S3/generate-s3cfg.sh exoscale "${EXOSCALE_API_KEY}" "${EXOSCALE_API_SECRET}" sos-ch-gva-2.exo.io ch-gva-2 > s3cfg.ini
          ./apps/scripts/S3/entry.sh --s3cfg s3cfg.ini create

      - name: Bootstrap rook
        env:
          KUBECONFIG: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf
          CK8S_CONFIG_PATH: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale
          CK8S_APPS_PIPELINE: "true"
          CLUSTER: ${{ matrix.cluster }}
        run: |
          ./scripts/pipeline/bootstrap-rook.sh
        working-directory: ./compliantkubernetes-kubespray/rook
        id: bootstrap-rook

      - name: Test rook
        env:
          KUBECONFIG: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf
          CK8S_CONFIG_PATH: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale
          CK8S_APPS_PIPELINE: "true"
          CLUSTER: ${{ matrix.cluster }}
        run: |
          ./scripts/pipeline/wait-for-rook.sh ${{ matrix.cluster }}
        working-directory: ./compliantkubernetes-kubespray/rook
        id: test-rook

      - name: Gather configs
        if: always() && steps.initialize-apps.outcome == 'success' && matrix.cluster == 'sc'
        shell: bash
        working-directory: ./apps/pipeline/config/exoscale
        run: |
          cp defaults/common-config.yaml common-defaults.yaml
          cp defaults/sc-config.yaml sc-defaults.yaml
          cp defaults/wc-config.yaml wc-defaults.yaml
          tar -czf configs.tar.gz common-defaults.yaml common-config.yaml sc-defaults.yaml sc-config.yaml wc-defaults.yaml wc-config.yaml
        id: gather-configs

      - name: Upload configs
        if: always() && steps.gather-configs.outcome == 'success'
        uses: actions/upload-artifact@v3
        with:
          name: compliantkubernetes-apps-pipeline-config
          path: ./apps/pipeline/config/exoscale/configs.tar.gz

      - name: Install apps
        if: steps.bootstrap-rook.outcome == 'success' && steps.test-rook.outcome == 'success'
        run: ./apps/pipeline/apply-${{ matrix.cluster }}.bash
        shell: bash
        id: install-apps

      - name: Wait for OpenSearch to be ready
        if: steps.install-apps.outcome == 'success' && matrix.cluster == 'wc'
        run: ./apps/pipeline/opensearch.bash
        continue-on-error: true
        shell: bash
        id: opensearch

      - name: Test apps
        if: always() && (steps.install-apps.outcome != 'cancelled' && steps.install-apps.outcome != 'skipped')
        run: ./apps/bin/ck8s test ${{ matrix.cluster }} apps --logging-enabled
        env:
          PIPELINE: "true"
        id: test-apps

      - name: Finalise deploy rook
        env:
          KUBECONFIG: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/artifacts/admin.conf
          CK8S_CONFIG_PATH: /__w/compliantkubernetes-apps/compliantkubernetes-apps/apps/pipeline/config/exoscale
          CK8S_APPS_PIPELINE: "true"
          CLUSTER: ${{ matrix.cluster }}
        run: |
          ./scripts/pipeline/finalise-deploy-rook.sh
        working-directory: ./compliantkubernetes-kubespray/rook
        id: finalise-rook

      - name: Upload logs
        if: steps.install-apps.outcome == 'failure' || steps.test-apps.outcome == 'failure'
        uses: actions/upload-artifact@v3
        with:
          name: compliantkubernetes-apps-logs
          path: ./logs

      - name: Upload events
        if: steps.install-apps.outcome == 'failure' || steps.test-apps.outcome == 'failure'
        uses: actions/upload-artifact@v3
        with:
          name: compliantkubernetes-apps-events
          path: ./events

      - name: Upload rook-ceph logs
        if: steps.finalise-rook.outcome == 'failure' || steps.bootstrap-rook.outcome == 'failure' || steps.test-rook.outcome == 'failure' || steps.install-apps.outcome == 'failure'
        uses: actions/upload-artifact@v3
        with:
          name: rook-ceph-logs
          path: ./compliantkubernetes-kubespray/rook/logs

      - name: Upload rook-ceph events
        if: steps.finalise-rook.outcome == 'failure' || steps.bootstrap-rook.outcome == 'failure' || steps.test-rook.outcome == 'failure' || steps.install-apps.outcome == 'failure'
        uses: actions/upload-artifact@v3
        with:
          name: rook-ceph-events
          path: ./compliantkubernetes-kubespray/rook/events

      - name: Destroy terraform
        continue-on-error: true
        id: destroyterraform0
        if: always()
        uses: docker://hashicorp/terraform:1.2.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale" destroy -auto-approve -var-file /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate

      - name: Destroy terraform - retry 1
        id: destroyterraform1
        if: always() && steps.destroyterraform0.outcome=='failure'
        uses: docker://hashicorp/terraform:1.2.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale" destroy -auto-approve -var-file /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/default.tfvars -state /github/workspace/apps/pipeline/config/exoscale/pipeline-${{ matrix.cluster }}-config/terraform.tfstate

      - name: Prepare DNS cleanup
        if: always() && (matrix.cluster == 'sc')
        run: |
          sed -i "s/CREATE/DELETE/g" apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
          cat apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json
      - name: Delete DNS records
        if: always() && (matrix.cluster == 'sc')
        uses: docker://amazon/aws-cli:2.1.26
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          AWS_EC2_METADATA_DISABLED: true
        with:
          args: route53 change-resource-record-sets --hosted-zone-id Z2STJRQSJO5PZ0 --change-batch file://apps/pipeline/config/exoscale/dns-${{ matrix.cluster }}.json

      - name: Delete buckets
        if: always() && (matrix.cluster == 'sc')
        env:
          EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
          EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
        run: |
          ./apps/scripts/S3/entry.sh --s3cfg s3cfg.ini delete

  clean-apps-image:
    needs: apps
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v3
        with:
          repository: elastisys/compliantkubernetes-apps
          ref: main
      - name: Cleanup docker image
        run: ./pipeline/cleanup-docker-image.bash
        env:
          GITHUB_ACTOR: ${{ github.actor }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  alert:
    runs-on: ubuntu-latest
    needs: [clean-apps-image]
    if: always() && github.ref == 'refs/heads/main' # Make sure this runs even if some jobs fail, and only on main
    steps:
      - id: get-sha
        run: |
          echo "sha=$( curl https://api.github.com/repos/elastisys/compliantkubernetes-apps/git/ref/heads/main | jq .object.sha | tr -d '"')" >> $GITHUB_OUTPUT
      - id: get-sha-short
        run: |
          echo "sha=$( echo ${{ steps.get-sha.outputs.sha }} | cut -c -8)" >> $GITHUB_OUTPUT
      - id: result
        run: |
          curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs \
          --header 'Authorization: token ${{ secrets.GITHUB_TOKEN }}' \
          --header 'Accept: application/vnd.github.v3+json' > response.json

          cat response.json | jq -r '.jobs[] | select(.conclusion=="failure") | .id' > jobs.failed
          if [ -s jobs.failed ]
          then
            while read i
            do
              cat response.json | jq --arg i $i -r '[.jobs[] | select(.id=='$i') | "The JOB: ", .name, " failed at STEPS: ", ([(.steps[] | select(.conclusion=="failure") | .name)] | join("; "))] | join("")' >> steps.failed
            done < jobs.failed
            export RESULT="$(cat steps.failed)"
          else
            export RESULT="All good."
          fi
          RESULT="${RESULT//'%'/'%25'}"
          RESULT="${RESULT//$'\n'/'%0A'}"
          RESULT="${RESULT//$'\r'/'%0D'}"
          echo "result=$(echo "$RESULT")" >> $GITHUB_OUTPUT
      - uses: technote-space/workflow-conclusion-action@v1
      - name: Send Slack alert
        uses: elastisys/action-slack@v3
        with:
          status: custom
          fields: repo,message,commit,author,action,workflow,job,took,ref
          custom_payload: |
            {
              attachments: [{
                color: `${process.env.WORKFLOW_CONCLUSION}` === 'success' ? 'good' : `${process.env.WORKFLOW_CONCLUSION}` === 'failure' ? 'danger' : 'warning',
                text: `Pipeline: ${process.env.AS_WORKFLOW}\n\ncompliantkubernetes-apps commit: <https://github.com/elastisys/compliantkubernetes-apps/commit/${{ steps.get-sha.outputs.sha }}|${{ steps.get-sha-short.outputs.sha }}>\n\n Status: ${process.env.WORKFLOW_CONCLUSION}\nTook: ${process.env.AS_TOOK}\nDetails: ${{ steps.result.outputs.result }}`,
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }} # required
        if: always() # Notify for both successes and failures
