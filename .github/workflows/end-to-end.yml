name: End-to-end

on:
  schedule:
    - cron: "0 4 * * *"

# TODO: Use workflow dispatch for testing manually. Can be removed before merging the PR
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to run tests against"
        type: environment
        required: true
        default: test
      alerts:
        description: "Send Slack alerts"
        type: boolean
        default: false

jobs:
  check-for-updates:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check-pushes.outputs.should_run }}
    steps:
      - name: Checkout main branch
        uses: actions/checkout@v5
        with:
          ref: main

      - name: Get last workflow run SHA
        id: get-last-sha
        run: |
          WORKFLOW_ID=$(curl -s -L \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/haoruipeng/compliantkubernetes-apps/actions/workflows" | jq '.workflows | .[] | select(.name=="End-to-end").id')
          LAST_COMPLETED_RUN_SHA=$(curl -s -L \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/haoruipeng/compliantkubernetes-apps/actions/workflows/${WORKFLOW_ID}/runs?status=completed&per_page=1" | \
            jq -r '.workflow_runs[0].head_sha // ""')
          echo "$LAST_COMPLETED_RUN_SHA"
          echo "LAST_COMPLETED_RUN_SHA=$LAST_COMPLETED_RUN_SHA" >> $GITHUB_OUTPUT

      - name: Check for new pushes
        id: check-pushes
        run: |
          CURRENT_MAIN_SHA=$(git rev-parse HEAD)
          echo "Current main branch HEAD SHA: $CURRENT_MAIN_SHA"
          echo "Last completed workflow run SHA: ${{ steps.get-last-sha.outputs.LAST_COMPLETED_RUN_SHA }}"

          if [ -z "${{ steps.get-last-sha.outputs.LAST_COMPLETED_RUN_SHA }}" ]; then
            echo "No previous successful runs found for this workflow. Running workflow."
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [ "$CURRENT_MAIN_SHA" == "${{ steps.get-last-sha.outputs.LAST_COMPLETED_RUN_SHA }}" ]; then
            echo "No new pushes on main since last successful run. Skipping workflow."
            echo "should_run=false" >> $GITHUB_OUTPUT
          else
            echo "New pushes detected on main. Proceeding with workflow."
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi

  build-apps-image:
    needs: check-for-updates
    if: needs.check-for-updates.outputs.should_run == 'true'
    environment: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v5
        with:
          repository: HaoruiPeng/compliantkubernetes-apps
          ref: main
      - name: Log in to the Container registry
        uses: docker/login-action@v3.6.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5.9.0
        with:
          images: ghcr.io/haoruipeng/compliantkubernetes-apps-pipeline

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3.11.1

      - name: Build and push pipeline Docker image
        uses: docker/build-push-action@v6.18.0
        with:
          file: pipeline/Dockerfile
          push: true
          tags: ghcr.io/haoruipeng/compliantkubernetes-apps-pipeline:${{ github.sha }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-to: type=gha,mode=max
          cache-from: type=gha

  apps:
    needs: [build-apps-image]
    runs-on: ubuntu-latest
    container: ghcr.io/haoruipeng/compliantkubernetes-apps-pipeline:${{ github.sha }}
    strategy:
      fail-fast: false
      matrix:
        cluster:
          - sc
          - wc
    environment: test

    env:
      CK8S_CONFIG_PATH: ./apps/pipeline/config/elastx
      CK8S_AUTO_APPROVE: true
      OS_PASSWORD: ${{ secrets.OS_PASSWORD }}
      OS_USERNAME: ${{ vars.OS_USERNAME }}
      OS_AUTH_URL: ${{ vars.OS_AUTH_URL }}
      OS_PROJECT_ID: ${{ vars.OS_PROJECT_ID }}
      OS_PROJECT_NAME: ${{ vars.OS_PROJECT_NAME }}
      OS_USER_DOMAIN_NAME: ${{ vars.OS_USER_DOMAIN_NAME }}
      OS_REGION_NAME: ${{ vars.OS_REGION_NAME }}
      OS_INTERFACE: ${{ vars.OS_INTERFACE }}
      OS_IDENTITY_API_VERSION: ${{ vars.OS_IDENTITY_API_VERSION }}
      CK8S_CLOUD_PROVIDER: elastx
      CK8S_ENVIRONMENT_NAME: pipeline-elastx
      CK8S_FLAVOR: prod
      CK8S_K8S_INSTALLER: kubespray

    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Get latest ck8s-kubespray release
        uses: oprypin/find-latest-tag@v1
        with:
          repository: elastisys/compliantkubernetes-kubespray
          releases-only: true
        id: kubespray-repo

      - name: Checkout ck8s-kubespray
        uses: actions/checkout@v5
        with:
          repository: elastisys/compliantkubernetes-kubespray
          ref: ${{ steps.kubespray-repo.outputs.tag }}
          submodules: recursive
          path: compliantkubernetes-kubespray

      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v5
        with:
          repository: HaoruiPeng/compliantkubernetes-apps
          ref: main
          path: apps
          fetch-depth: 0

      - name: Generate and insert public SSH key
        run: |
          ssh-keygen -q -t rsa -N "" -f apps/pipeline/config/elastx/id_rsa
          sed -i "s!PUBLIC_SSH_KEY_HERE!$(cat apps/pipeline/config/elastx/id_rsa.pub)!g" apps/pipeline/config/elastx/${{ matrix.cluster }}-config/group_vars/all/ck8s-ssh-keys.yaml

      - name: Init terraform
        uses: docker://hashicorp/terraform:1.3.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack" init

      - name: Run terraform
        continue-on-error: true
        id: runterraform0
        uses: docker://hashicorp/terraform:1.3.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack" apply -auto-approve -var-file /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/cluster.tfvars -state /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/terraform.tfstate

      - name: Run terraform - retry 1
        id: runterraform1
        if: steps.runterraform0.outcome=='failure'
        uses: docker://hashicorp/terraform:1.3.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack" apply -auto-approve -var-file /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/cluster.tfvars -state /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/terraform.tfstate

      - name: Setup subnetIDs
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        run: |
          subnet_id="$(jq -r .outputs.private_subnet_id.value ${{ matrix.cluster }}-config/terraform.tfstate)"
          yq -i ".external_openstack_lbaas_subnet_id = \"$subnet_id\"" "${{ matrix.cluster }}-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml"

      - name: Prepare sc DNS
        if: matrix.cluster == 'sc'
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        run: |
          k8s_master_ip="$(jq -r .outputs.k8s_master_fips.value ${{ matrix.cluster }}-config/terraform.tfstate)"
          for ip in $(echo "$k8s_master_ip" | jq -r '.[]'); do
            yq -i ".supplementary_addresses_in_ssl_keys += [\"$ip\"]" "${{ matrix.cluster }}-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml"
          done
          new_records=$(echo $k8s_master_ip | jq 'map({Value: .})')
          jq --argjson new_records "$new_records" --arg name "kube.ops.pipeline-elastx.dev-ck8s.com" \
          '
          (.Changes[] | select(.ResourceRecordSet.Name == $name) | .ResourceRecordSet.ResourceRecords) = $new_records
          ' \
          dns/${{ matrix.cluster }}-api.json > tmp.json && mv tmp.json  dns/${{ matrix.cluster }}-api.json

      - name: Prepare wc DNS
        if: matrix.cluster == 'wc'
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        run: |
          k8s_master_ip="$(jq -r .outputs.k8s_master_fips.value ${{ matrix.cluster }}-config/terraform.tfstate)"
          for ip in $(echo "$k8s_master_ip" | jq -r '.[]'); do
            yq -i ".supplementary_addresses_in_ssl_keys += [\"$ip\"]" "${{ matrix.cluster }}-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml"
          done
          new_records=$(echo $k8s_master_ip | jq 'map({Value: .})')
          jq --argjson new_records "$new_records" --arg name "kube.pipeline-elastx.dev-ck8s.com" \
          '
          (.Changes[] | select(.ResourceRecordSet.Name == $name) | .ResourceRecordSet.ResourceRecords) = $new_records
          ' \
          dns/${{ matrix.cluster }}-api.json > tmp.json && mv tmp.json  dns/${{ matrix.cluster }}-api.json

      - name: Create DNS records
        uses: docker://amazon/aws-cli:2.30.4
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          AWS_EC2_METADATA_DISABLED: true
        with:
          args: route53 change-resource-record-sets --hosted-zone-id Z1001117397DAU71G3RN2 --change-batch file://apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-api.json

      - name: Run kubespray
        continue-on-error: true
        id: runkubespray0
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            -i compliantkubernetes-kubespray/bin/node-labels-and-taints-inventory.bash
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root cluster.yml"

      - name: Run kubespray - retry 1
        id: runkubespray1
        if: steps.runkubespray0.outcome=='failure'
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            -i compliantkubernetes-kubespray/bin/node-labels-and-taints-inventory.bash
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root
            cluster.yml"

      - name: Get kubeconfig
        if: steps.runkubespray0.outcome=='success' || steps.runkubespray1.outcome=='success'
        id: get-kubeconfig
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root
            ../playbooks/kubeconfig.yml"

      - name: Adding cluster-admin ClusterRoleBinding
        id: add-admin-rbac
        if: steps.get-kubeconfig.outcome=='success'
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root
            ../playbooks/cluster_admin_rbac.yml"

      - name: Master cis benchmark patching
        id: master-cis
        if: steps.get-kubeconfig.outcome=='success'
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root
            ../playbooks/master_cis_benchmark_patch.yml"

      - name: Worker cis benchmark patching
        id: node-cis
        if: steps.get-kubeconfig.outcome=='success'
        uses: docker://quay.io/kubespray/kubespray:v2.28.0
        env:
          ANSIBLE_CONFIG: ./compliantkubernetes-kubespray/kubespray/ansible.cfg
        with:
          args: /bin/bash -c "cd compliantkubernetes-kubespray/kubespray &&
            ansible-playbook
            -i ../../${{env.CK8S_CONFIG_PATH}}/${{ matrix.cluster }}-config/inventory.ini
            --private-key ../../${{env.CK8S_CONFIG_PATH}}/id_rsa
            --become --become-user=root
            ../playbooks/worker_cis_benchmark_patch.yml"

      - name: Label nodes
        if: matrix.cluster == 'wc'
        env:
          KUBECONFIG: "${{ env.CK8S_CONFIG_PATH }}/.state/kube_config_${{ matrix.cluster }}.yaml"
        run: |
          kubectl get nodes -o custom-columns=NAME:.metadata.name --no-headers | grep "elastisys" | xargs -I {} kubectl label node {} elastisys.io/node-group=elastisys elastisys.io/node-type=elastisys

      - name: Import PGP key and configure GPG agent
        run: ./apps/pipeline/setup-pgp.bash
        env:
          PGP_PASSPHRASE: ${{ secrets.PGP_PASSPHRASE }}
          PGP_EMAIL: ${{ vars.PGP_EMAIL }}

      - name: Prepare for buckets
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        env:
          S3_ACCESS_KEY: ${{ secrets.ELASTX_API_ACCESS_KEY }}
          S3_SECRET_KEY: ${{ secrets.ELASTX_API_SECRET_KEY }}
        run: |
          sed -i "s/access_key_value/${{env.S3_ACCESS_KEY}}/g" ".state/s3cfg.ini"
          sed -i "s/secret_key_value/${{env.S3_SECRET_KEY}}/g" ".state/s3cfg.ini"

      - name: Initialize apps
        env:
          CK8S_ENVIRONMENT_NAME: pipeline-elastx
          CK8S_CLOUD_PROVIDER: elastx
          CK8S_FLAVOR: prod
        run: |
          sops --config "${{ env.CK8S_CONFIG_PATH }}/.sops.yaml" -e -i "${{ env.CK8S_CONFIG_PATH }}/secrets.yaml"
          ./apps/bin/ck8s init both
        id: initialize-apps

      - name: Create buckets
        if: matrix.cluster == 'sc'
        run: |
          ./apps/scripts/S3/entry.sh --s3cfg "${{ env.CK8S_CONFIG_PATH}}/.state/s3cfg.ini" create
          sops --config "${{ env.CK8S_CONFIG_PATH}}/.sops.yaml" -e -i "${{ env.CK8S_CONFIG_PATH}}/.state/s3cfg.ini"
        id: create-buckets

      - name: Gather configs
        if: steps.initialize-apps.outcome == 'success' && matrix.cluster == 'wc'
        shell: bash
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        run: |
          cp .state/kube_config_wc.yaml kube_config_wc.yaml
          cp defaults/wc-config.yaml wc-defaults.yaml
          tar -czf configs.tar.gz wc-defaults.yaml wc-config.yaml kube_config_wc.yaml
        id: gather-configs

      - name: Upload configs
        if: steps.gather-configs.outcome == 'success'
        uses: actions/upload-artifact@v4.6.2
        with:
          name: compliantkubernetes-apps-pipeline-config
          path: ./apps/pipeline/config/elastx/configs.tar.gz

      - name: Wait for configs
        if: steps.initialize-apps.outcome == 'success' && matrix.cluster == 'sc'
        uses: lexbritvin/wait-action@v1
        with:
          condition-type: "artifact"
          artifact-name: compliantkubernetes-apps-pipeline-config
          timeout-minutes: 20
        id: wait-configs

      - name: Download configs
        if: steps.wait-configs.outcome == 'success'
        uses: actions/download-artifact@v6
        with:
          name: compliantkubernetes-apps-pipeline-config
          path: ${{ env.CK8S_CONFIG_PATH}}
        id: download-configs

      - name: Unpack configs
        if: steps.download-configs.outcome == 'success'
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        run: |
          tar -xzf configs.tar.gz
          mv kube_config_wc.yaml ./.state/kube_config_wc.yaml
        id: unpack-configs

      - name: Create Google SA secret for dex
        if: matrix.cluster == 'sc'
        env:
          GOOGLE_SA: ${{ secrets.GOOGLE_SA }}
        run: |
          yq -i ".data[\"sa.json\"] = \"$GOOGLE_SA\"" "${{ env.CK8S_CONFIG_PATH }}/dex-google-group-claim/secret/google-sa-secret.yml"
          sops --config "${{ env.CK8S_CONFIG_PATH }}/.sops.yaml" -e -i "${{ env.CK8S_CONFIG_PATH }}/dex-google-group-claim/secret/google-sa-secret.yml"

      - name: Create QA configs
        if: matrix.cluster == 'sc'
        id: qa-config
        working-directory: ${{ env.CK8S_CONFIG_PATH }}
        shell: bash
        env:
          S3_ACCESS_KEY: ${{ secrets.ELASTX_API_ACCESS_KEY }}
          S3_SECRET_KEY: ${{ secrets.ELASTX_API_SECRET_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          GCP_CLIENT_ID: ${{ secrets.GCP_CLIENT_ID }}
          GCP_CLIENT_SECRET: ${{ secrets.GCP_CLIENT_SECRET }}
        run: |
          ls
          yq -i ".dex.gcp.clientID = \"${GCP_CLIENT_ID}\"" qa-config.json
          yq -i ".dex.gcp.clientSecret = \"${GCP_CLIENT_SECRET}\"" qa-config.json
          yq -i ".dnsProvider.aws.secrets.accessKey = \"${AWS_ACCESS_KEY_ID}\"" qa-config.json
          yq -i ".dnsProvider.aws.secrets.secretKey = \"${AWS_SECRET_ACCESS_KEY}\"" qa-config.json
          yq -i ".objectStorage.secrets.s3.accessKey = \"${S3_ACCESS_KEY}\"" qa-config.json
          yq -i ".objectStorage.secrets.s3.secretKey = \"${S3_SECRET_KEY}\"" qa-config.json

      - name: Install apps
        if: steps.qa-config.outcome == 'success' && matrix.cluster == 'sc'
        run: |
          git config --global --add safe.directory /__w/compliantkubernetes-apps/compliantkubernetes-apps
          git config --global --add safe.directory apps
          ./scripts/qa/install_apps.py -c ${{ env.CK8S_CONFIG_PATH }}/qa-config.json
        id: install-apps

      - name: Notify apps installation complete
        if: steps.install-apps.outcome == 'success'
        uses: actions/upload-artifact@v4.6.2
        with:
          name: apps-installed
          path: /dev/null

      - name: Wait until apps are installed
        uses: lexbritvin/wait-action@v1
        with:
          condition-type: "artifact"
          artifact-name: apps-installed
          timeout-minutes: 45

      - name: Update Ingress DNS records
        run: |
          ingress_ip=$(./apps/bin/ck8s ops kubectl ${{ matrix.cluster }} -n ingress-nginx get service ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[].ip}')
          ingress_ip_json=$(echo "{\"Value\": \"$ingress_ip\"}" | jq)
          echo $ingress_ip_json
          jq --argjson value "$ingress_ip_json" \
          '
          .Changes[].ResourceRecordSet.ResourceRecords += [ $value ]
          ' \
          ${{ env.CK8S_CONFIG_PATH }}/dns/${{ matrix.cluster }}-ingress.json > tmp.json && mv tmp.json ${{ env.CK8S_CONFIG_PATH }}/dns/${{ matrix.cluster }}-ingress.json
          cat ${{ env.CK8S_CONFIG_PATH }}/dns/${{ matrix.cluster }}-ingress.json

      - name: Create ingress DNS records
        uses: docker://amazon/aws-cli:2.30.4
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          AWS_EC2_METADATA_DISABLED: true
        with:
          args: route53 change-resource-record-sets --hosted-zone-id Z1001117397DAU71G3RN2 --change-batch file://apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-ingress.json

      - name: Test apps in sc
        if: matrix.cluster == 'sc'
        continue-on-error: true
        timeout-minutes: 15
        run: ./apps/bin/ck8s test ${{ matrix.cluster }} apps --logging-enabled
        env:
          PIPELINE: "true"
        id: test-apps-sc

      - name: Test apps in wc
        if: matrix.cluster == 'wc'
        timeout-minutes: 15
        continue-on-error: true
        run: ./apps/bin/ck8s test ${{ matrix.cluster }} apps --logging-enabled
        env:
          PIPELINE: "true"
        id: test-apps-wc

      - name: Signal that apps test on workload cluster is done
        if: matrix.cluster  == 'wc'
        uses: actions/upload-artifact@v5
        with:
          name: wc-apps-done
          path: /dev/null

      - name: Wait for workload cluster to finish test
        if: matrix.cluster == 'sc'
        uses: lexbritvin/wait-action@v1
        with:
          condition-type: "artifact"
          artifact-name: wc-apps-done
          timeout-minutes: 20
        id: wait-wc

      - name: Run end-to-end tests
        continue-on-error: true
        # if: steps.wait-wc.outcome == 'success'
        id: e2e-test
        run: ./apps/pipeline/test/end-to-end.bash

      - name: Signal that end-to-end test is finished
        if: always() && matrix.cluster == 'sc'
        uses: actions/upload-artifact@v5
        with:
          name: e2e-done
          path: /dev/null

      - name: Wait for end-to-end test to finish
        if: always() && matrix.cluster == 'wc'
        uses: lexbritvin/wait-action@v1
        with:
          condition-type: "artifact"
          artifact-name: e2e-done
          timeout-minutes: 60
        id: wait-e2e

      - name: Upload logs
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4.6.2
        with:
          name: compliantkubernetes-apps-logs-${{ matrix.cluster }}
          path: ./logs

      - name: Upload events
        if: always()
        continue-on-error: true
        uses: actions/upload-artifact@v4.6.2
        with:
          name: compliantkubernetes-apps-events-${{ matrix.cluster }}
          path: ./events

      - name: Clear apps
        continue-on-error: true
        if: always()
        id: cleanapps0
        shell: bash
        run: ./apps/pipeline/clean-${{ matrix.cluster }}.bash

      - name: Clear apps - retry
        if: steps.cleanapps0.outcome == 'failure'
        id: cleanapps1
        shell: bash
        run: ./apps/pipeline/clean-${{ matrix.cluster }}.bash

      - name: Destroy terraform
        continue-on-error: true
        id: destroyterraform0
        if: always()
        uses: docker://hashicorp/terraform:1.3.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack" destroy -auto-approve -var-file /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/cluster.tfvars -state /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/terraform.tfstate

      - name: Destroy terraform - retry 1
        id: destroyterraform1
        if: steps.destroyterraform0.outcome=='failure'
        uses: docker://hashicorp/terraform:1.3.9
        with:
          args: -chdir="/github/workspace/compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack" destroy -auto-approve -var-file /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/cluster.tfvars -state /github/workspace/apps/pipeline/config/elastx/${{ matrix.cluster }}-config/terraform.tfstate

      - name: Prepare DNS cleanup
        if: always()
        run: |
          sed -i "s/CREATE/DELETE/g" apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-api.json
          sed -i "s/CREATE/DELETE/g" apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-ingress.json
          cat apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-api.json
          cat apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-ingress.json

      - name: Delete DNS records
        if: always()
        uses: docker://amazon/aws-cli:2.30.4
        continue-on-error: true
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_DNS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_DNS_SECRET_ACCESS_KEY }}
          AWS_EC2_METADATA_DISABLED: true
        with:
          args:
            route53 change-resource-record-sets --hosted-zone-id Z1001117397DAU71G3RN2 --change-batch file://apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-ingress.json
            route53 change-resource-record-sets --hosted-zone-id Z1001117397DAU71G3RN2 --change-batch file://apps/pipeline/config/elastx/dns/${{ matrix.cluster }}-api.json

      - name: Delete buckets
        if: always() && (steps.create-buckets.outcome != 'skipped' && steps.create-buckets.outcome != 'cancelled')
        continue-on-error: true
        run: |
          sops exec-file --no-fifo "${{ env.CK8S_CONFIG_PATH }}/.state/s3cfg.ini" "./apps/scripts/S3/entry.sh --s3cfg {} delete"
        id: delete-buckets

  clean-apps-image:
    environment: test
    needs: [apps, build-apps-image]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout compliantkubernetes-apps
        uses: actions/checkout@v5
        with:
          repository: HaoruiPeng/compliantkubernetes-apps
          ref: main

      - name: Cleanup artifacts
        uses: geekyeggo/delete-artifact@v5
        with:
          name: |
              compliantkubernetes-apps-pipeline-config
              apps-installed
              wc-apps-done
              e2e-done

      - name: Cleanup docker image
        run: ./pipeline/cleanup-docker-image.bash
        env:
          GITHUB_ACTOR: ${{ github.actor }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # TODO: alert job condition should be changed to always() before PR is merged
  alert:
    needs: [clean-apps-image]
    runs-on: ubuntu-latest
    if: github.event.inputs.alerts == 'true' && github.ref == 'refs/heads/main' && github.workflow.on == 'workflow_dispatch'
    environment: test
    steps:
      - id: get-sha
        run: |
          echo "sha=$( curl https://api.github.com/repos/elastisys/compliantkubernetes-apps/git/ref/heads/main | jq .object.sha | tr -d '"')" >> $GITHUB_OUTPUT
      - id: get-sha-short
        run: |
          echo "sha=$( echo ${{ steps.get-sha.outputs.sha }} | cut -c -8)" >> $GITHUB_OUTPUT
      - id: result
        run: |
          curl -H "Accept: application/vnd.github.v3+json" https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs \
          --header 'Authorization: token ${{ secrets.GITHUB_TOKEN }}' \
          --header 'Accept: application/vnd.github.v3+json' > response.json

          cat response.json | jq -r '.jobs[] | select(.conclusion=="failure") | .id' > jobs.failed
          if [ -s jobs.failed ]
          then
            while read i
            do
              cat response.json | jq --arg i $i -r '[.jobs[] | select(.id=='$i') | "The JOB: ", .name, " failed at STEPS: ", ([(.steps[] | select(.conclusion=="failure") | .name)] | join("; "))] | join("")' >> steps.failed
            done < jobs.failed
            export RESULT="$(cat steps.failed)"
          else
            export RESULT="All good."
          fi
          RESULT="${RESULT//'%'/'%25'}"
          RESULT="${RESULT//$'\n'/'%0A'}"
          RESULT="${RESULT//$'\r'/'%0D'}"
          echo "result=$(echo "$RESULT")" >> $GITHUB_OUTPUT
      - uses: technote-space/workflow-conclusion-action@v1
      - name: Send Slack alert
        uses: elastisys/action-slack@v3
        with:
          status: custom
          fields: repo,message,commit,author,action,workflow,job,took,ref
          custom_payload: |
            {
              attachments: [{
                color: `${process.env.WORKFLOW_CONCLUSION}` === 'success' ? 'good' : `${process.env.WORKFLOW_CONCLUSION}` === 'failure' ? 'danger' : 'warning',
                text: `Pipeline: ${process.env.AS_WORKFLOW}\n\ncompliantkubernetes-apps commit: <https://github.com/elastisys/compliantkubernetes-apps/commit/${{ steps.get-sha.outputs.sha }}|${{ steps.get-sha-short.outputs.sha }}>\n\n Status: ${process.env.WORKFLOW_CONCLUSION}\nTook: ${process.env.AS_TOOK}\nDetails: ${{ steps.result.outputs.result }}`,
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
