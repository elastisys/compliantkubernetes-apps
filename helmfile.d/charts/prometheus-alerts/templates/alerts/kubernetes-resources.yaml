{{- if and .Values.defaultRules.create .Values.defaultRules.rules.kubernetesResources }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ printf "%s-%s" (include "prometheus-alerts.fullname" .) "kubernetes-resources" | trunc 63 | trimSuffix "-" }}
  labels:
    app: {{ template "prometheus-alerts.name" . }}
{{ include "prometheus-alerts.labels" . | indent 4 }}
{{- if .Values.defaultRules.alertLabels }}
{{ toYaml .Values.defaultRules.alertLabels | indent 4 }}
{{- end }}
{{- if .Values.defaultRules.annotations }}
  annotations:
{{ toYaml .Values.defaultRules.annotations | indent 4 }}
{{- end }}
spec:
  groups:
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: Node group {{`{{`}} $labels.label_elastisys_io_node_group {{`}}`}} has overcommitted CPU resource requests for Pods by {{`{{`}} $value {{`}}`}} CPU shares and cannot tolerate node failure.
        runbook_url: {{ .Values.runbookUrls.kubernetesResources.KubeCPUOvercommit }}
        summary: Node group {{`{{`}} $labels.label_elastisys_io_node_group {{`}}`}} has overcommitted CPU resource requests.
      expr: |-
        sum by (label_elastisys_io_node_group, cluster) (
          cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)")
        )
        -
        (
          sum by (label_elastisys_io_node_group, cluster) (kube_node_status_allocatable{resource="cpu"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)"))
          -
          max by (label_elastisys_io_node_group, cluster) (kube_node_status_allocatable{resource="cpu"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)"))
        )
        > 0
        and
        sum by (label_elastisys_io_node_group, cluster) (
          kube_node_status_allocatable{resource="cpu"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)")
        )
        -
        max by (label_elastisys_io_node_group, cluster) (
          kube_node_status_allocatable{resource="cpu"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude}}"}, "instance", "$1", "node", "(.*)")
        )
        > 0
      for: 10m
      labels:
        severity: warning
        group: KubernetesResources
{{- if .Values.defaultRules.additionalRuleLabels }}
{{ toYaml .Values.defaultRules.additionalRuleLabels | indent 8 }}
{{- end }}
    - alert: KubeMemoryOvercommit
      annotations:
        description: Node group {{`{{`}} $labels.label_elastisys_io_node_group {{`}}`}} has overcommitted memory resource requests for Pods by {{`{{`}} $value | humanize {{`}}`}} bytes and cannot tolerate node failure.
        runbook_url: {{ .Values.runbookUrls.kubernetesResources.KubeMemoryOvercommit }}
        summary: Node group {{`{{`}} $labels.label_elastisys_io_node_group {{`}}`}} has overcommitted memory resource requests.
      expr: |-
        sum by (label_elastisys_io_node_group, cluster) (
          cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)")
        )
        -
        (
          sum by (label_elastisys_io_node_group, cluster) (kube_node_status_allocatable{resource="memory"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)"))
          -
          max by (label_elastisys_io_node_group, cluster) (kube_node_status_allocatable{resource="memory"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)"))
        )
        > 0
        and
        sum by (label_elastisys_io_node_group, cluster) (
          kube_node_status_allocatable{resource="memory"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)")
        )
        -
        max by (label_elastisys_io_node_group, cluster) (
          kube_node_status_allocatable{resource="memory"}
            * on (node) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!~"{{ .Values.kubeResourceOvercommitAlertExclude }}"}, "instance", "$1", "node", "(.*)")
        )
        > 0
      for: 10m
      labels:
        severity: warning
        group: KubernetesResources
{{- if .Values.defaultRules.additionalRuleLabels }}
{{ toYaml .Values.defaultRules.additionalRuleLabels | indent 8 }}
{{- end }}
{{- if .Values.defaultRules.enabled.CPUThrottlingHigh }}
    - alert: CPUThrottlingHigh
      annotations:
        description: '{{`{{`}} $value | humanizePercentage {{`}}`}} throttling of CPU in namespace {{`{{`}} $labels.namespace {{`}}`}} for container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.pod {{`}}`}}.'
        runbook_url: {{ .Values.runbookUrls.kubernetesResources.CPUThrottlingHigh }}
        summary: Processes experience elevated CPU throttling.
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (cluster, container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (cluster, container, pod, namespace)
          > ( 25 / 100 )
      for: 15m
      labels:
        severity: info
        group: KubernetesResources
{{- if .Values.defaultRules.additionalRuleLabels }}
{{ toYaml .Values.defaultRules.additionalRuleLabels | indent 8 }}
{{- end }}
{{- end }}
{{- end }}
