{{ with .Values.fluentd.forwarder.buffer -}}
buffer: {{- toYaml . | nindent 2 }}
{{- end }}

livenessThresholdSeconds: {{ .Values.fluentd.forwarder.livenessThresholdSeconds }}
stuckThresholdSeconds: {{ .Values.fluentd.forwarder.stuckThresholdSeconds }}

extraConfigMaps:
  buffer.prop: |-
    @type "#{ENV['OUTPUT_BUFFER_TYPE']}"
    flush_mode "#{ENV['OUTPUT_BUFFER_FLUSH_MODE']}"
    retry_type "#{ENV['OUTPUT_BUFFER_RETRY_TYPE']}"
    flush_thread_count "#{ENV['OUTPUT_BUFFER_FLUSH_THREAD_TYPE']}"
    flush_interval "#{ENV['OUTPUT_BUFFER_FLUSH_INTERVAL']}"
    retry_forever "#{ENV['OUTPUT_BUFFER_RETRY_FOREVER']}"
    retry_max_interval "#{ENV['OUTPUT_BUFFER_RETRY_MAX_INTERVAL']}"
    chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
    total_limit_size "#{ENV['OUTPUT_BUFFER_TOTAL_LIMIT_SIZE']}"
    overflow_action "#{ENV['OUTPUT_BUFFER_OVERFLOW_ACTION']}"

  00-system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
      <log>
        format json
      </log>
    </system>

  01-monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @id prometheus
      @type prometheus
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @id prometheus_monitor
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @id prometheus_output_monitor
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # Don't include prometheus_tail_monitor since this will cause number of metrics to increase indefinitely
    # https://github.com/fluent/fluent-plugin-prometheus/issues/20

  10-authlog.conf: |-
    # Tail the authlog
    <source>
      @id authlog
      @type tail
      path /var/log/auth.log
      pos_file /var/log/auth.pos
      pos_file_compaction_interval 72h
      tag authlog.*
      skip_refresh_on_startup true
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$/
      </parse>
    </source>

  11-kubeaudit.conf: |-
    # Tail the kubeaudit
    <source>
      @id kubeaudit
      @type tail
      path /var/log/kube-audit/kube-apiserver.log,/var/log/kubernetes/audit/kube-apiserver-audit.log
      pos_file /var/log/kube-audit/fluentd-kube-apiserver.pos
      pos_file_compaction_interval 72h
      tag kubeaudit.*
      read_from_head true
      skip_refresh_on_startup true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key stageTimestamp
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
      </parse>
    </source>

    # Jsonify responseObject and requestObject keys so that their
    # contents won't be dynamically mapped in Opensearch and instead be indexed
    # as json text.
    <filter kubeaudit.**>
      @id kube_api_audit_normalize
      @type record_transformer
      auto_typecast false
      enable_ruby true
      <record>
        responseObject ${record["responseObject"].nil? ? "none": record["responseObject"].to_json}
        requestObject ${record["requestObject"].nil? ? "none": record["requestObject"].to_json}
      </record>
    </filter>

  12-kubernetes.conf: |-
    # Tail the kubernetes container logs

    # This config is taken from a default config that we have disabled
    # See the value "configMaps.useDefaults.containersInputConf: false" above
    # But we added "reserve_time true" in order to allow falco logs to use json
    <source>
      @id kubernetes
      @type tail
      path /var/log/containers/*.log
      exclude_path ["/var/log/containers/fluentd*"]
      pos_file /var/log/containers.log.pos
      pos_file_compaction_interval 72h
      tag raw.kubernetes.*
      read_from_head true
      skip_refresh_on_startup true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>

    # Concatenate multi-line logs
    {{- if eq .Values.global.containerRuntime "docker" }}
    <filter **>
      @id filter_concat
      @type concat
      key log
      multiline_end_regexp /\n$/
      separator ""
    </filter>
    {{- else if eq .Values.global.containerRuntime "containerd" }}
    <filter **>
      @id filter_concat
      @type concat
      key message
      use_first_timestamp true
      partial_key logtag
      partial_value P
      separator ""
      # TODO: When we have updated this plugin to v2.5.0 change to this instead
      # @id filter_concat
      # @type concat
      # key message
      # use_partial_cri_logtag true
      # partial_cri_logtag_key logtag
      # partial_cri_stream_key stream
    </filter>
    {{- else }}
      {{ fail "Misconfigured `global.containerRuntime`. Supported container runtimes are 'containerd' and 'docker'" }}
    {{- end }}

    # Fixes json fields in Elasticsearch
    <filter kubernetes.**>
      @id filter_parser
      @type parser
      key_name log
      reserve_data true
      reserve_time true # This is the line that is changed from the default config
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id filter_kubernetes_metadata
      @type kubernetes_metadata
    </filter>

  20-filters.conf: |-
    <filter **>
      @type record_modifier
      char_encoding utf-8:utf-8
    </filter>

    <filter **>
      @type record_transformer
      <record>
        cluster.name "{{ .Values.global.clusterName }}"
      </record>
    </filter>

    <filter kubernetes.**>
      @type dedot
      de_dot true
      de_dot_separator _
      de_dot_nested true
    </filter>

    # count the number of incoming records per tag
    <filter **>
      @type prometheus
      @id filter_prometheus
      <metric>
        name fluentd_input_status_num_records_total
        type counter
        desc The total number of incoming records
        <labels>
          tag ${tag_parts[0]}
          hostname ${hostname}
          namespace $.kubernetes.namespace_name
        </labels>
      </metric>
    </filter>
