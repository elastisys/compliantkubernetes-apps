falco:
  json_output: true
  syscall_event_drops:
    # Should be removed when fixed. Maybe in 2024 https://github.com/falcosecurity/falco/issues/2657
    actions:
      - ignore
  grpc:
    enabled: true
    bind_address: "unix:///var/run/falco/falco.sock"
    threadiness: 8

  grpc_output:
    enabled: true

  rules_file:
    {{- if eq .Values.falco.rulesFiles.default.enabled true }}
    - /etc/falco/falco_rules.yaml
    {{end}}
    {{- if eq .Values.falco.rulesFiles.incubating.enabled true }}
    - /etc/falco/falco-incubating_rules.yaml
    {{end}}
    {{- if eq .Values.falco.rulesFiles.sandbox.enabled true }}
    - /etc/falco/falco-sandbox_rules.yaml
    {{end}}
    - /etc/falco/falco_rules.local.yaml
    - /etc/falco/rules.d

tty: {{ .Values.falco.tty }}

{{- if eq .Values.falco.driver.kind "module" }}
containerSecurityContext:
  privileged: true
  allowPrivilegeEscalation: true
{{- end }}

podSecurityContext:
  runAsUser: 0

driver:
  kind: {{ .Values.falco.driver.kind }}
  {{- if eq .Values.falco.driver.kind "ebpf" }}
  ebpf:
    path: {{ .Values.falco.driver.ebpf.path }}
    hostNetwork: {{ .Values.falco.driver.ebpf.hostNetwork }}
    leastPrivileged: true
  {{- else if eq .Values.falco.driver.kind "modern-bpf" }}
  modern_bpf:
    leastPrivileged: true
  {{- end }}
  loader:
    initContainer:
      securityContext:
        runAsUser: 0
        privileged: true
        allowPrivilegeEscalation: true
      {{- with .Values.falco.driver.module.repoURL }}
      env:
      - name: FALCOCTL_DRIVER_NAME
        value: {{ . }}
      {{- end }}

resources: {{- toYaml .Values.falco.resources | nindent 2  }}
nodeSelector: {{- toYaml .Values.falco.nodeSelector | nindent 2  }}
affinity: {{- toYaml .Values.falco.affinity | nindent 2  }}
tolerations: {{- toYaml .Values.falco.tolerations | nindent 2  }}

falcoctl:
  artifact:
    install:
      enabled: {{ .Values.falco.artifact.install.enabled }}
    follow:
      enabled: false
  config:
    artifact:
      install:
        refs:
          {{- if eq .Values.falco.rulesFiles.default.enabled true }}
          - falco-rules:{{.Values.falco.rulesFiles.default.version}}
          {{end}}
          {{- if eq .Values.falco.rulesFiles.incubating.enabled true }}
          - falco-incubating-rules:{{.Values.falco.rulesFiles.incubating.version}}
          {{end}}
          {{- if eq .Values.falco.rulesFiles.sandbox.enabled true }}
          - falco-sandbox-rules:{{.Values.falco.rulesFiles.sandbox.version}}
          {{end}}
    {{- with .Values.falco.customIndexes }}
    indexes:
      {{- toYaml . | nindent 6 }}
    {{- end }}
customRules:
  {{- if .Values.falco.customRules }}
    {{ toYaml .Values.falco.customRules | nindent 2}}
  {{- end }}
  overwrites.yaml: |-
    {{- if eq  .Values.falco.rulesFiles.default.enabled true }}
    # This will be added in a later falco rules version as well
    # The fix was added upstream here (with a new condition): https://github.com/falcosecurity/rules/pull/177
    - macro: allowed_clear_log_files
      condition: (
        proc.name=containerd and (
          fd.name startswith "/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/" or
          fd.name startswith "/var/lib/containerd/tmpmounts/"
        ))
    # Adding a repository to this list will add an exception to the rules:
    # Run shell untrusted
    # Contact K8S API Server From Container
    - list: trusted_image_repositories
      items: [
        docker.io/elastisys/curl-jq,
        docker.io/jaegertracing/jaeger-operator,
        docker.io/library/rabbitmq,
        docker.io/openpolicyagent/gatekeeper,
        docker.io/rabbitmqoperator/cluster-operator,
        docker.io/velero/velero,
        gcr.io/k8s-staging-multitenancy/hnc-manager,
        ghcr.io/aquasecurity/trivy-operator,
        ghcr.io/elastisys/argocd-managed-namespaces-manager,
        ghcr.io/elastisys/fluentd,
        ghcr.io/elastisys/logical-backup,
        ghcr.io/zalando/spilo-15,
        quay.io/argoproj/argocd,
        quay.io/calico/node,
        quay.io/jetstack/cert-manager-controller,
        quay.io/jetstack/cert-manager-webhook,
        quay.io/kiwigrid/k8s-sidecar,
        quay.io/prometheus/prometheus,
        registry.k8s.io/ingress-nginx/controller-chroot,
        registry.k8s.io/ingress-nginx/controller,
        registry.k8s.io/kube-state-metrics/kube-state-metrics
        ]
    # Contact K8S API Server From Container
    - macro: k8s_containers
      append: true
      condition: or (
          container.image.repository in (trusted_image_repositories)
        ) or (
          proc.cmdline = "kubectl get rolebindings --all-namespaces -o json"
        ) or (
          proc.cmdline startswith "kubectl patch secret -n argocd-system argocd-manager-config -p"
        )
    # Run shell untrusted
    - macro: user_shell_container_exclusions
      condition: ( container.image.repository in (trusted_image_repositories) )

    # Read sensitive file trusted after startup
    - macro: user_known_read_sensitive_files_activities
      condition: ( container.image.repository = ghcr.io/zalando/spilo-15 )

    # Redirect STDOUT/STDIN to Network Connection in Container
    - macro: user_known_stand_streams_redirect_activities
      condition: (
          (container.image.repository = quay.io/calico/node and proc.name = calico-node) or
          (container.image.repository = registry.k8s.io/dns/k8s-dns-node-cache and proc.name = node-cache)
        )
    {{- end }}

    {{- if eq .Values.falco.rulesFiles.incubating.enabled true }}
    - rule: Change namespace privileges via unshare
      append: true
      condition: and not (
          (k8s.ns.name = kube-system and
          k8s.pod.name startswith calico-node) or
          (k8s.ns.name = monitoring and
          k8s.pod.name startswith prometheus-kube-prometheus-stack-prometheus)
        )
    # Non sudo setuid
    - macro: user_known_non_sudo_setuid_conditions
      append: true
      condition: or container.image.repository in (
          quay.io/prometheuscommunity/elasticsearch-exporter,
          docker.io/jaegertracing/jaeger-query,
          docker.io/library/redis,
          quay.io/argoproj/argocd
        ) or (
          container.id = host
        )
    # Contact EC2 Instance Metadata Service From Container
    - macro: ec2_metadata_containers
      condition: (
          container.image.repository = ghcr.io/zalando/spilo-15 or
          container.image.repository = gcr.io/k8s-staging-multitenancy/hnc-manager or
          container.image.repository = docker.io/k8scloudprovider/cinder-csi-plugin
        )
    # Launch Package Management Process in Container
    - macro: user_known_package_manager_in_container
      condition: (
          container.image.repository = ghcr.io/elastisys/fluentd
        )
    # Set Setuid or Setgid bit
    - macro: user_known_set_setuid_or_setgid_bit_conditions
      condition: (
          container.image.repository = ghcr.io/zalando/spilo-15 or
          container.image.repository = quay.io/calico/cni or
          proc.name = containerd
        )
    # Unexpected UDP Traffic
    - macro: expected_udp_traffic
      append: true
      condition: or container.image.repository in (
          docker.io/bitnami/fluentd,
          ghcr.io/elastisys/compliantkubernetes-apps-log-manager,
          ghcr.io/elastisys/logical-backup,
          docker.io/elastisys/rabbitmqadmin,
          ghcr.io/zalando/spilo-15
        ) or (
          proc.pname = systemd and proc.name = check-new-relea
        ) or (
          proc.name = iptables and user.name = root
        )
    # System procs network activity
    - macro: user_expected_system_procs_network_activity_conditions
      condition: (
          container.image.repository = docker.io/library/redis or
          container.image.repository = quay.io/calico/cni
        )
    # Launch Privileged Container
    - macro: user_privileged_containers
      condition: container.image.repository in (
          ghcr.io/elastisys/logical-backup,
          quay.io/calico/pod2daemon-flexvol,
          ghcr.io/elastisys/calico-accountant,
          docker.io/k8scloudprovider/cinder-csi-plugin,
          quay.io/calico/cni,
          docker.io/velero/velero
        )
    # DB program spawned process
    - macro: user_known_db_spawned_processes
      condition: (
          container.image.repository = ghcr.io/zalando/spilo-15
        )
    # Schedule Cron Jobs
    - macro: user_known_cron_jobs
      condition: ( container.image.repository = ghcr.io/zalando/spilo-15 )
    {{- end }}
    {{- if eq .Values.falco.rulesFiles.sandbox.enabled true }}
    # Decoding Payload in Container
    - list: known_decode_payload_containers
      append: true
      items: [ghcr.io/aquasecurity/trivy]

    # Image repository exception for rule BPF Program Not Profiled
    - list: bpf_profiled_binaries
      append: true
      items: [systemd]
    # Mkdir binary dirs
    - macro: user_known_mkdir_bin_dir_activities
      condition: (
          container.image.repository = registry.k8s.io/kube-proxy
        )
    # Modify binary dirs
    - macro: user_known_modify_bin_dir_activitiess
      condition: (
          container.image.repository = registry.k8s.io/kube-proxy
        )
    # Kubernetes Client Tool Launched in Container
    - macro: user_known_k8s_client_container
      append: true
      condition: or (
          container.image.repository = ghcr.io/elastisys/argocd-managed-namespaces-manager or
          container.image.repository = ghcr.io/zalando/spilo-15
        ) or (
          proc.cmdline = "kubectl get rolebindings --all-namespaces -o json"
        )
    # Write below etc
    - macro: user_known_write_etc_conditions
      append: true
      condition: or (
          container.image.repository = quay.io/prometheus-operator/prometheus-config-reloader or
          container.image.repository = quay.io/calico/node or
          container.image.repository = ghcr.io/zalando/spilo-15
        )
    # Write below root
    - macro: user_known_write_below_root_activities
      condition: (
          container.image.repository = ghcr.io/elastisys/logical-backup
        )
    # Launch Sensitive Mount Container
    - macro: user_sensitive_mount_containers
      condition: (
          container.image.repository = quay.io/prometheus/node-exporter or
          container.image.repository = docker.io/k8scloudprovider/cinder-csi-plugin
        )
    # Mount Launched in Privileged Container
    - macro: user_known_mount_in_privileged_containers
      condition: (
          container.image.repository = docker.io/k8scloudprovider/cinder-csi-plugin
        )
    # Contact cloud metadata service from container
    - macro: user_known_metadata_access
      append: true
      condition: or (
          container.image.repository = gcr.io/k8s-staging-multitenancy/hnc-manager or
          container.image.repository = ghcr.io/zalando/spilo-15
        )
    {{- end }}

falcosidekick:
  enabled: {{ .Values.falco.alerts.enabled }}
  config:
    debug: false
    {{ if eq .Values.falco.alerts.type "slack" }}
    slack:
      webhookurl: {{ .Values.falco.alerts.slackWebhook }}
      outputformat: "all"
      footer: ""
      icon: ""
      minimumpriority: {{ .Values.falco.alerts.priority }}
      messageformat: "Falco Alert : rule *{{`{{ .Rule }}`}}*"
    {{ end }}

  resources:    {{- toYaml .Values.falco.falcoSidekick.resources | nindent 4  }}
  nodeSelector: {{- toYaml .Values.falco.falcoSidekick.nodeSelector | nindent 4  }}
  affinity:     {{- toYaml .Values.falco.falcoSidekick.affinity | nindent 4  }}
  tolerations:  {{- toYaml .Values.falco.falcoSidekick.tolerations | nindent 4  }}
