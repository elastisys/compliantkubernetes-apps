aggregator:
  configMaps:
    fluentd.conf: |
      # Prometheus Exporter Plugin
      # input plugin that exports metrics
      <source>
        @id prometheus
        @type prometheus
      </source>

      # input plugin that collects metrics from MonitorAgent
      <source>
        @id prometheus_monitor
        @type prometheus_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # input plugin that collects metrics for output plugin
      <source>
        @id prometheus_output_monitor
        @type prometheus_output_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # Don't include prometheus_tail_monitor since this will cause number of metrics to increase indefinitely
      # https://github.com/fluent/fluent-plugin-prometheus/issues/20

      # TCP input to receive logs from the forwarders
      <source>
        @type forward
        bind 0.0.0.0
        port 24224
      </source>

      # HTTP input for the liveness and readiness probes
      <source>
        @type http
        bind 0.0.0.0
        port 9880
      </source>

      <match kubernetes.var.log.containers.fluentd-**>
        @type null
      </match>
      <match **>
        @id output-s3
        {{ if eq .Values.objectStorage.type "gcs" -}}
        @type gcs

        project {{ .Values.objectStorage.gcs.project }}
        keyfile /etc/fluent/config.d/gcp_credentials.json
        bucket {{ .Values.objectStorage.buckets.scFluentd }}
        object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
        {{ else if eq .Values.objectStorage.type "s3" -}}
        @type s3

        aws_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
        aws_sec_key "#{ENV['AWS_ACCESS_SECRET_KEY']}"
        {{ if eq .Values.fluentd.forwarder.useRegionEndpoint true -}}
        s3_endpoint {{ .Values.objectStorage.s3.regionEndpoint }}
        {{ end -}}
        force_path_style {{ .Values.objectStorage.s3.forcePathStyle }}
        s3_region {{ .Values.objectStorage.s3.region }}
        s3_bucket {{ .Values.objectStorage.buckets.scFluentd }}
        {{- end }}

        path logs/%Y%m%d/${tag}/

        <buffer tag,time>
          @type file

          path /opt/bitnami/fluentd/logs/buffers/s3
          overflow_action block
          total_limit_size 9GB

          timekey 10m
          timekey_wait 1m
          timekey_use_utc true
          chunk_limit_size 50MB
        </buffer>
      </match>
    {{ if eq .Values.objectStorage.type "gcs" -}}
    gcp_credentials.json: |
      {{ .Values.objectStorage.gcs.keyfileData | nindent 6 }}
    {{- end }}

  plugins:
    enabled: true
    pluginsList:
      {{ if eq .Values.objectStorage.type "s3" -}}
      # TODO: pin version
      - fluent-plugin-s3
      {{ else if eq .Values.objectStorage.type "gcs" -}}
      - digest-crc -v "0.5.1"
      - fluent-plugin-gcs
      {{- end }}


forwarder:
  configMaps:
    fluentd.conf: |
      # Prometheus Exporter Plugin
      # input plugin that exports metrics
      <source>
        @id prometheus
        @type prometheus
      </source>

      # input plugin that collects metrics from MonitorAgent
      <source>
        @id prometheus_monitor
        @type prometheus_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # input plugin that collects metrics for output plugin
      <source>
        @id prometheus_output_monitor
        @type prometheus_output_monitor
        <labels>
          host ${hostname}
        </labels>
      </source>

      # Don't include prometheus_tail_monitor since this will cause number of metrics to increase indefinitely
      # https://github.com/fluent/fluent-plugin-prometheus/issues/20

      # Throw the healthcheck to the standard output instead of forwarding it
      <match fluentd.healthcheck>
        @type stdout
      </match>

      # HTTP input for the liveness and readiness probes
      <source>
        @type http
        bind 0.0.0.0
        port 9880
      </source>

      <system>
        root_dir /tmp/fluentd-buffers/
        <log>
          format json
        </log>
      </system>

      <source>
        @id kube-audit
        @type tail
        path /var/log/kube-audit/kube-apiserver.log,/var/log/kubernetes/audit/kube-apiserver-audit.log
        pos_file /var/log/audit/fluentd-kube-apiserver.pos
        pos_file_compaction_interval 72h
        tag kubeaudit.*
        read_from_head true
        skip_refresh_on_startup true
        enable_watch_timer false
        <parse>
          @type multi_format
          <pattern>
            format json
            time_key requestReceivedTimestamp
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </pattern>
        </parse>
      </source>

      # Remove keys that include raw data causing errors
      <filter kubeaudit.**>
        @id kube_api_audit_normalize
        @type record_transformer
        remove_keys responseObject,requestObject
      </filter>


      #Added "reserve_time true" in order to allow falco logs to use json
      <source>
        @id fluentd-containers.log
        @type tail
        path /var/log/containers/*.log
        pos_file /var/log/containers.log.pos
        pos_file_compaction_interval 72h
        tag raw.kubernetes.*
        read_from_head true
        skip_refresh_on_startup true
        enable_watch_timer false
        <parse>
          @type multi_format
          <pattern>
            format json
            time_key time
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </pattern>
          <pattern>
            format /^(?<time>.+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$/
            time_format %Y-%m-%dT%H:%M:%S.%N%:z
          </pattern>
        </parse>
      </source>

      # Detect exceptions in the log output and forward them as one log entry.
      <match raw.kubernetes.**>
        @id raw.kubernetes
        @type detect_exceptions
        remove_tag_prefix raw
        message log
        stream stream
        multiline_flush_interval 5
        max_bytes 500000
        max_lines 1000
      </match>

      {{- if eq .Values.global.containerRuntime "docker" }}
      # Concatenate multi-line logs
      <filter **>
        @id filter_concat
        @type concat
        key message
        multiline_end_regexp /\n$/
        separator ""
      </filter>
      {{- else if eq .Values.global.containerRuntime "containerd" }}
      # Concatenate multi-line logs
      <filter **>
        @id filter_concat
        @type concat
        key message
        use_first_timestamp true
        partial_key logtag
        partial_value P
        separator ""
        # TODO: When we have updated this plugin to v2.5.0 change to this instead
        # @id filter_concat
        # @type concat
        # key message
        # use_partial_cri_logtag true
        # partial_cri_logtag_key logtag
        # partial_cri_stream_key stream
      </filter>
      {{- else }}
        {{ fail "Misconfigured `global.containerRuntime`. Supported container runtimes are 'containerd' and 'docker'" }}
      {{- end }}

      # Enriches records with Kubernetes metadata
      <filter kubernetes.**>
        @id filter_kubernetes_metadata
        @type kubernetes_metadata
      </filter>

      # Fixes json fields in Elasticsearch
      <filter kubernetes.**>
        @id filter_parser
        @type parser
        key_name log
        reserve_data true
        reserve_time true #This is the line that is changed from the default config
        remove_key_name_field true
        <parse>
          @type multi_format
          <pattern>
            format json
          </pattern>
          <pattern>
            format none
          </pattern>
        </parse>
      </filter>

      <filter **>
        @type record_transformer
        <record>
          cluster.name "{{ .Values.global.clusterName }}"
        </record>
      </filter>

      <match **>
        @id output-forwarding
        @type forward
        send_timeout 60s
        recover_wait 10s
        hard_timeout 60s
        <server>
          name aggregator
          host  fluentd-aggregator
          port 24224
          weight 60
        </server>
        #<secondary>
        #  @type file
        #  path /var/log/fluentd/forward-failed
        #</secondary>
        <buffer>
          @type file
          path  /var/log/fluentd-buffers
          flush_mode interval
          retry_type exponential_backoff
          flush_thread_count 2
          flush_interval 5s
          retry_forever
          retry_max_interval 30
          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
          total_limit_size "#{ENV['OUTPUT_BUFFER_TOTAL_LIMIT_SIZE']}"
          overflow_action block
        </buffer>
      </match>

  plugins:
    enabled: false
