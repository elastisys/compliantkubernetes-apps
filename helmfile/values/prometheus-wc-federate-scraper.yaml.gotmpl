prometheusSpec:
  version: "v2.19.2"
  alerting:
    # Use the default alertmanager that comes with prometheus-operator
    alertmanagers:
    - name: prometheus-operator-alertmanager
      namespace: monitoring
      pathPrefix: /
      port: web
  resources: {{- toYaml .Values.prometheus.wcScraper.resources | nindent 4  }}
  nodeSelector: {{- toYaml .Values.prometheus.wcScraper.nodeSelector | nindent 4 }}
  affinity: {{- toYaml .Values.prometheus.wcScraper.affinity | nindent 4 }}
  tolerations: {{- toYaml .Values.prometheus.wcScraper.tolerations | nindent 4 }}

    ## How long to retain metrics
    ##
  retention: {{ .Values.prometheus.retention.age }}

  ## Maximum size of metrics
  ##
  retentionSize: {{ .Values.prometheus.retention.size }}

  ## Prometheus StorageSpec for persistent data
  ## ref: https://github.com/coreos/prometheus-operator/blob/release-0.29/Documentation/user-guides/storage.md
  ##
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: {{ .Values.global.storageClass }}
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: {{ .Values.prometheus.storage.size }}

  enableAdminAPI: false
  logFormat: logfmt
  logLevel: info
  remoteRead:
  - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/read?db=workload_cluster&u=admin&p={{ .Values.influxDB.password }}
  remoteWrite:
  - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/write?db=workload_cluster&u=admin&p={{ .Values.influxDB.password }}
  replicas: 1
  retention: 10d
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-operator-prometheus
  serviceMonitorSelector:
    matchLabels:
      target: none  # so that operator wont generate new config
  ruleNamespaceSelector: {}
  ruleSelector:
    matchLabels:
      cluster: workload

  externalLabels:
    scraper: wc-scraper

  # Don't add prometheus info.
  prometheusExternalLabelName: ""
  replicaExternalLabelName: ""

scrapeConfig:
#The idea with splitting the federation in two is to (hopefully) reduce the resources needed for prometheus.
#If I have understood it correctly prometheus will use more resources (primarily memory) if you have larger scrapes.
#So if we scrape everything from another prometheus it will be a large job where prometheus might run out of memory.
#But splitting them might reduce the memory needed.
#The kubelet is roughly half of the metrics, so these two scrapes will be roughly the same size.
- job_name: 'federate-kubelet'
  scrape_interval: 30s

  honor_labels: true
  metrics_path: '/federate'

  scheme: https

  params:
    'match[]':
      - '{job="kubelet"}'
  basic_auth:
    username: prometheus
    password: {{ .Values.user.prometheusPassword }}

  tls_config:
    insecure_skip_verify: {{ not .Values.global.verifyTls }}

  static_configs:
    - targets:
      - prometheus.{{ .Values.global.baseDomain }}

- job_name: 'federate-others'
  scrape_interval: 30s

  honor_labels: true
  metrics_path: '/federate'

  scheme: https

  params:
    'match[]':
      # Match all non empty job labels except "kubelet"
      - '{job!="kubelet", job=~".+"}'
  basic_auth:
    username: prometheus
    password: {{ .Values.user.prometheusPassword }}

  tls_config:
    insecure_skip_verify: {{ not .Values.global.verifyTls }}

  static_configs:
    - targets:
      - prometheus.{{ .Values.global.baseDomain }}
