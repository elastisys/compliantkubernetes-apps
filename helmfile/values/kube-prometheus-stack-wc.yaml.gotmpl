defaultRules:
  # Provided by prometheus-alerts instead
  create: false

alertmanager:
  enabled: false

grafana:
  enabled: false

kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

prometheusOperator:
  resources: {{- toYaml .Values.prometheusOperator.resources | nindent 4 }}

  prometheusConfigReloader:
    resources: {{- toYaml .Values.prometheusOperator.prometheusConfigReloader.resources | nindent 6 }}

kube-state-metrics:
  resources: {{- toYaml .Values.kubeStateMetrics.resources | nindent 4 }}
  {{- if .Values.kubeStateMetrics.clusterAPIMetrics.enabled }}
  rbac:
    extraRules:
      - apiGroups:
          - cluster.x-k8s.io
        resources:
          - clusters
          - machinedeployments
          - machinesets
          - machines
          - machinehealthchecks
          - machinepools
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - controlplane.cluster.x-k8s.io
        resources:
          - kubeadmcontrolplanes
        verbs:
          - get
          - list
          - watch
  collectors:
    - certificatesigningrequests
    - configmaps
    - cronjobs
    - daemonsets
    - deployments
    - endpoints
    - horizontalpodautoscalers
    - ingresses
    - jobs
    - limitranges
    - mutatingwebhookconfigurations
    - namespaces
    - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    - poddisruptionbudgets
    - pods
    - replicasets
    - replicationcontrollers
    - resourcequotas
    - secrets
    - services
    - statefulsets
    - storageclasses
    - validatingwebhookconfigurations
    - volumeattachments
    - clusters
    - machinedeployments
    - machinesets
    - machines
    - machinehealthchecks
    - kubeadmcontrolplanes
  volumeMounts:
    - mountPath: /etc/config
      name: kube-state-metrics-clusterapi-volume
  volumes:
    - configMap:
        name: kube-state-metrics-clusterapi
      name: kube-state-metrics-clusterapi-volume
  extraArgs:
    - "--custom-resource-state-config-file=/etc/config/clusterapi-metrics.yaml"
  {{- end }}

prometheus-node-exporter:
  resources: {{- toYaml .Values.prometheusNodeExporter.resources | nindent 4 }}
  prometheus:
    monitor:
      relabelings:
        - action: replace
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: instance

kubeApiServer:
  serviceMonitor:
    metricRelabelings:
      - action: keep
        sourceLabels: [__name__]
        regex: '(process_cpu_seconds_total|process_resident_memory_bytes|go_goroutines|workqueue_queue_duration_seconds_bucket|workqueue_adds_total|workqueue_depth|apiserver_request_total|apiserver_request_count|apiserver_request_latencies_bucket|apiserver_client_certificate_expiration_seconds_count|apiserver_request_duration_seconds_sum|apiserver_request_duration_seconds_count|APIServiceRegistrationController_depth)'

kubeEtcd:
  service:
    port: 4001
    targetPort: 4001

prometheus:
  ingress:
    enabled: false

  prometheusSpec:
    replicas: {{ .Values.prometheus.replicas }}
    topologySpreadConstraints: {{- toYaml .Values.prometheus.topologySpreadConstraints | nindent 6 }}

    externalLabels:
      cluster: {{ .Values.global.clusterName }}

    # Dont add prometheus labels.
    prometheusExternalLabelNameClear: true
    {{- if le .Values.prometheus.replicas 1 }}
    replicaExternalLabelNameClear: true
    {{- end }}

    {{- if and .Values.thanos.enabled .Values.thanos.receiver.enabled }}
    remoteWrite:
    - url: https://{{ .Values.thanos.receiver.subdomain }}.{{ .Values.global.opsDomain }}./api/v1/receive
      tlsConfig:
        insecureSkipVerify: {{ not .Values.global.verifyTls }}
      headers:
        THANOS-TENANT: {{ .Values.global.clusterName }}
      basicAuth:
        username:
          key: username
          name: thanos-ingress-secret-basic-auth
        password:
          key: password
          name: thanos-ingress-secret-basic-auth
    {{- end }}


    # Select everything
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false


    {{ if .Values.user.alertmanager.enabled }}
    # Connect to separately managed alertmanager
    alertingEndpoints:
      - name: alertmanager-operated
        namespace: alertmanager
        port: http-web
        pathPrefix: /
    {{ end }}

    resources:    {{- toYaml .Values.prometheus.resources | nindent 6 }}
    nodeSelector: {{- toYaml .Values.prometheus.nodeSelector | nindent 6 }}
    affinity:     {{- toYaml .Values.prometheus.affinity | nindent 6 }}
    tolerations:  {{- toYaml .Values.prometheus.tolerations | nindent 6 }}

    ## How long to retain metrics
    ##
    retention: {{ .Values.prometheus.retention.age }}

    ## Maximum size of metrics
    ##
    retentionSize: {{ .Values.prometheus.retention.size }}

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/release-0.29/Documentation/user-guides/storage.md
    ##
    {{- if .Values.prometheus.storage.enabled }}
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: {{ .Values.storageClasses.default }}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: {{ .Values.prometheus.storage.size }}
    {{- end }}

    additionalScrapeConfigs:

    {{ if .Values.user.alertmanager.enabled }}
    - job_name: 'alertmanager/alertmanager-operated'
      scrape_interval: 30s
      metrics_path: /metrics
      scheme: http
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - alertmanager
        selectors:
          - role: endpoints
            label: 'operated-alertmanager=true'
      relabel_configs:
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          action: keep
          regex: http-web # keep only the web port (9093 by default) and drop the remaining (for example 9094)
        - source_labels: [__meta_kubernetes_endpoints_name]
          action: replace
          target_label: job # needed to make Prometheus realize that it is the alertmanager
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: namespace # needed to make Prometheus realize that it is the alertmanager
        # The remaining items are to keep the set of labels for this target consistent with the other targets.
        # Without replacing them, the labels with __ prefix are dropped.
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          action: replace
          target_label: endpoint
        - source_labels: [__address__]
          action: replace
          target_label: instance
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: service
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: pod
    {{ end }}

    {{- with .Values.prometheus.additionalScrapeConfigs }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
