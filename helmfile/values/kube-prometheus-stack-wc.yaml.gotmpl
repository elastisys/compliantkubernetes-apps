defaultRules:
  create: true
  rules:
    # disabled rules from kube-prometheus-stack, but installed using charts/prometheus-alerts
    alertmanager: false
    # disabled rules from kube-prometheus-stack and not installed using another chart
    etcd: false
    kubeScheduler: false

global:
  rbac:
    pspEnabled: true

alertmanager:
  enabled: false

grafana:
  enabled: false

kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

prometheusOperator:
  createCustomResource: false
  resources: {{- toYaml .Values.prometheusOperator.resources | nindent 4 }}

kube-state-metrics:
  podSecurityPolicy:
    enabled: true
  resources: {{- toYaml .Values.kubeStateMetrics.resources | nindent 4 }}

prometheus-node-exporter:
  resources: {{- toYaml .Values.prometheusNodeExporter.resources | nindent 4 }}

kubeApiServer:
  serviceMonitor:
    metricRelabelings:
      - action: keep
        sourceLabels: [__name__]
        regex: '(process_cpu_seconds_total|process_resident_memory_bytes|go_goroutines|workqueue_queue_duration_seconds_bucket|workqueue_adds_total|workqueue_depth|apiserver_request_total|apiserver_request_count|apiserver_request_latencies_bucket|apiserver_client_certificate_expiration_seconds_count|apiserver_request_duration_seconds_sum|apiserver_request_duration_seconds_count|apiserver_request_duration_seconds_bucket|APIServiceRegistrationController_depth)'

kubeEtcd:
  service:
    port: 4001
    targetPort: 4001

prometheus:
  ingress:
    enabled: false

  prometheusSpec:
    externalLabels:
      cluster: {{ .Values.global.clusterName }}

    remoteWrite:
    - url: https://{{ .Values.influxDB.subdomain }}.{{ .Values.global.opsDomain }}/api/v1/prom/write?db=workload_cluster&u={{ .Values.prometheus.remoteWrite.user }}&p={{ .Values.prometheus.remoteWrite.password }}
      tlsConfig:
        insecureSkipVerify: {{ not .Values.global.verifyTls }}

    # Dont add prometheus labels.
    prometheusExternalLabelNameClear: true
    replicaExternalLabelNameClear: true

    # Select everything
    serviceMonitorNamespaceSelector:
      matchLabels: {}
    serviceMonitorSelector:
      matchLabels: {}
    podMonitorNamespaceSelector:
      matchLabels: {}
    podMonitorSelector:
      matchLabels: {}
    ruleNamespaceSelector:
      matchLabels: {}
    ruleSelector:
      matchLabels: {}

    {{ if .Values.user.alertmanager.enabled }}
    # Connect to separately managed alertmanager
    alertingEndpoints:
      - name: alertmanager-operated
        namespace: {{ .Values.user.alertmanager.namespace }}
        port: web
        pathPrefix: /
    {{ end }}

    resources:    {{- toYaml .Values.prometheus.resources | nindent 6 }}
    nodeSelector: {{- toYaml .Values.prometheus.nodeSelector | nindent 6 }}
    affinity:     {{- toYaml .Values.prometheus.affinity | nindent 6 }}
    tolerations:  {{- toYaml .Values.prometheus.tolerations | nindent 6 }}

    ## How long to retain metrics
    ##
    retention: {{ .Values.prometheus.retention.age }}

    ## Maximum size of metrics
    ##
    retentionSize: {{ .Values.prometheus.retention.size }}

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/release-0.29/Documentation/user-guides/storage.md
    ##
    {{- if .Values.prometheus.storage.enabled }}
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: {{ .Values.storageClasses.default }}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: {{ .Values.prometheus.storage.size }}
    {{- end }}

    additionalScrapeConfigs:
    {{ if .Values.storageClasses.nfs.enabled }}
    - job_name: 'node-exporter'
      scrape_interval: 30s
      metrics_path: /metrics
      scheme: http
      static_configs:
      - targets:
        - '{{ .Values.nfsProvisioner.server }}:9100'
    {{ end }}

    {{ if .Values.user.alertmanager.enabled }}
    - job_name: '{{ .Values.user.alertmanager.namespace }}/alertmanager-operated'
      scrape_interval: 30s
      metrics_path: /metrics
      scheme: http
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - {{ .Values.user.alertmanager.namespace }}
        selectors:
          - role: endpoints
            label: 'operated-alertmanager=true'
      relabel_configs:
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          action: keep
          regex: web # keep only the web port (9093 by default) and drop the remaining (for example 9094)
        - source_labels: [__meta_kubernetes_endpoints_name]
          action: replace
          target_label: job # needed to make Prometheus realize that it is the alertmanager
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: namespace # needed to make Prometheus realize that it is the alertmanager
        # The remaining items are to keep the set of labels for this target consistent with the other targets.
        # Without replacing them, the labels with __ prefix are dropped.
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          action: replace
          target_label: endpoint
        - source_labels: [__address__]
          action: replace
          target_label: instance
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: service
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: pod
    {{ end }}

    {{- with .Values.prometheus.additionalScrapeConfigs }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
