defaultRules:
  # Provided by prometheus-alerts instead
  create: false

global:
  rbac:
    pspEnabled: {{ .Values.kubernetesPSP }}

kube-state-metrics:
  podSecurityPolicy:
    enabled: {{ .Values.kubernetesPSP }}
  resources: {{- toYaml .Values.kubeStateMetrics.resources | nindent 4 }}

kubeApiServer:
  serviceMonitor:
    metricRelabelings:
      - action: keep
        sourceLabels: [__name__]
        regex: '(process_cpu_seconds_total|process_resident_memory_bytes|go_goroutines|workqueue_queue_duration_seconds_bucket|workqueue_adds_total|workqueue_depth|apiserver_request_total|apiserver_request_count|apiserver_request_latencies_bucket|apiserver_client_certificate_expiration_seconds_count|apiserver_request_duration_seconds_sum|apiserver_request_duration_seconds_count|APIServiceRegistrationController_depth)'

kubeEtcd:
  service:
    port: 4001
    targetPort: 4001

alertmanager:
  config:
    # See https://prometheus.io/docs/alerting/configuration/
    inhibit_rules: []
    route:
      group_by: {{ toYaml .Values.prometheus.alertmanagerSpec.groupBy | nindent 8 }}
      # default receiver
      receiver: '{{ .Values.alerts.alertTo }}'
      routes:
        {{ if .Values.alerts.opsGenieHeartbeat.enabled }}
        - receiver: heartbeat
        {{ else }}
        - receiver: 'null'
        {{ end }}
          # Repeat often to integreate with opsgenies heartbeat feture.
          # If this goes silent, opsgenie will send out alerts.
          repeat_interval: 1m
          group_interval: 1m
          matchers:
            # The watchdog alert is always active to show that things are working
            - alertname = Watchdog
        {{ if not .Values.user.alertmanager.enabled }}
        {{- range .Values.global.clustersMonitoring }}
        - receiver: 'null'
          matchers:
            - alertname =~ "AlertmanagerDown|PrometheusNotConnectedToAlertmanagers"
            - cluster = {{ . }}
        {{- end }}
        {{ end }}
        - receiver: 'null'
          matchers:
            - alertname =~ "CPUThrottlingHigh|KubeCPUOvercommit|KubeMemOvercommit"
    receivers:
    - name: 'null'
    - name: heartbeat
      {{ if .Values.alerts.opsGenieHeartbeat.enabled }}
      webhook_configs:
        - url: {{ .Values.alerts.opsGenieHeartbeat.url }}/{{ .Values.alerts.opsGenieHeartbeat.name }}/ping
          send_resolved: false
          http_config:
            basic_auth:
              # https://docs.opsgenie.com/docs/authentication
              # username: no username
              password: {{ .Values.alerts.opsGenie.apiKey }}
      {{ end }}
    - name: slack
      {{ if eq .Values.alerts.alertTo "slack" }}
      slack_configs:
      # Note: the channel here does not apply if the webhook URL is for a specific channel
      - channel: {{ .Values.alerts.slack.channel }}
        # Webhook URL for slack
        api_url: {{ .Values.alerts.slack.apiUrl }}
        send_resolved: true
        # Alertmanager templating: https://prometheus.io/docs/alerting/notifications/
        # We need to escape the templating brackets for alertmanager here with
        # {{``}} to prevent helm from parsing them.
        # See: https://github.com/helm/helm/issues/2798#issuecomment-467319526
        text: |-
          <!channel>
          *Cluster:* {{ .Values.prometheus.grafana.subdomain }}.{{ .Values.global.opsDomain }}
          {{`
          *Common summary:* {{ .CommonAnnotations.summary }}
          *Common description:* {{ .CommonAnnotations.description }}
          {{ range .CommonLabels.SortedPairs }}
          *{{ .Name }}:* {{ .Value }}
          {{ end }}

          *Individual alerts below*
          {{ range .Alerts }}
          *Status:* {{ .Status }}
          {{ range .Annotations.SortedPairs }}
          *{{ .Name }}:* {{ .Value }}
          {{ end }}
          {{ end }}`}}
      {{ end }}
    - name: opsgenie
      {{ if eq .Values.alerts.alertTo "opsgenie" }}
      opsgenie_configs:
        # See https://prometheus.io/docs/alerting/configuration/#opsgenie_config
        - api_key: {{ .Values.alerts.opsGenie.apiKey }}
          api_url: {{ .Values.alerts.opsGenie.apiUrl }}
          source: {{ .Values.user.grafana.subdomain }}.{{ .Values.global.opsDomain }}
          priority: {{`'{{ if eq .GroupLabels.severity "critical"}}P1{{else if eq .GroupLabels.severity "warning"}}P2{{else if eq .GroupLabels.severity "medium"}}P3{{else if eq .GroupLabels.severity "low"}}P4{{else}}P5{{end}}'`}}
      {{ end }}

  alertmanagerSpec:
    replicas: {{ .Values.prometheus.alertmanagerSpec.replicas }}
    retention: {{ .Values.prometheus.retention.alertmanager }}
    resources: {{- toYaml .Values.prometheus.alertmanagerSpec.resources | nindent 6 }}
    storage: {{- toYaml .Values.prometheus.alertmanagerSpec.storage | nindent 6 }}
    topologySpreadConstraints: {{- toYaml .Values.prometheus.alertmanagerSpec.topologySpreadConstraints | nindent 4 }}

kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false

grafana:
  image:
    tag: 9.3.8

  testFramework:
    enabled: false

  # admin username is "admin"
  adminPassword: {{ .Values.grafana.password }}

  defaultDashboardsTimezone: ""

  initChownData:
    enabled: false

  deploymentStrategy:
    type: Recreate

  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      ingress.kubernetes.io/rewrite-target: /
      cert-manager.io/cluster-issuer: {{ .Values.global.issuer }}
    {{ if and .Values.externalTrafficPolicy.local .Values.externalTrafficPolicy.whitelistRange.opsGrafana }}
      nginx.ingress.kubernetes.io/whitelist-source-range: {{ .Values.externalTrafficPolicy.whitelistRange.opsGrafana }}
    {{ end }}
    hosts:
    - {{ .Values.prometheus.grafana.subdomain }}.{{ .Values.global.opsDomain }}
    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    tls:
    - secretName: grafana-ops-general-tls
      hosts:
      - {{ .Values.prometheus.grafana.subdomain }}.{{ .Values.global.opsDomain }}
  rbac:
    pspUseAppArmor: false
    pspEnabled: {{ .Values.kubernetesPSP }}

  persistence:
    type: pvc
    enabled: true
    size: 10Gi
    accessModes:
      - ReadWriteOnce

  serviceAccount:
    create: true
    name:
    nameTest:

  sidecar:
    dashboards:
      labelValue: "" # should be set to "" to load all dashboards, regardless of the label value
      multicluster:
        global:
          enabled: true
    datasources:
      defaultDatasourceEnabled: false
    resources: {{- toYaml .Values.prometheus.grafana.sidecar.resources | nindent 6 }}

  additionalDataSources:
  - name: prometheus-sc
    access: proxy
    basicAuth: false
    editable: false
    isDefault: false
    orgId: 1
    type: prometheus
    url: http://kube-prometheus-stack-prometheus:9090
    version: 1
  {{- if and .Values.thanos.enabled .Values.thanos.query.enabled }}
  - name: "Thanos All"
    access: proxy
    basicAuth: false
    editable: false
    isDefault: true
    orgId: 1
    type: prometheus
    url: http://thanos-query-query-frontend.thanos:9090
    jsonData:
      prometheusType: Thanos
    version: 1
  - name: "Thanos SC Only"
    access: proxy
    basicAuth: false
    editable: false
    isDefault: false
    orgId: 1
    type: prometheus
    url: http://grafana-label-enforcer:9090
    jsonData:
      customQueryParameters: "tenant_id={{ .Values.global.clusterName }}"
      manageAlerts: false
    version: 1
  {{- range .Values.global.clustersMonitoring }}
  - name: "Thanos {{ . }} only"
    access: proxy
    basicAuth: false
    editable: false
    isDefault: false
    orgId: 1
    type: prometheus
    url: http://grafana-label-enforcer:9090
    version: 1
    jsonData:
      customQueryParameters: "tenant_id={{ . }}"
      manageAlerts: false
  {{- end }}
  {{- end }}

  grafana.ini:
    server:
      root_url: https://{{ .Values.prometheus.grafana.subdomain }}.{{ .Values.global.opsDomain }}
    {{ if .Values.prometheus.grafana.oidc.enabled -}}
    auth.generic_oauth:
      name: dex
      enabled: {{ .Values.prometheus.grafana.oidc.enabled }}
      client_id: grafana-ops
      client_secret: {{ .Values.grafana.opsClientSecret }}
      scopes: {{ .Values.prometheus.grafana.oidc.scopes }}
      auth_url: https://{{ .Values.dex.subdomain }}.{{ .Values.global.baseDomain }}/auth
      token_url: http://dex.dex.svc.cluster.local:5556/token
      api_url: http://dex.dex.svc.cluster.local:5556/api
      allowed_domains: {{ join " " .Values.prometheus.grafana.oidc.allowedDomains }}
      allow_sign_up: true
      tls_skip_verify_insecure: {{ not .Values.global.verifyTls }}
      role_attribute_path: contains(groups[*], '{{ .Values.prometheus.grafana.oidc.userGroups.grafanaAdmin }}') && 'Admin' || contains(groups[*], '{{ .Values.prometheus.grafana.oidc.userGroups.grafanaEditor }}') && 'Editor' || contains(groups[*], '{{ .Values.prometheus.grafana.oidc.userGroups.grafanaViewer }}') && 'Viewer'
      {{- end }}
    users:
      viewers_can_edit: {{ .Values.prometheus.grafana.viewersCanEdit }}
    analytics:
      reporting_enabled: false
      check_for_updates: false

  resources: {{- toYaml .Values.prometheus.grafana.resources | nindent 4 }}
  tolerations: {{- toYaml .Values.prometheus.grafana.tolerations | nindent 4 }}
  affinity: {{- toYaml .Values.prometheus.grafana.affinity | nindent 4 }}
  nodeSelector: {{- toYaml .Values.prometheus.grafana.nodeSelector | nindent 4 }}

prometheusOperator:
  resources: {{- toYaml .Values.prometheusOperator.resources | nindent 4 }}

  prometheusConfigReloader:
    resources: {{- toYaml .Values.prometheusOperator.prometheusConfigReloader.resources | nindent 6 }}

prometheus:
  prometheusSpec:
    replicas: {{ .Values.prometheus.replicas }}
    topologySpreadConstraints: {{- toYaml .Values.prometheus.topologySpreadConstraints | nindent 6 }}

    externalLabels:
      cluster: {{ .Values.global.clusterName }}

    # Don't add prometheus labels.
    prometheusExternalLabelNameClear: true
    {{- if le .Values.prometheus.replicas 1 }}
    replicaExternalLabelNameClear: true
    {{- end }}

    resources: {{- toYaml .Values.prometheus.resources | nindent 8  }}
    nodeSelector: {{- toYaml .Values.prometheus.nodeSelector | nindent 8 }}
    affinity: {{- toYaml .Values.prometheus.affinity | nindent 8 }}
    tolerations: {{- toYaml .Values.prometheus.tolerations | nindent 8 }}

    {{- if and .Values.thanos.enabled .Values.thanos.receiver.enabled }}
    remoteWrite:
    - url: http://thanos-receiver-receive.thanos.svc.cluster.local:19291/api/v1/receive
      headers:
        THANOS-TENANT: {{ .Values.global.clusterName }}
    {{- end }}

    # Empty selector to select all namespaces
    podMonitorNamespaceSelector: {}
    ruleNamespaceSelector: {}
    serviceMonitorNamespaceSelector: {}

    # Select all service monitors
    serviceMonitorSelector:
      matchLabels: {}
    # for the sc prometheus we want to select only the relevant rules for sc
    ruleSelector:
      matchLabels:
        evaluate_prometheus: "1"

    ## How long to retain metrics
    ##
    retention: {{ .Values.prometheus.retention.age }}

    ## Maximum size of metrics
    ##
    retentionSize: {{ .Values.prometheus.retention.size }}

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/release-0.29/Documentation/user-guides/storage.md
    ##
    {{- if .Values.prometheus.storage.enabled }}
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: {{ .Values.storageClasses.default }}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: {{ .Values.prometheus.storage.size }}
    {{- end }}

prometheus-node-exporter:
  resources: {{- toYaml .Values.prometheusNodeExporter.resources | nindent 4 }}
  rbac:
    pspEnabled: {{ .Values.kubernetesPSP }}
  prometheus:
    monitor:
      relabelings:
        - action: replace
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: instance
  securityContext:
    fsGroup: 3000
    runAsNonRoot: true
    runAsUser: 1000
    supplementalGroups: [2000]
