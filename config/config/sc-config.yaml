# This configuration applies to the service cluster.
# It will override settings set in "defaults/common-config.yaml".
global:
  clusterName: ${CK8S_ENVIRONMENT_NAME}-sc

  # Names of the workload clusters that sends metrics to this cluster.
  # Mainly used for filtering of metrics.
  clustersMonitoring:
    - ${CK8S_ENVIRONMENT_NAME}-wc

objectStorage:
  buckets:
    harbor: ${CK8S_ENVIRONMENT_NAME}-harbor
    opensearch: ${CK8S_ENVIRONMENT_NAME}-opensearch
    scFluentd: ${CK8S_ENVIRONMENT_NAME}-sc-logs

  ## Off-site backup replication between two providers or regions using rclone sync
  sync:
    enabled: false
    # dryrun: false

    ## Options are 's3'
    type: none
    # s3:
    #   region: set-me
    #   regionEndpoint: set-me
    #   # Generally false when using AWS and Exoscale and true for other providers.
    #   forcePathStyle: set-me
    #   # v2Auth: false

    ## Sync all buckets under 'objectStorage.buckets'
    ## These will be appended to 'buckets' using the same name from source as destination, and the default schedule.
    syncDefaultBuckets: false

    ## Default schedule for sync jobs
    defaultSchedule: 0 5 * * *

    ## Buckets to sync.
    buckets: []
      # - source: source-bucket
      #   # destination: destination-bucket # if unset it will be the same as 'source'
      #   # schedule: 0 5 * * *             # if unset it will be the same as 'defaultSchedule'

    ## Sync job resources
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 300m
        memory: 300Mi

user:
  grafana:
    enabled: true
    subdomain: grafana
    resources:
      limits:
        cpu: 100m
        memory: 160Mi
      requests:
        cpu: 50m
        memory: 80Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    oidc:
      scopes: profile email openid
      userGroups:
        grafanaAdmin: grafana_admin   # maps to grafana role admin
        grafanaEditor: grafana_editor # maps to grafana role editor
        grafanaViewer: grafana_viewer # maps to grafana role viewer
      allowedDomains:
        - set-me
        - example.com
    viewersCanEdit: true
    sidecar:
      resources:
        requests:
          cpu: 5m
          memory: 80Mi
        limits:
          cpu: 10m
          memory: 100Mi
harbor:
  enabled: true
  subdomain: harbor
  # The tolerations, affinity, and nodeSelector are applied to all harbor pods.
  tolerations: []
  affinity: {}
  nodeSelector: {}
  chartmuseum:
    replicas: 1
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
  core:
    replicas: 1
    resources:
      requests:
        cpu: 125m
        memory: 250Mi
      limits:
        cpu: 250m
        memory: 500Mi
  database:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        memory: 512Mi
        cpu: 250m
  jobservice:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
  registry:
    replicas: 1
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
    controller:
      resources:
        requests:
          cpu: 125m
          memory: 125Mi
        limits:
          cpu: 250m
          memory: 250Mi
  redis:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        memory: 32Mi
        cpu: 10m
  notary:
    replicas: 1
    subdomain: notary.harbor
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
  notarySigner:
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
  portal:
    replicas: 1
    resources:
      requests:
        cpu: 125m
        memory: 125Mi
      limits:
        cpu: 250m
        memory: 250Mi
  trivy:
    replicas: 1
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 125m
        memory: 263Mi
      limits:
        cpu: 250m
        memory: 512Mi
  persistence:
    # Valid options are "filesystem" (persistent volume), "swift", or "objectStorage" (matching global config)
    type: set-me
    disableRedirect: set-me
  oidc:
    # group claim name used by OIDC Provider
    groupClaimName: set-me
    # Name of the group that autmatically will get admin
    # Set to "" to disable
    adminGroupName: set-me
    scope: openid,email,profile,offline_access,groups
  backup:
    enabled: true
    retentionDays: 7

grafanaLabelEnforcer:
  resources:
    limits:
      cpu: 100m
      memory: 50Mi
    requests:
      cpu: 20m
      memory: 20Mi

prometheus:
  retention:
    alertmanager: 72h

  wcReader:
    resources:
      requests:
        memory: 1Gi
        cpu: 300m
      limits:
        memory: 2Gi
        cpu: "1"
    storage:
      enabled: false
      size: 5Gi
    retention:
      size: 4GiB
      age: 3d
    tolerations: []
    affinity: {}
    nodeSelector: {}

  # Todo remove dependencie on alertmanager from service cluster
  alertmanagerSpec:
    groupBy:
      - '...'
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

  grafana:
    subdomain: grafana
    resources:
      requests:
        cpu: 50m
        memory: 60Mi
      limits:
        cpu: 100m
        memory: 160Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    oidc:
      enabled: false
      # Only used if the above is true
      # userGroups:
      #   grafanaAdmin: grafana_admin   # maps to grafana role admin
      #   grafanaEditor: grafana_editor # maps to grafana role editor
      #   grafanaViewer: grafana_viewer # maps to grafana role viewer
      # scopes: "openid profile email groups"
      # allowedDomains: []
    viewersCanEdit: true

    sidecar:
      resources:
        requests:
          cpu: 5m
          memory: 80Mi
        limits:
          cpu: 10m
          memory: 100Mi
  ## Predictive alert if the resource usage will hit the set percentage in 3 days
  predictLinear:
    enabled: true
    limit: 66
  s3BucketPercentLimit: 80
  s3BucketSizeQuotaGB: 1000
dex:
  replicaCount: 2
  subdomain: dex
  additionalKubeloginRedirects: []
  enableStaticLogin: true
  serviceMonitor:
    enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 25Mi
  tolerations: []
  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app: dex
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
  affinity: {}
  nodeSelector: {}
  google:
    # Enables extra config needed to enable google connector to fetch group info.
    # When this is enabled the SASecretName needs to be set.
    groupSupport: false
    # Name of the secret that includes the key file for the service account that is used for fetching group info.
    # The secret will be mounted to the folder /etc/dex/google/ this means that multiple files from the same secret can be used.
    # Simply add `serviceAccountFilePath: /etc/dex/google/secret-key` for each google connector.
    # For more details, see https://elastisys.com/elastisys-engineering-how-to-use-dex-with-google-accounts-to-manage-access-in-kubernetes/
    # SASecretName: set-me

thanos:
  query:
    # Enables the query/query-frontend components
    enabled: true

    # Number of query pods
    # They are infinitly scalable
    replicaCount: 1

    resources:
      requests:
        cpu: 50m
        memory: 60Mi
      limits:
        cpu: 500m
        memory: 1Gi

  queryFrontend:
    resources:
      requests:
        cpu: 50m
        memory: 40Mi
      limits:
        cpu: 300m
        memory: 100Mi

  receiveDistributor:
    resources:
      requests:
        cpu: 150m
        memory: 200Mi
      limits:
        cpu: 500m
        memory: 300Mi

  ruler:
    # Enables the ruler component
    # Evaluates alert rules through thanos, record rules and thanos alert rules are still evaluated through prometheus.
    enabled: true

    replicaCount: 2

    resources:
      requests:
        cpu: 50m
        memory: 40Mi
      limits:
        cpu: 300m
        memory: 100Mi

    # With our configuration of rules, thanos ruler has no recording rules so it should be fine without persistence.
    persistence:
      enabled: false
      size: 8Gi

  receiver:
    persistence:
      enabled: true
      size: 50Gi

    # Sets the mode of operation for the receiver component
    # dual-mode: Multiple instances of the receiver. For redundancy
    # standalone: Single instance of receiver.
    mode: dual-mode

    # Retention for the metrics in the receiver
    tsdbRetention: 15d

    # Number of receiver instances
    replicaCount: 2

    # How many nodes that a time-series needs to be written to for a write to be considered successful.
    # It needs to write to (REPLICATION_FACTOR + 1)/2 nodes for success.
    # For more info: https://thanos.io/v0.24/proposals-done/201812-thanos-remote-receive.md/#replication
    replicationFactor: 1

    resources:
      requests:
        cpu: 200m
        memory: 2Gi
      limits:
        cpu: 500m
        memory: 2Gi

  compactor:
    # Set to 0s to disable retention
    retentionResolutionRaw: 30d
    retentionResolution5m: 90d
    retentionResolution1h: 0s

    # Persistence is recommended for caching.
    # https://thanos.io/v0.24/components/compact.md/#disk
    persistence:
      enabled: true
      size: 8Gi

    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        cpu: 500m
        memory: 200Mi

  storegateway:
    resources:
      requests:
        cpu: 100m
        memory: 300Mi
      limits:
        cpu: 300m
        memory: 500Mi

  bucketweb:
    resources:
      requests:
        cpu: 20m
        memory: 40Mi
      limits:
        cpu: 50m
        memory: 40Mi

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

opensearch:
  # 'subdomain' and 'indexPerNamespace' is set in common-config.yaml

  clusterName: opensearch

  # Create initial indices upon first startup
  createIndices: true

  dashboards:
    subdomain: opensearch
    # Note SSO is enabled via `opensearch.sso.enabled`
    resources:
      requests:
        memory: 286Mi
        cpu: 100m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity: {}
    nodeSelector: {}
    tolerations: []

  # Single-sign-on using OIDC, note this requires LetsEncrypt Production
  sso:
    enabled: false
    # Where to find subject
    subjectKey: email
    # Where to find roles
    rolesKey: groups
    # Scope - add 'groups' if groups claim is supported
    scope: openid profile email

  extraRoles: []
    # - role_name: log_reader
    #   definition:
    #     index_permissions:
    #       - index_patterns:
    #           - kubernetes-*
    #         allowed_actions:
    #           - read

  extraRoleMappings:
    - mapping_name: readall_and_monitor
      definition:
        users:
          - set-me # Developer Name
    - mapping_name: kibana_user
      definition:
        users:
          - set-me # dashboards_dev
    - mapping_name: kubernetes_log_reader
      definition:
        users:
          - set-me # dashboards_dev
    - mapping_name: all_access
      definition:
        users:
          - set-me # dashboards_admin

  # Overwrite index templates
  overwriteTemplates: true
  # Create default index templates - kubernetes, kubeaudit, and other
  defaultTemplates: true
  additionalTemplates: {}

  # Index state management
  ism:
    rolloverSizeGB: 1
    rolloverAgeDays: 1
    # Overwrite ism policies
    overwritePolicies: true
    # Create default policies - kubernetes, kubeaudit, authlog, and other
    defaultPolicies: true
    additionalPolicies: {}

  masterNode:
    count: 1
    ## If null, no storageclass is specified in pvc and default storageClass is used
    ## if the DefaultStorageClass admission plugin is enabled.
    ## If "-", "" will be used as storageClassName.
    storageClass: null
    storageSize: 8Gi
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      ## Note: The OpenSearch chart has some restrictions on pod anti affinity:
      ## - Only one rule and term can be set
      ## - The label selector is hardcoded and changing it here does not affect it
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - opensearch-master
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - opensearch
    nodeSelector: {}
    tolerations: []

  dataNode:
    ## Enables dedicated statefulset for data nodes, else the master nodes will assume data role.
    dedicatedPods: true
    count: 2
    ## If null, no storageclass is specified in pvc and default storageClass is used
    ## if the DefaultStorageClass admission plugin is enabled.
    ## If "-", "" will be used as storageClassName.
    storageClass: null
    storageSize: 25Gi
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      ## Note: Same restrictions applies here as on the master nodes
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - opensearch-data
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - opensearch
    nodeSelector: {}
    tolerations: []

  clientNode:
    ## Enables dedicated deployment for client/ingest nodes, else the master nodes will assume client/ingest roles
    dedicatedPods: true
    count: 1
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      ## Note: Same restrictions applies here as on the master nodes
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - opensearch-client
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - opensearch
    nodeSelector: {}
    tolerations: []

  # Config for https://www.elastic.co/guide/en/elasticsearch/client/curator/5.8/about.html
  curator:
    enabled: true
    retention:
      - pattern: other-*
        sizeGB: 1
        ageDays: 7
      - pattern: kubeaudit-*
        sizeGB: 50
        ageDays: 30
      - pattern: kubernetes-*
        sizeGB: 50
        ageDays: 50
      - pattern: authlog-*
        sizeGB: 1
        ageDays: 30
    startingDeadlineSeconds: 600
    activeDeadlineSeconds: 2700
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi
    affinity: {}
    nodeSelector: {}
    tolerations: []

  # Snapshot and snapshot lifecycle configuration
  # Requires S3 or GCS to be enabled
  snapshot:
    enabled: true
    repository: opensearch-snapshots # Uses the bucket set in `objectStorage.buckets.opensearch`
    min: 7
    max: 14
    ageSeconds: 864000
    maxRequestSeconds: 1200
    backupSchedule: 0 */2 * * *
    backupStartingDeadlineSeconds: 600
    backupActiveDeadlineSeconds: 600
    retentionSchedule: '@daily'
    retentionStartingDeadlineSeconds: 600
    retentionActiveDeadlineSeconds: 2700
    retentionResources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 128Mi

  exporter:
    serviceMonitor:
      interval: 30s
      scrapeTimeout: 30s
    resources:
      requests:
        cpu: 15m
        memory: 30Mi
      limits:
        cpu: 30m
        memory: 60Mi
    tolerations: []

  ingress:
    maxbodysize: 8m

fluentd:
  enabled: true
  forwarder:
    resources:
      limits:
        cpu: 500m
        memory: 572Mi
      requests:
        cpu: 200m
        memory: 300Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    livenessProbe:
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    # Set to 'false' when using AWS S3,
    # and 'true' when using any other S3 provider.
    useRegionEndpoint: set-me
    chunkLimitSize: 256MB
    totalLimitSize: 20GB
  aggregator:
    resources:
      limits:
        cpu: 500m
        memory: 1000Mi
      requests:
        cpu: 300m
        memory: 300Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
  scLogsRetention:
    days: 7
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
      limits:
        cpu: 300m
        memory: 100Mi

alerts:
  alertTo: "null"
  opsGenieHeartbeat:
    enabled: false
    url: https://api.eu.opsgenie.com/v2/heartbeats
    name: set-me-if-enabled
  slack:
    channel: set-me-if-enabled
  opsGenie:
    apiUrl: https://api.eu.opsgenie.com

externalTrafficPolicy:
  whitelistRange:
    dex: false
    harbor: false
    opensearch: false
    opensearchDashboards: false
    userGrafana: false
    opsGrafana: false
    prometheusWc: false

s3Exporter:
  # Also requries objectStorage.type=s3
  enabled: true
  interval: 60m
  scrapeTimeout: 10m
  resources:
    limits: {}
    requests:
      cpu: 50m
      memory: 20Mi
  tolerations: []
  affinity: {}
  nodeSelector: {}
