## Adjust the containers resources requests and limits based on your cluster size and usage.

global:
  ## Compliantkubernetes-apps version.
  ## Use version number if you are exactly at a release tag.
  ## Otherwise use full commit hash of current commit.
  ## 'any', can be used to disable this validation.
  ck8sVersion: ${CK8S_VERSION}
  clusterName: ${CK8S_ENVIRONMENT_NAME}-sc
  baseDomain: set-me
  opsDomain: set-me
  issuer: letsencrypt-staging
  verifyTls: true
  clusterDns: 10.233.0.3

storageClasses:
  # Name of the default storageclass.
  # Normally one of 'nfs-client', 'cinder-csi', 'local-storage', 'ebs-gp2', 'rook-ceph-block'
  default: set-me

  nfs:
    enabled: set-me
  cinder:
    enabled: set-me
  local:
    enabled: set-me
  ebs:
    enabled: set-me

objectStorage:
  # Options are "s3", "gcs", or "none"
  # If "none", remember to disable features that depend on object storage:
  #   all backups (velero, harbor, influxdb, elasticsearch), sc logs (fluentd)
  #   Also set harbor persistence to "filesystem" or "swift"
  # Otherwise configure the features to match this type.
  type: set-me
  # gcs:
  #   project: set-me
  # s3:
  #   region: set-me
  #   regionEndpoint: set-me
  #   # Generally false when using AWS and Exoscale and true for other providers.
  #   forcePathStyle: set-me
  buckets:
    harbor: ${CK8S_ENVIRONMENT_NAME}-harbor
    velero: ${CK8S_ENVIRONMENT_NAME}-velero
    elasticsearch: ${CK8S_ENVIRONMENT_NAME}-es-backup
    influxDB: ${CK8S_ENVIRONMENT_NAME}-influxdb
    scFluentd: ${CK8S_ENVIRONMENT_NAME}-sc-logs

user:
  grafana:
    enabled: true
    subdomain: grafana
    resources:
      limits:
        cpu: 100m
        memory: 160Mi
      requests:
        cpu: 50m
        memory: 80Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    userGroups:
      grafanaAdmin: grafana_admin #maps to grafana role admin
      grafanaEditor: grafana_editor #maps to grafana role editor
      grafanaViewer: grafana_viewer #maps to grafana role viewer
    oidc:
      scopes: profile email openid
      allowedDomains:
        - set-me
        - example.com
    sidecar:
      resources:
        requests:
          cpu: 5m
          memory: 80Mi
        limits:
          cpu: 10m
          memory: 100Mi
  # Todo remove dependencie on alertmanager from service cluster
  alertmanager:
    enabled: false
    namespace: alertmanager
    ingress:
      enabled: false
    group_by:
    - cluster
    - alertname
    - severity
harbor:
  enabled: true
  subdomain: harbor
  # The tolerations, affinity, and nodeSelector are applied to all harbor pods.
  tolerations: []
  affinity: {}
  nodeSelector: {}
  chartmuseum:
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 2m
        memory: 25Mi
      limits:
        cpu: 5m
        memory: 50Mi
  core:
    resources:
      requests:
        cpu: 5m
        memory: 50Mi
      limits:
        cpu: 10m
        memory: 100Mi
  database:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        memory: 512Mi
        cpu: 250m
  jobservice:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        cpu: 2m
        memory: 25Mi
      limits:
        cpu: 5m
        memory: 50Mi
  registry:
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 2m
        memory: 25Mi
      limits:
        cpu: 5m
        memory: 50Mi
    controller:
      resources:
        requests:
          cpu: 2m
          memory: 20Mi
        limits:
          cpu: 5m
          memory: 40Mi
  redis:
    persistentVolumeClaim:
      size: 1Gi
    resources:
      requests:
        memory: 32Mi
        cpu: 10m
  notary:
    subdomain: notary.harbor
    resources:
      requests:
        cpu: 2m
        memory: 15Mi
      limits:
        cpu: 5m
        memory: 20Mi
  notarySigner:
    resources:
      requests:
        cpu: 2m
        memory: 10Mi
      limits:
        cpu: 5m
        memory: 20Mi
  portal:
    resources:
      requests:
        cpu: 2m
        memory: 5Mi
      limits:
        cpu: 5m
        memory: 10Mi
  trivy:
    persistentVolumeClaim:
      size: 5Gi
    resources:
      requests:
        cpu: 10m
        memory: 263Mi
      limits:
        cpu: 20m
        memory: 512Mi
  persistence:
    # Valid options are "filesystem" (persistent volume), "swift", or "objectStorage" (matching global config)
    type: set-me
    disableRedirect: set-me
  oidc:
    # group claim name used by OIDC Provider
    groupClaimName: set-me
    # Name of the group that autmatically will get admin
    # Set to "" to disable
    adminGroupName: set-me
    scope: openid,email,profile,offline_access,groups
  backup:
    enabled: true

prometheusBlackboxExporter:
  resources:
    requests:
      cpu: 5m
      memory: 25Mi
    limits:
      cpu: 10m
      memory: 50Mi
prometheusOperator:
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 200Mi

prometheusNodeExporter:
  resources:
    requests:
      cpu: 5m
      memory: 25Mi
    limits:
      cpu: 10m
      memory: 50Mi

kubeStateMetrics:
  resources:
    requests:
      cpu: 5m
      memory: 25Mi
    limits:
      cpu: 10m
      memory: 50Mi

prometheus:
  storage:
    enabled: false
    size: 5Gi
  retention:
    size: 4GiB
    age: 3d
    alertmanager: 72h
  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1"
  tolerations: []
  affinity: {}
  nodeSelector: {}
  wcReader:
    resources:
      requests:
        memory: 1Gi
        cpu: 300m
      limits:
        memory: 2Gi
        cpu: "1"
    storage:
      enabled: false
      size: 5Gi
    retention:
      size: 4GiB
      age: 3d
    tolerations: []
    affinity: {}
    nodeSelector: {}
  alertmanagerSpec:
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
  grafana:
    subdomain: grafana
    resources:
      requests:
        cpu: 50m
        memory: 60Mi
      limits:
        cpu: 100m
        memory: 160Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    oidc:
      enabled: false
      # Only used if the above is true
      # userGroups:
      #   grafanaAdmin: grafana_admin #maps to grafana role admin
      #   grafanaEditor: grafana_editor #maps to grafana role editor
      #   grafanaViewer: grafana_viewer #maps to grafana role viewer
      # scopes: "openid profile email groups"
      # allowedDomains: []

    sidecar:
      resources:
        requests:
          cpu: 5m
          memory: 80Mi
        limits:
          cpu: 10m
          memory: 100Mi
dex:
  subdomain: dex
  additionalKubeloginRedirects: []
  enableStaticLogin: true
  resources:
    limits:
      cpu: 10m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 25Mi
  tolerations: []
  affinity: {}
  nodeSelector: {}
  google:
    # Enables extra config needed to enable google connector to fetch group info.
    # When this is enabled the SASecretName needs to be set.
    groupSupport: false
    # Name of the secret that includes the key file for the service account that is used for fetching group info.
    # The secret will be mounted to the folder /etc/dex/google/ this means that multiple files from the same secret can be used.
    # Simply add `serviceAccountFilePath: /etc/dex/google/secret-key` for each google connector.
    # For more details, see https://elastisys.com/elastisys-engineering-how-to-use-dex-with-google-accounts-to-manage-access-in-kubernetes/
    # SASecretName: set-me

kibana:
  subdomain: kibana
  # Note sso is enabled via `elasticsearch.sso.enabled`
  resources:
    requests:
      cpu: 100m
      memory: 286Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}

elasticsearch:
  subdomain: elastic
  snapshotRepository: elastic-snapshots
  # Create initial indices upon first startup
  createIndices: true
  # Single-sign-on using OIDC
  # NOTE: SSO using OIDC requires LetsEncrypt Production
  sso:
    enabled: false
    # Where to find subject
    subject_key: email
    # Where to find roles
    roles_key: groups
    # Scope - add 'groups' if groups claim is supported
    scope: openid profile email
  masterNode:
    count: 1
    storageSize: 8Gi
    ## If null, no storageclass is specified in pvc and default storageClass is used
    ## if the DefaultStorageClass admission plugin is enabled.
    ## If "-", "" will be used as storageClassName.
    storageClass: null
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                role: master
    tolerations: []
    nodeSelector: {}
  dataNode:
    ## Enables dedicated statefulset for data nodes.
    ## If false, master nodes will assume data role.
    dedicatedPods: true
    count: 2
    storageSize: 25Gi
    ## If null, no storageclass is specified in pvc and default storageClass is used
    ## if the DefaultStorageClass admission plugin is enabled.
    ## If "-", "" will be used as storageClassName.
    storageClass: null
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                role: data
    tolerations: []
    nodeSelector: {}
  clientNode:
    ## Enables dedicated deployment for client/ingest nodes.
    ## If false, master nodes will assume client/ingest roles
    dedicatedPods: true
    count: 1
    javaOpts: -Xms512m -Xmx512m
    resources:
      requests:
        memory: 1024Mi
        cpu: 200m
      limits:
        memory: 1024Mi
        cpu: 1
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                role: client
    tolerations: []
    nodeSelector: {}
  # Config for https://www.elastic.co/guide/en/elasticsearch/client/curator/5.8/about.html
  curator:
    enabled: true
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 64Mi
    affinity: {}
    nodeSelector: {}
    tolerations: []
    retention:
      kubeAuditSizeGB: 50
      kubeAuditAgeDays: 30
      kubernetesSizeGB: 50
      kubernetesAgeDays: 50
      otherSizeGB: 1
      otherAgeDays: 7
      authLogSizeGB: 1
      authLogAgeDays: 30
      # (Optional) retention for indices matched by 'postgresql-*'
      # postgresql: false
      # postgresqlSizeGB: 30
      # postgresqlAgeDays: 30
  # Index state management
  ism:
    # Overwrite ism policies
    overwritePolicies: true
    rolloverSizeGB: 1
    rolloverAgeDays: 1
    # Create default policies - kubernetes, kubeaudit, authlog, and other
    defaultPolicies: true
    additionalPolicies: {}
  # Snapshot and snapshot lifecycle configuration
  snapshot:
    enabled: true
    min: 7
    max: 14
    ageSeconds: 864000
    retentionSchedule: '@daily'
    backupSchedule: 0 */2 * * *
    retentionActiveDeadlineSeconds: 2700
  extraRoles: []
  # - role_name: log_reader
  #   definition:
  #     index_permissions:
  #     - index_patterns:
  #       - "kubernetes-*"
  #       allowed_actions:
  #       - "read"
  extraRoleMappings:
  # - mapping_name: readall_and_monitor
  #   definition:
  #    users:
  #      - "Developer Name"
    - mapping_name: kibana_user
      definition:
        users:
          - set-me
    - mapping_name: kubernetes_log_reader
      definition:
        users:
          - set-me
    - mapping_name: all_access
      definition:
        users:
          - set-me
  #  - mapping_name: kibana_user
  #    definition:
  #      backend_roles:
  #        - "kibana_dev"
  #  - mapping_name: kubernetes_log_reader
  #    definition:
  #      backend_roles:
  #        - "kibana_dev"
  #  - mapping_name: all_access
  #    definition:
  #      backend_roles:
  #        - "kibana_admin"
  overwriteTemplates: true
  # Create default index templates - kubernetes, kubeaudit, and other
  defaultTemplates: true
  additionalTemplates: {}
  exporter:
    serviceMonitor:
      interval: 30s
      scrapeTimeout: 30s
    resources:
      requests:
        cpu: 15m
        memory: 30Mi
      limits:
        cpu: 30m
        memory: 60Mi
    tolerations: []
  ingress:
    maxbodysize: 8m

fluentd:
  enabled: true
  forwarder:
    resources:
      limits:
        cpu: 500m
        memory: 572Mi
      requests:
        cpu: 200m
        memory: 300Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    livenessProbe:
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    # Set to 'false' when using AWS S3,
    # and 'true' when using any other S3 provider.
    useRegionEndpoint: set-me
    chunkLimitSizeBytes: 2048
    queueLimitSizeBytes: 4096
  aggregator:
    resources:
      limits:
        cpu: 500m
        memory: 1000Mi
      requests:
        cpu: 300m
        memory: 300Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

# Log retention for service cluster logs stored in object storage.
logRetention:
  days: 7
influxDB:
  subdomain: influxdb
  users:
    admin: admin
    wcWriter: wcWriter
    scWriter: scWriter
  createdb: true
  resources:
    requests:
      memory: 2Gi
      cpu: 0.5
    limits:
      memory: 6Gi
      cpu: 2
  persistence:
    size: 10Gi
  tolerations: []
  affinity: {}
  nodeSelector: {}
  # Configuration for size based retention
  retention:
    # Enable size based retention job
    enabled: true
    sizeWC: 4500000
    sizeSC: 4500000
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 500Mi
    # The duration of the retention policy for each database
    durationWC: 2d
    durationSC: 3d
  backup:
    enabled: true
    schedule: 0 0 * * *
    startingDeadlineSeconds: 200
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 300Mi
  backupRetention:
    enabled: true
    daysToRetain: 7
    schedule: 0 0 * * *
    startingDeadlineSeconds: 200
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 300Mi

  sidecar:
    nodeexporter:
      resources:
        requests:
          cpu: 5m
          memory: 15Mi
        limits:
          cpu: 10m
          memory: 30Mi
    cronjob:
      resources:
        requests:
          cpu: 5m
          memory: 50Mi
        limits:
          cpu: 10m
          memory: 100Mi
alerts:
  alertTo: "null"
  opsGenieHeartbeat:
    enabled: false
    url: https://api.eu.opsgenie.com/v2/heartbeats
    name: set-me-if-enabled
  slack:
    channel: set-me-if-enabled
  opsGenie:
    apiUrl: https://api.eu.opsgenie.com

externalTrafficPolicy:
  local: false
  whitelistRange:
    global: 0.0.0.0/0
    dex: false
    kibana: false
    elasticsearch: false
    harbor: false
    userGrafana: false
    opsGrafana: false
    prometheusWc: false

nfsProvisioner:
  server: ""
  path: /nfs
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
  tolerations: []
  affinity: {}
  nodeSelector: {}

ingressNginx:
  controller:
    resources:
      requests:
        cpu: 100m
        memory: 263Mi
      limits:
        cpu: 200m
        memory: 512Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    config:
      useProxyProtocol: set-me
    useHostPort: set-me
    service:
      enabled: set-me
      type: set-me
      annotations: set-me
    # Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}

  defaultBackend:
    resources:
      requests:
        cpu: 5m
        memory: 10Mi
      limits:
        cpu: 10m
        memory: 20Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  schedule: 0 0 * * * #once per day
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 50m
      memory: 100Mi
  restic:
    tolerations: []
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi

issuers:
  letsencrypt:
    enabled: true
    prod:
      email: set-me
    staging:
      email: set-me
  extraIssuers: []

certmanager:
  resources:
    requests:
      cpu: 25m
      memory: 100Mi
    limits:
      cpu: 50m
      memory: 200Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}
  extraArgs: []

  webhook:
    resources:
      requests:
        cpu: 25m
        memory: 25Mi
      limits:
        cpu: 50m
        memory: 50Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

  cainjector:
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 200Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

## Configuration for metric-server
metricsServer:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 20m
      memory: 50Mi

calicoAccountant:
  enabled: true

calicoFelixMetrics:
  enabled: true

clusterAdmin:
  users:
    - set-me
    - admin@example.com
  groups: []
s3Exporter:
  # Also requries objectStorage.type=s3
  enabled: true
  interval: 60m
  scrapeTimeout: 10m
  resources:
    limits: {}
    requests:
      cpu: 50m
      memory: 20Mi
  tolerations: []
  affinity: {}
  nodeSelector: {}

starboard:
  # Note: The developers of starboard explicitly recommend against setting your own resources
  resources:
    requests:
      cpu: 5m
      memory: 50Mi
    limits:
      cpu: 10m
      memory: 128Mi
  tolerations: []
  affinity: {}

vulnerabilityExporter:
  resources:
    requests:
      cpu: 20m
      memory: 15Mi
    limits:
      cpu: 40m
      memory: 30Mi
  tolerations: []
  affinity: {}

  curlcronjob:
    resources:
      requests:
        cpu: 20m
        memory: 64Mi
      limits:
        cpu: 40m
        memory: 128Mi
monitoring:
  rook:
    enabled: false
