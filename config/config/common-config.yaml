## Common Kubernetes configuration options.
## This configuration applies to both service and workload clusters.

## Define resources requests and limits for single Pods.
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## resources: {}

## Node labels for Pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
## nodeSelector: {}

## Tolerations for Pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## tolerations: []

## Affinity for Pod assignment
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

## Some common options used in various helm charts.
##
global:
  ## Compliantkubernetes-apps version.
  ## Use version number if you are exactly at a release tag.
  ## Otherwise use full commit hash of current commit.
  ## 'any', can be used to disable this validation.
  ck8sVersion: ${CK8S_VERSION}
  ck8sCloudProvider: ${CK8S_CLOUD_PROVIDER}
  ck8sEnvironmentName: ${CK8S_ENVIRONMENT_NAME}
  ck8sFlavor: ${CK8S_FLAVOR}

  ## Domain intended for ingress usage in the workload cluster
  ## and to reach user facing services such as Grafana, Harbor and OpenSearch Dashboards.
  ## E.g. with 'prod.domain.com', OpenSearch Dashboards is reached via 'opensearch.prod.domain.com'.
  baseDomain: set-me

  ## Domain intended for ingress usage in the service cluster and to reach
  ## non-user facing services such as InfluxDB and OpenSearch.
  ## E.g. with 'ops.prod.domain.com', OpenSearch is reached via 'opensearch.ops.prod.domain.com'.
  opsDomain: set-me

  ## Default cert-manager issuer to use for issuing certificates for ingresses.
  ## Normally one of 'letsencrypt-staging' or 'letsencrypt-prod'.
  issuer: letsencrypt-staging

  ## Verify ingress certificates
  verifyTls: true

  ## IP of the cluster DNS in kubernetes
  clusterDns: 10.233.0.3

  ## Container runtime.
  ## Supported values are 'docker', 'containerd'
  containerRuntime: containerd

## Configuration of storageclasses.
storageClasses:
  # Name of the default storageclass.
  # Normally one of 'cinder-csi' or "rook-ceph-block".
  default: set-me

## Object storage configuration for backups.
objectStorage:
  ## Options are 's3', 'gcs', or 'none'
  ## If 'none', remember to disable backups (velero)
  # If "none", remember to disable features that depend on object storage:
  #   all backups (velero, harbor, opensearch), sc logs (fluentd)
  #   Also set harbor persistence to "filesystem" or "swift"
  # Otherwise configure the features to match this type.
  type: set-me
  # gcs:
  #   project: set-me
  # s3:
  #   region: set-me
  #   regionEndpoint: set-me
  #   # Generally false when using AWS and Exoscale and true for other providers.
  #   forcePathStyle: set-me

  ## Buckets where each respctive application will store its backups.
  buckets:
    velero: ${CK8S_ENVIRONMENT_NAME}-velero
    thanos: ${CK8S_ENVIRONMENT_NAME}-thanos

## User configuration.
user:
  ## User controlled alertmanager configuration.
  alertmanager:
    enabled: false

    ## Todo remove dependencies on alertmanager from service cluster
    ## Create basic-auth protected ingress to alertmanager
    ingress:
      enabled: false

## Falco configuration.
falco:
  enabled: true
  ## additional falco rules
  ## ref: https://falco.org/docs/rules/
  customRules:
    elastisys-images.yaml: |-
      - macro: user_known_contact_k8s_api_server_activities
        condition: (container.image.repository in (docker.io/elastisys/curl-jq, docker.io/elastisys/calico-accountant, docker.io/elastisys/fluentd, quay.io/kiwigrid/k8s-sidecar))
      - macro: user_known_package_manager_in_container
        condition: (container.image.repository in (docker.io/elastisys/curl-jq))
      - list: falco_privileged_images
        append: true
        items: [docker.io/elastisys/calico-accountant]
    velero.yaml: |-
      - macro: user_known_contact_k8s_api_server_activities
        append: true
        condition: or (container.image.repository in (docker.io/velero/velero))
  resources:
    limits:
      cpu: 200m
      memory: 1024Mi
    requests:
      cpu: 100m
      memory: 512Mi

  ## Run on master nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  affinity: {}
  nodeSelector: {}

  ## Falco alerting configuration.
  alerts:
    enabled: true
    ## supported: 'alertmanager', 'slack'.
    type: alertmanager
    priority: notice
    hostPort: http://alertmanager-operated.alertmanager:9093

  falcoSidekick:
    resources:
      limits:
        cpu: 20m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 25Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

  falcoExporter:
    resources:
      requests:
        cpu: 30m
        memory: 20Mi
      limits:
        cpu: 30m
        memory: 20Mi
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    affinity: {}
    nodeSelector: {}

## Hierarchical namespace controller configuration.
hnc:
  enabled: true

## Prometheus configuration.
## Prometheus collects metrics and pushes it to InfluxDB.
prometheusBlackboxExporter:
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 20m
      memory: 50Mi
  targets:
    rook: false
    nginx: true
    prometheus: true
prometheusOperator:
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 200Mi

prometheusNodeExporter:
  resources:
    requests:
      cpu: 25m
      memory: 50Mi
    limits:
      cpu: 50m
      memory: 100Mi

kubeStateMetrics:
  resources:
    requests:
      cpu: 15m
      memory: 50Mi
    limits:
      cpu: 30m
      memory: 100Mi

prometheus:
  replicas: 1

  ## Persistence for prometheus to store metrics and wal.
  storage:
    enabled: false
    size: 5Gi

  ## When prometheus should start to remove metrics from local storage.
  retention:
    size: 4GiB
    age: 3d

  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1" ## Must be a string (integers might be suported in newer versions)

  tolerations: []
  affinity: {}
  nodeSelector: {}
  ## Predictive alert if the resource usage will hit the set percentage in 3 days
  capacityManagementAlerts:
    enabled: true
    disklimit: 66
    usagelimit: 95
    ## for each cpu and memory add the node pattern for which you want to create an alert
    requestlimit:
      cpu:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80
      memory:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80

thanos:
  # Enables Thanos components.
  # If this isn't set, no components will be deployed
  enabled: true

  receiver:
    # Enables the Thanos receiver with the additional components it needs.
    enabled: true

    # Subdomain that will be used for the receiver ingress
    subdomain: thanos-receiver

    # Username that will be used for metrics authentication
    basic_auth:
      username: thanos

opensearch:
  subdomain: opensearch

  indexPerNamespace: false

## Set external traffic policy to: "Local" to preserve source IP on
## providers supporting it
## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
externalTrafficPolicy:
  local: false

  ## Source IP range to allow.
  whitelistRange:
    global: 0.0.0.0/0

## Nginx ingress controller configuration
ingressNginx:
  controller:
    allowSnippetAnnotations: false
    resources:
      requests:
        cpu: 100m
        memory: 263Mi
      limits:
        cpu: 200m
        memory: 512Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    enablepublishService: false
    config:
      ## If 'true', use PROXY protocol
      ## ref: https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
      useProxyProtocol: set-me

    ## If 'true', nginx will use host ports 80 and 443
    useHostPort: set-me

    ## Kubernetes service configuration.
    service:
      enabled: set-me

      ## Type of service.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: set-me

      ## Annotations to add to service
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      annotations: set-me

      ## Whitelist IP address for octavia loadbalancer
      ## ref: https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#restrict-access-for-loadbalancer-service
      loadBalancerSourceRanges: []

    ## Additional configuration options for Nginx
    ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}

  defaultBackend:
    resources:
      requests:
        cpu: 5m
        memory: 10Mi
      limits:
        cpu: 10m
        memory: 20Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

## Configration for Velero and Restic.
## Check out https://compliantkubernetes.io/user-guide/backup/ to see what's included in backups.
velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  schedule: 0 0 * * * # once per day
  retentionPeriod: 720h0m0s
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 50m
      memory: 100Mi

  restic:
    tolerations: []
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi

## Configuration for cert-manager issuers.
issuers:
  ## Deploy let's encrypt ACME issuers
  ## "letsencrypt-prod" and "letsencrypt-staging".
  letsencrypt:
    enabled: true
    prod:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []
      # - selector:
      #     dnsZones:
      #     - example.org
      #   dns01:
      #     route53:
      #       region: eu-north-1
      #       hostedZoneID: set-me
      #       accessKeyID: set-me
      #       secretAccessKeySecretRef:
      #         name: route53-credentials-secret # Can be created in secrets.yaml
      #         key: secretKey
    staging:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []

  ## Additional issuers to create.
  ## ref: https://cert-manager.io/docs/configuration/
  extraIssuers: []
  # - apiVersion: cert-manager.io/v1
  #   kind: Issuer
  #   metadata:
  #     name: selfsigned-issuer
  #     namespace: sandbox
  #   spec:
  #     selfSigned: {}

## Configration for cert-manager and it's components.
certmanager:
  resources:
    requests:
      cpu: 25m
      memory: 100Mi
    limits:
      cpu: 250m
      memory: 250Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

  ## if you need use cert-manager with HTTP01 challenge and a custom image registry in wc, please update the wc-config.yaml with the correct extraArgs
  extraArgs: []

  webhook:
    resources:
      requests:
        cpu: 25m
        memory: 25Mi
      limits:
        cpu: 250m
        memory: 250Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

  cainjector:
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 250Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

## Configuration for metric-server
metricsServer:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 100m
      memory: 100Mi

calicoAccountant:
  enabled: true
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi


calicoFelixMetrics:
  enabled: true

clusterAdmin:
  users:
    - set-me
    - admin@example.com
  groups: []

starboard:
  enabled: true
  # All durations must be specified in seconds, minutes or hours
  # Example 40s / 30m / 20h
  vulnerabilityScanner:
    reportTTL: "720h"
    scanOnlyCurrentRevisions: true
  scanJobs:
    concurrentLimit: 1
    retryDelay: 1m
    timeout: 5m
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 20m
      memory: 172Mi
  tolerations: []
  affinity: {}

vulnerabilityExporter:
  # Will be disabled if starboard is disabled
  resources:
    requests:
      cpu: 20m
      memory: 15Mi
    limits:
      cpu: 40m
      memory: 30Mi
  tolerations: []
  affinity: {}

  curlcronjob:
    resources:
      requests:
        cpu: 20m
        memory: 64Mi
      limits:
        cpu: 40m
        memory: 128Mi

monitoring:
  rook:
    enabled: false


kured:
  enabled: false
  # See options at https://github.com/weaveworks/kured/blob/1.9.1/charts/kured/values.yaml#L24
  configuration:
    lockReleaseDelay: 5m
  metrics:
    enabled: true
    interval: 60s
    labels: {}
  extraArgs: {}
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 15m
      memory: 15Mi
    limits:
      cpu: 100m
      memory: 100Mi
  affinity: {}
  nodeSelector: {}
  dsAnnotations: {}
