## Common Kubernetes configuration options.
## This configuration applies to both service and workload clusters.

## Define resources requests and limits for single Pods.
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## resources: {}

## Node labels for Pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
## nodeSelector: {}

## Tolerations for Pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## tolerations: []

## Affinity for Pod assignment
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

## Some common options used in various helm charts.
##
global:
  ## Compliantkubernetes-apps version.
  ## Use version number if you are exactly at a release tag.
  ## Otherwise use full commit hash of current commit.
  ## 'any', can be used to disable this validation.
  ck8sVersion: ${CK8S_VERSION}
  ck8sCloudProvider: ${CK8S_CLOUD_PROVIDER}
  ck8sEnvironmentName: ${CK8S_ENVIRONMENT_NAME}
  ck8sFlavor: ${CK8S_FLAVOR}

  ## Domain intended for ingress usage in the workload cluster
  ## and to reach user facing services such as Grafana, Harbor and OpenSearch Dashboards.
  ## E.g. with 'prod.domain.com', OpenSearch Dashboards is reached via 'opensearch.prod.domain.com'.
  baseDomain: set-me

  ## Domain intended for ingress usage in the service cluster and to reach
  ## non-user facing services such as Thanos and OpenSearch.
  ## E.g. with 'ops.prod.domain.com', OpenSearch is reached via 'opensearch.ops.prod.domain.com'.
  opsDomain: set-me

  ## Default cert-manager issuer to use for issuing certificates for ingresses.
  ## Normally one of 'letsencrypt-staging' or 'letsencrypt-prod'.
  issuer: letsencrypt-staging

  ## Verify ingress certificates
  verifyTls: true

  ## IP of the cluster DNS in kubernetes
  clusterDns: 10.233.0.3

  ## Container runtime.
  ## Supported values are 'docker', 'containerd'
  containerRuntime: containerd

clusterApi:
  ## Set to true if kubernetes is installed with cluster-api
  enabled: false
  monitoring:
    enabled: false
  clusters:
  - sc
  - wc

## Configuration of storageclasses.
storageClasses:
  # Name of the default storageclass.
  # Normally one of 'cinder-csi' or "rook-ceph-block".
  default: set-me

## Object storage configuration for backups.
objectStorage:
  ## Options are 's3', 'gcs', or 'none'
  ## If "none", remember to disable features that depend on object storage:
  ##   All backups (Velero, Harbor, OpenSearch), SC logs (Fluentd)
  ##   Long term metrics with Thanos.
  ##   Also set Harbor persistence to "filesystem" or "swift"
  ## Otherwise configure the features to match this type.
  type: set-me
  # gcs:
  #   project: set-me
  # s3:
  #   region: set-me
  #   # S3 endpoint for non-AWS implementation of S3. Make sure to prepend the protocol (i.e., https:// or http://) with the URL, e.g., https://s3.sto1.safedc.net
  #   regionEndpoint: set-me
  #   # Generally false when using AWS and Exoscale and true for other providers.
  #   forcePathStyle: set-me

  ## Buckets where each respctive application will store its backups.
  buckets:
    audit: ${CK8S_ENVIRONMENT_NAME}-audit
    velero: ${CK8S_ENVIRONMENT_NAME}-velero
    thanos: ${CK8S_ENVIRONMENT_NAME}-thanos

## User configuration.
user:
  ## User controlled alertmanager configuration.
  alertmanager:
    enabled: true

    ## Todo remove dependencies on alertmanager from service cluster
    ## Create basic-auth protected ingress to alertmanager
    ingress:
      enabled: false

## Falco configuration.
falco:
  enabled: true
  # Available rule files and addons can be found at: https://falcosecurity.github.io/falcoctl/index.yaml
  rulesFiles:
    default:
      enabled: true
      version: 2.0.0
    incubating:
      enabled: false
      version: 2.0.0
    sandbox:
      enabled: false
      version: 2.0.0

  # Setting tty to "true" will immediately display Falco logs by flushing them as they are emitted
  tty: true

  ## configure syscall source
  ## ref: https://falco.org/docs/event-sources/kernel/
  driver:
    kind: module

    ebpf:
      # -- Path where the eBPF probe is located. It comes handy when the probe have been installed in the nodes using tools other than the init
      # container deployed with the chart.
      path:
      # -- Needed to enable eBPF JIT at runtime for performance reasons.
      # Can be skipped if eBPF JIT is enabled from outside the container
      hostNetwork: false

  ## additional falco rules
  ## ref: https://falco.org/docs/rules/
  customRules: {}
  # my-rules-example.yaml : |-
  # - macro: <name of macro to add/change> # possible to use rules and lists as well
  #   append: <true/false> # if re-using an upstream macro/rule/list, append can be used to not overwrite.
  #   condition: <add condition here>
  #   ...

  resources:
    limits:
      cpu: 200m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 150Mi

  ## Run on master nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  affinity: {}
  nodeSelector: {}

  falcoSidekick:
    resources:
      limits:
        cpu: 20m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 25Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

  falcoExporter:
    resources:
      requests:
        cpu: 30m
        memory: 20Mi
      limits:
        cpu: 30m
        memory: 20Mi
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    affinity: {}
    nodeSelector: {}

grafana:
  ops:
    subdomain: grafana
  user:
    subdomain: grafana

fluentd:
  audit:
    enabled: false
    # Add filter stages to capture audit logs, add the audit tag prefix
    filters: ""
      ## Example:
      # filters: |
      # # Set aside OpenSearch Master Audit events
      #   <match kubernetes.var.log.containers.opensearch-master-**>
      #     @type rewrite_tag_filter
      #     <rule>
      #       key message
      #       pattern /\[audit *\]/
      #       tag audit.${tag}
      #     </rule>
      #   </match>

  aggregator:
    # All buffer settings are available, keys are transformed to snake_case as required by fluentd config
    buffer:
      chunkLimitSize: 50MB
      totalLimitSize: 9GB

      flushMode: interval
      flushInterval: 15m
      flushThreadCount: 4
      flushThreadBurstInterval: 0.2

      retryType: exponential_backoff
      retryForever: true
      retryMaxInterval: 30

      timekey: 10m
      timekeyWait: 1m
      timekeyUseUtc: true

    persistence:
      storage: 10Gi

    resources:
      requests:
        cpu: 300m
        memory: 500Mi
      limits:
        cpu: 750m
        memory: 1000Mi

    tolerations: []

    nodeSelector: ""
    affinity: {}

  forwarder:
    buffer:
      totalLimitSize: 20GB

    requestTimeout: 60s

    resources:
      requests:
        cpu: 200m
        memory: 300Mi
      limits:
        cpu: 500m
        memory: 572Mi

    tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
        value: ""
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        value: ""

    nodeSelector: ""
    affinity: {}

    livenessThresholdSeconds: 900
    stuckThresholdSeconds: 1200

harbor:
  enabled: true
  subdomain: harbor
  notary:
    subdomain: notary.harbor

## Hierarchical namespace controller configuration.
hnc:
  enabled: true

## Prometheus configuration.
## Prometheus collects metrics and pushes it to Thanos.
prometheusBlackboxExporter:
  # Hostaliases allow to add additional DNS entries to be injected directly into pods.
  # This will take precedence over your implemented DNS solution
  hostAliases: []
  #  - ip: 192.168.1.1
  #    hostNames:
  #      - test.example.com
  #      - another.example.net
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 20m
      memory: 50Mi
  targets:
    rook: false
    nginx: true
    prometheus: true
prometheusOperator:
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 200Mi

  prometheusConfigReloader:
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        cpu: 50m
        memory: 50Mi

prometheusNodeExporter:
  resources:
    requests:
      cpu: 25m
      memory: 50Mi
    limits:
      cpu: 50m
      memory: 100Mi

kubeStateMetrics:
  resources:
    requests:
      cpu: 15m
      memory: 50Mi
    limits:
      cpu: 30m
      memory: 100Mi
  # Enable this if your cluster is managed by Cluster API to get Cluster API metrics.
  clusterAPIMetrics:
    enabled: false

prometheus:
  replicas: 1

  topologySpreadConstraints:
  - labelSelector:
      matchLabels:
        app.kubernetes.io/name: prometheus
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule

  ## Persistence for prometheus to store metrics and wal.
  storage:
    enabled: false
    size: 5Gi

  ## When prometheus should start to remove metrics from local storage.
  retention:
    size: 4GiB
    age: 3d

  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1" ## Must be a string (integers might be supported in newer versions)

  tolerations: []
  affinity: {}
  nodeSelector: {}
  ## Predictive alert if the resource usage will hit the set percentage in 3 days
  capacityManagementAlerts:
    enabled: true
    disklimit: 75
    persistentVolume:
      enabled: true
      limit: 75
    usagelimit: 95
    ## for each cpu and memory add the node pattern for which you want to create an alert
    requestlimit:
      cpu:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80
      memory:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80
  diskAlerts:
    storage:
      predictLinear:
        - hours: 24
          freeSpacePercentage: 5
          severity: warning
          for: 2h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
      space:
        - freeSpacePercentage: 20
          severity: warning
          for: 30m
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
        - freeSpacePercentage: 10
          severity: critical
          for: 30m
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
    inode:
      predictLinear:
        - hours: 24
          freeSpacePercentage: 40
          severity: warning
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
      space:
        - freeSpacePercentage: 5
          severity: warning
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
        - freeSpacePercentage: 3
          severity: critical
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""

dex:
  subdomain: dex

thanos:
  # Enables Thanos components.
  # If this isn't set, no components will be deployed
  enabled: true

  receiver:
    # Enables the Thanos receiver with the additional components it needs.
    enabled: true

    # Subdomain that will be used for the receiver ingress
    subdomain: thanos-receiver

    # Username that will be used for metrics authentication
    basic_auth:
      username: thanos

opensearch:
  enabled: true
  subdomain: opensearch

  indexPerNamespace: false

  dashboards:
    subdomain: opensearch

## Set external traffic policy to: "Local" to preserve source IP on
## providers supporting it
## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
externalTrafficPolicy:
  local: false

  ## Source IP range to allow.
  whitelistRange:
    global: 0.0.0.0/0

## Nginx ingress controller configuration
ingressNginx:
  controller:
    allowSnippetAnnotations: false
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 200m
        memory: 250Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    enablepublishService: false

    #  Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/cli-arguments.md
    extraArgs: {}
    # metrics-per-host: false

    config:
      ## If 'true', use PROXY protocol
      ## ref: https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
      useProxyProtocol: set-me

    ## If 'true', nginx will use host ports 80 and 443
    useHostPort: set-me

    ## Kubernetes service configuration.
    service:
      enabled: set-me

      ## Type of service.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: set-me

      ## Annotations to add to service
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      annotations: {}

      ## Whitelist IP address for octavia loadbalancer
      ## ref: https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#restrict-access-for-loadbalancer-service
      loadBalancerSourceRanges: []

      ## type: NodePort
      nodePorts:
        http: "30080"
        https: "30443"

      clusterIP: ""

    ## Additional configuration options for Nginx
    ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}

  defaultBackend:
    resources:
      requests:
        cpu: 5m
        memory: 10Mi
      limits:
        cpu: 10m
        memory: 20Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

## Configuration for Velero and node-agent.
## Check out https://compliantkubernetes.io/user-guide/backup/ to see what's included in backups.
velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  schedule: 0 0 * * * # once per day
  retentionPeriod: 720h0m0s
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 50m
      memory: 100Mi

  nodeAgent:
    tolerations: []
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi

## Configuration for cert-manager issuers.
issuers:
  ## Deploy let's encrypt ACME issuers
  ## "letsencrypt-prod" and "letsencrypt-staging".
  letsencrypt:
    enabled: true
    prod:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []
      # - selector:
      #     dnsZones:
      #     - example.org
      #   dns01:
      #     route53:
      #       region: eu-north-1
      #       hostedZoneID: set-me
      #       accessKeyID: set-me
      #       secretAccessKeySecretRef:
      #         name: route53-credentials-secret # Can be created in secrets.yaml
      #         key: secretKey
    staging:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []

  ## Additional issuers to create.
  ## ref: https://cert-manager.io/docs/configuration/
  extraIssuers: []
  # - apiVersion: cert-manager.io/v1
  #   kind: Issuer
  #   metadata:
  #     name: selfsigned-issuer
  #     namespace: sandbox
  #   spec:
  #     selfSigned: {}

## Configuration for cert-manager and it's components.
certmanager:
  resources:
    requests:
      cpu: 25m
      memory: 100Mi
    limits:
      cpu: 250m
      memory: 250Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

  ## if you need use cert-manager with HTTP01 challenge and a custom image registry in wc, please update the wc-config.yaml with the correct extraArgs
  extraArgs: []

  webhook:
    resources:
      requests:
        cpu: 25m
        memory: 25Mi
      limits:
        cpu: 250m
        memory: 100Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

  cainjector:
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 250Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

## Configuration for metric-server
metricsServer:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 100m
      memory: 100Mi

calicoAccountant:
  enabled: true
  # Backend for networkpolicy rules - Ubuntu 20.04 and older use iptables - Ubuntu 22.04 and newer use nftables
  backend: iptables
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

calicoFelixMetrics:
  enabled: true

clusterAdmin:
  users:
    - set-me
    - admin@example.com
  groups: []

## Open policy agent configuration
opa:
  mutations:
    enabled: true

    jobTTL:
      enabled: true
      ttlSeconds: 604800

    ndots:
      enabled: false
      ndotAmount: 3
      labelSelector:
        # None for clusterwide effect or one for each pod or podgroup to be effected by this mutation.
        matchLabels:
          # labelkey: labelvalue

  controllerManager:
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 400m
        memory: 400Mi
    affinity: {}
    tolerations: []
    nodeSelector: {kubernetes.io/os: linux}
    topologySpreadConstraints: []

  audit:
    resources:
      requests:
        cpu: 200m
        memory: 150Mi
      limits:
        cpu: 500m
        memory: 300Mi
    affinity: {}
    tolerations: []
    nodeSelector: {kubernetes.io/os: linux}

    # Enable for audit pod to use cache instead of disk
    writeToRAMDisk: false

  # How often gatekeeper audits kubernetes resources, in seconds
  auditIntervalSeconds: 600

  # A lower chunk size can reduce memory consumption of the auditing Pod
  # but can increase the number of requests to the Kubernetes API server
  auditChunkSize: 500

  # Enable audit informer cache instead of requesting resources from Kubernetes API
  auditFromCache: false

  # The maximum number of audit violations reported on a constraint
  constraintViolationsLimit: 20

  validatingWebhookTimeoutSeconds: 30
  mutatingWebhookTimeoutSeconds: 5

  ## Enable rule that requires pods to come from
  ## the image registry defined by "URL".
  ## "enforcement" can assume either "dryrun" or "deny".
  imageRegistry:
    enabled: true
    enforcement: warn
    URL:
      - set-me
      - harbor.example.com
      ## cert-manager-acmesolver resolves the ACME (LetsEncrypt) challenge for provisioning TLS certificates.
      ## It runs in the same namespace as the Ingress, hence, this needs to be added as an exception.
      - quay.io/jetstack/cert-manager-acmesolver

  ## Enable rule that requires pods to be targeted
  ## by at least one network policy.
  networkPolicies:
    enabled: true
    enforcement: warn

  ## Enable rule that requires pods to have resource requests.
  resourceRequests:
    enabled: true
    enforcement: deny

  ## It will not allow any image with the latest tag
  disallowedTags:
    enabled: true
    enforcement: deny
    tags:
      - latest

trivy:
  enabled: true
  # All durations must be specified in seconds, minutes or hours
  # Example 40s / 30m / 20h
  vulnerabilityScanner:
    scannerReportTTL: "720h"
    scanOnlyCurrentRevisions: true
  scanJobs:
    concurrentLimit: 1
    retryDelay: 1m
    timeout: 5m
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 172Mi
  serviceMonitor:
  # enabled determines whether a serviceMonitor should be deployed
    enabled: true
  tolerations: []
  affinity: {}
kured:
  enabled: false
  # See options at https://github.com/weaveworks/kured/blob/1.9.1/charts/kured/values.yaml#L24
  configuration:
    lockReleaseDelay: 5m
  metrics:
    enabled: true
    interval: 60s
    labels: {}
  extraArgs: {}
  extraEnvVars: {}
  notification:
    slack:
      enabled: false
      channel: ""
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 15m
      memory: 15Mi
    limits:
      cpu: 100m
      memory: 100Mi
  affinity: {}
  nodeSelector: {}
  dsAnnotations: {}

networkPolicies:
  enableAlerting: true
  # Default deny requires Calico as network plugin
  defaultDeny: false
  allowedNameSpaces: []
  additionalEgressPolicies: []
  additionalIngressPolicies: []

  additional: {}
# |-
#   kind: NetworkPolicy
#   apiVersion: networking.k8s.io/v1
#   metadata:
#     name: example-np
#     namespace: default
#   spec:
#     policyTypes:
#       - Ingress
#       - Egress
#     podSelector:
#       matchLabels:
#         foo: bar
#     ingress: {}
#     egress: {}
# ---
  global:
    objectStorage:
      # ips to s3 service
      ips:
      - set-me
      # ports to s3 service
      ports:
      - set-me
    scIngress:
      # ip(s) to sc loadbalancer if available, otherwise sc worker nodes
      ips:
        - set-me
    wcIngress:
      # ip(s) to wc loadbalancer if available, otherwise wc worker nodes
      ips:
        - set-me
    # only true if loadbalancer is not controlled by a kubernetes cloud controller
    externalLoadBalancer: set-me
    ingressUsingHostNetwork: set-me
    trivy:
      ips:
        - set-me
      port: 443

  kured:
    enabled: true
    notificationSlack:
      ips:
        - set-me-if-kured.notification.slack.enabled
      ports:
        - 443

  velero:
    enabled: true

  certManager:
    enabled: true
    # letsencrypt ip addresses
    letsencrypt:
      ips:
        - set-me

  ingressNginx:
    enabled: true
    ingressOverride:
      enabled : false
  falco:
    enabled: true
    plugins:
      ips:
        - set-me
      ports:
        - 443

  gatekeeper:
    enabled: true

  alertmanager:
    # alert receiver, e.g. slack or opsgenie
    alertReceivers:
      ips:
        - set-me
      ports:
        - 443

  rookCeph:
    enabled: false

  kubeSystem:
    enabled: true

  coredns:
    enabled: true
    externalDns:
      ips:
        - set-me
    serviceIp:
      ips:
        - "10.233.0.3/32"

  dnsAutoscaler:
    enabled: true

# Monitoring and Gatekeeper PSP for rook-ceph enabled/disabled
rookCeph:
  monitoring:
    enabled: false
  gatekeeperPsp:
    enabled: false

nodeLocalDns:
  # See docs for details about the config format and options:
  # https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configmap-options
  customConfig: #|-
  #  example.com:53 {
  #      errors
  #      cache 30
  #      reload
  #      loop
  #      forward . 127.0.0.1:9005
  #      }
