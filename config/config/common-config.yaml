## Common Kubernetes configuration options.
## This configuration applies to both service and workload clusters.

## Define resources requests and limits for single Pods.
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## resources: {}

## Node labels for Pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
## nodeSelector: {}

## Tolerations for Pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## tolerations: []

## Affinity for Pod assignment
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

## Some common options used in various helm charts.
##
global:
  ## Compliantkubernetes-apps version.
  ## Use version number if you are exactly at a release tag.
  ## Otherwise use full commit hash of current commit.
  ## 'any', can be used to disable this validation.
  ck8sVersion: ${CK8S_VERSION}
  ck8sCloudProvider: ${CK8S_CLOUD_PROVIDER}
  ck8sEnvironmentName: ${CK8S_ENVIRONMENT_NAME}
  ck8sFlavor: ${CK8S_FLAVOR}

  ## Domain intended for ingress usage in the workload cluster
  ## and to reach user facing services such as Grafana, Harbor and OpenSearch Dashboards.
  ## E.g. with 'prod.domain.com', OpenSearch Dashboards is reached via 'opensearch.prod.domain.com'.
  baseDomain: set-me

  ## Domain intended for ingress usage in the service cluster and to reach
  ## non-user facing services such as InfluxDB and OpenSearch.
  ## E.g. with 'ops.prod.domain.com', OpenSearch is reached via 'opensearch.ops.prod.domain.com'.
  opsDomain: set-me

  ## Default cert-manager issuer to use for issuing certificates for ingresses.
  ## Normally one of 'letsencrypt-staging' or 'letsencrypt-prod'.
  issuer: letsencrypt-staging

  ## Verify ingress certificates
  verifyTls: true

  ## IP of the cluster DNS in kubernetes
  clusterDns: 10.233.0.3

  ## Container runtime.
  ## Supported values are 'docker', 'containerd'
  containerRuntime: containerd

## Configuration of storageclasses.
storageClasses:
  # Name of the default storageclass.
  # Normally one of 'cinder-csi' or "rook-ceph-block".
  default: set-me

## Object storage configuration for backups.
objectStorage:
  ## Options are 's3', 'gcs', or 'none'
  ## If "none", remember to disable features that depend on object storage:
  ##   All backups (Velero, Harbor, OpenSearch), SC logs (Fluentd)
  ##   Long term metrics with Thanos.
  ##   Also set Harbor persistence to "filesystem" or "swift"
  ## Otherwise configure the features to match this type.
  type: set-me
  # gcs:
  #   project: set-me
  # s3:
  #   region: set-me
  #   # S3 endpoint for non-AWS implementation of S3. Make sure to prepend the protocol (i.e., https:// or http://) with the URL, e.g., https://s3.sto1.safedc.net
  #   regionEndpoint: set-me
  #   # Generally false when using AWS and Exoscale and true for other providers.
  #   forcePathStyle: set-me

  ## Buckets where each respctive application will store its backups.
  buckets:
    audit: ${CK8S_ENVIRONMENT_NAME}-audit
    velero: ${CK8S_ENVIRONMENT_NAME}-velero
    thanos: ${CK8S_ENVIRONMENT_NAME}-thanos

## User configuration.
user:
  ## User controlled alertmanager configuration.
  alertmanager:
    enabled: true

    ## Todo remove dependencies on alertmanager from service cluster
    ## Create basic-auth protected ingress to alertmanager
    ingress:
      enabled: false

## Falco configuration.
falco:
  enabled: true
  ## additional falco rules
  ## ref: https://falco.org/docs/rules/
  customRules: []
  resources:
    limits:
      cpu: 200m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 150Mi

  ## Run on master nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  affinity: {}
  nodeSelector: {}

  ## Falco alerting configuration.
  alerts:
    enabled: true
    ## supported: 'alertmanager', 'slack'.
    type: alertmanager
    priority: notice
    hostPort: http://alertmanager-operated.alertmanager:9093

  falcoSidekick:
    resources:
      limits:
        cpu: 20m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 25Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

  falcoExporter:
    resources:
      requests:
        cpu: 30m
        memory: 20Mi
      limits:
        cpu: 30m
        memory: 20Mi
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    affinity: {}
    nodeSelector: {}

fluentd:
  audit:
    enabled: false
    # Add filter stages to capture audit logs, add the audit tag prefix
    filters: ""
      ## Example:
      # filters: |
      # # Set aside OpenSearch Master Audit events
      #   <match kubernetes.var.log.containers.opensearch-master-**>
      #     @type rewrite_tag_filter
      #     <rule>
      #       key message
      #       pattern /\[audit *\]/
      #       tag audit.${tag}
      #     </rule>
      #   </match>

  aggregator:
    buffer: {}

    persistence:
      storage: 10Gi

    resources:
      requests:
        cpu: 300m
        memory: 500Mi
      limits:
        cpu: 500m
        memory: 1000Mi

    tolerations: []

    nodeSelector: ""
    affinity: {}

  forwarder:
    buffer:
      totalLimitSize: 20GB

    resources:
      requests:
        cpu: 200m
        memory: 300Mi
      limits:
        cpu: 500m
        memory: 572Mi

    tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
        value: ""
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        value: ""

    nodeSelector: ""
    affinity: {}

    livenessThresholdSeconds: 900
    stuckThresholdSeconds: 1200


## Hierarchical namespace controller configuration.
hnc:
  enabled: true

## Prometheus configuration.
## Prometheus collects metrics and pushes it to InfluxDB.
prometheusBlackboxExporter:
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 20m
      memory: 50Mi
  targets:
    rook: false
    nginx: true
    prometheus: true
prometheusOperator:
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 200Mi

  prometheusConfigReloader:
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        cpu: 50m
        memory: 50Mi

prometheusNodeExporter:
  resources:
    requests:
      cpu: 25m
      memory: 50Mi
    limits:
      cpu: 50m
      memory: 100Mi

kubeStateMetrics:
  resources:
    requests:
      cpu: 15m
      memory: 50Mi
    limits:
      cpu: 30m
      memory: 100Mi

prometheus:
  replicas: 1

  topologySpreadConstraints:
  - labelSelector:
      matchLabels:
        app.kubernetes.io/name: prometheus
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule

  ## Persistence for prometheus to store metrics and wal.
  storage:
    enabled: false
    size: 5Gi

  ## When prometheus should start to remove metrics from local storage.
  retention:
    size: 4GiB
    age: 3d

  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1" ## Must be a string (integers might be suported in newer versions)

  tolerations: []
  affinity: {}
  nodeSelector: {}
  ## Predictive alert if the resource usage will hit the set percentage in 3 days
  capacityManagementAlerts:
    enabled: true
    disklimit: 75
    usagelimit: 95
    ## for each cpu and memory add the node pattern for which you want to create an alert
    requestlimit:
      cpu:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80
      memory:
        - name: worker
          pattern: ".*-worker-.*"
          limit: 80
        - name: elastisys
          pattern: ".*-elastisys-.*"
          limit: 80

thanos:
  # Enables Thanos components.
  # If this isn't set, no components will be deployed
  enabled: true

  receiver:
    # Enables the Thanos receiver with the additional components it needs.
    enabled: true

    # Subdomain that will be used for the receiver ingress
    subdomain: thanos-receiver

    # Username that will be used for metrics authentication
    basic_auth:
      username: thanos

opensearch:
  subdomain: opensearch

  indexPerNamespace: false

## Set external traffic policy to: "Local" to preserve source IP on
## providers supporting it
## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
externalTrafficPolicy:
  local: false

  ## Source IP range to allow.
  whitelistRange:
    global: 0.0.0.0/0

## Nginx ingress controller configuration
ingressNginx:
  controller:
    allowSnippetAnnotations: false
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 200m
        memory: 250Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    enablepublishService: false
    config:
      ## If 'true', use PROXY protocol
      ## ref: https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
      useProxyProtocol: set-me

    ## If 'true', nginx will use host ports 80 and 443
    useHostPort: set-me

    ## Kubernetes service configuration.
    service:
      enabled: set-me

      ## Type of service.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: set-me

      ## Annotations to add to service
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      annotations: {}

      ## Whitelist IP address for octavia loadbalancer
      ## ref: https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#restrict-access-for-loadbalancer-service
      loadBalancerSourceRanges: []

      ## type: NodePort
      nodePorts:
        http: "30080"
        https: "30443"

    ## Additional configuration options for Nginx
    ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}

  defaultBackend:
    resources:
      requests:
        cpu: 5m
        memory: 10Mi
      limits:
        cpu: 10m
        memory: 20Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

## Configration for Velero and Restic.
## Check out https://compliantkubernetes.io/user-guide/backup/ to see what's included in backups.
velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  schedule: 0 0 * * * # once per day
  retentionPeriod: 720h0m0s
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 50m
      memory: 100Mi

  restic:
    tolerations: []
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi

## Configuration for cert-manager issuers.
issuers:
  ## Deploy let's encrypt ACME issuers
  ## "letsencrypt-prod" and "letsencrypt-staging".
  letsencrypt:
    enabled: true
    prod:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []
      # - selector:
      #     dnsZones:
      #     - example.org
      #   dns01:
      #     route53:
      #       region: eu-north-1
      #       hostedZoneID: set-me
      #       accessKeyID: set-me
      #       secretAccessKeySecretRef:
      #         name: route53-credentials-secret # Can be created in secrets.yaml
      #         key: secretKey
    staging:
      ## Mail through which letsencrypt can contact you.
      email: set-me
      ## Solvers, sets a default http01 when empty.
      solvers: []

  ## Additional issuers to create.
  ## ref: https://cert-manager.io/docs/configuration/
  extraIssuers: []
  # - apiVersion: cert-manager.io/v1
  #   kind: Issuer
  #   metadata:
  #     name: selfsigned-issuer
  #     namespace: sandbox
  #   spec:
  #     selfSigned: {}

## Configration for cert-manager and it's components.
certmanager:
  resources:
    requests:
      cpu: 25m
      memory: 100Mi
    limits:
      cpu: 250m
      memory: 250Mi
  nodeSelector: {}
  tolerations: {}
  affinity: {}

  ## if you need use cert-manager with HTTP01 challenge and a custom image registry in wc, please update the wc-config.yaml with the correct extraArgs
  extraArgs: []

  webhook:
    resources:
      requests:
        cpu: 25m
        memory: 25Mi
      limits:
        cpu: 250m
        memory: 100Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

  cainjector:
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 250Mi
    nodeSelector: {}
    tolerations: {}
    affinity: {}

## Configuration for metric-server
metricsServer:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 100m
      memory: 100Mi

calicoAccountant:
  enabled: true
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi


calicoFelixMetrics:
  enabled: true

clusterAdmin:
  users:
    - set-me
    - admin@example.com
  groups: []

## Open policy agent configuration
opa:
  enabled: true

  controllerManager:
    resources:
      requests:
        cpu: 50m
        memory: 80Mi
      limits:
        cpu: 200m
        memory: 100Mi

  audit:
    resources:
      requests:
        cpu: 200m
        memory: 100Mi
      limits:
        cpu: 500m
        memory: 200Mi

  # how often gatekeeper audits kubernetes resources, in seconds
  auditIntervalSeconds: 600

  ## Enable rule that requires pods to come from
  ## the image registry defined by "URL".
  ## "enforcement" can assume either "dryrun" or "deny".
  imageRegistry:
    enabled: true
    enforcement: warn
    URL:
      - set-me
      - harbor.example.com
      ## cert-manager-acmesolver resolves the ACME (LetsEncrypt) challenge for provisioning TLS certificates.
      ## It runs in the same namespace as the Ingress, hence, this needs to be added as an exception.
      - quay.io/jetstack/cert-manager-acmesolver

  ## Enable rule that requires pods to be targeted
  ## by at least one network policy.
  networkPolicies:
    enabled: true
    enforcement: warn

  ## Enable rule that requires pods to have resource requests.
  resourceRequests:
    enabled: true
    enforcement: deny

  ## It will not allow any image with the latest tag
  disallowedTags:
    enabled: true
    enforcement: deny
    tags:
     - latest

starboard:
  enabled: true
  # All durations must be specified in seconds, minutes or hours
  # Example 40s / 30m / 20h
  vulnerabilityScanner:
    reportTTL: "720h"
    scanOnlyCurrentRevisions: true
  scanJobs:
    concurrentLimit: 1
    retryDelay: 1m
    timeout: 5m
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 20m
      memory: 172Mi
  tolerations: []
  affinity: {}

vulnerabilityExporter:
  # Will be disabled if starboard is disabled
  resources:
    requests:
      cpu: 20m
      memory: 15Mi
    limits:
      cpu: 40m
      memory: 30Mi
  tolerations: []
  affinity: {}

  curlcronjob:
    resources:
      requests:
        cpu: 20m
        memory: 64Mi
      limits:
        cpu: 40m
        memory: 128Mi

ciskubebenchExporter:
  # Will be disabled if starboard is disabled
  resources:
    requests:
      cpu: 20m
      memory: 15Mi
    limits:
      cpu: 100m
      memory: 30Mi
  tolerations: []
  affinity: {}

  curlcronjob:
    resources:
      requests:
        cpu: 20m
        memory: 64Mi
      limits:
        cpu: 40m
        memory: 128Mi

monitoring:
  rook:
    enabled: false


kured:
  enabled: false
  # See options at https://github.com/weaveworks/kured/blob/1.9.1/charts/kured/values.yaml#L24
  configuration:
    lockReleaseDelay: 5m
  metrics:
    enabled: true
    interval: 60s
    labels: {}
  extraArgs: {}
  extraEnvVars: {}
  notification:
    slack:
      enabled: false
      channel: ""
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 15m
      memory: 15Mi
    limits:
      cpu: 100m
      memory: 100Mi
  affinity: {}
  nodeSelector: {}
  dsAnnotations: {}

networkPolicies:
  enableAlerting: true
  # Default deny requires Calico as network plugin
  defaultDeny: false
  allowedNameSpaces: []
  additionalEgressPolicies: []
  additionalIngressPolicies: []

  additional: {}
# |-
#   kind: NetworkPolicy
#   apiVersion: networking.k8s.io/v1
#   metadata:
#     name: example-np
#     namespace: default
#   spec:
#     policyTypes:
#       - Ingress
#       - Egress
#     podSelector:
#       matchLabels:
#         foo: bar
#     ingress: {}
#     egress: {}
# ---
  global:
    objectStorage:
      # ips to s3 service
      ips:
      - set-me
      # ports to s3 service
      ports:
      - set-me
    scIngress:
      # ip(s) to sc loadbalancer if available, otherwise sc worker nodes
      ips:
        - set-me
    wcIngress:
      # ip(s) to wc loadbalancer if available, otherwise wc worker nodes
      ips:
        - set-me
    # only true if loadbalancer is not controlled by a kubernetes cloud controller
    externalLoadBalancer: false
    ingressUsingHostNetwork: false
    trivy:
      ips:
        - set-me
      port: 443

  kured:
    enabled: true
    notificationSlack:
      ips:
        - set-me-if-kured.notification.slack.enabled
      ports:
        - 443

  velero:
    enabled: true

  certManager:
    enabled: true
    # letsencrypt ip addresses
    letsencrypt:
      ips:
        - set-me

  ingressNginx:
    enabled: true
    ingressOverride:
      enabled : false
  falco:
    enabled: true
    plugins:
      ips:
        - set-me
      ports:
        - 443

  alertmanager:
    # alert receiver, e.g. slack or opsgenie
    alertReceivers:
      ips:
        - set-me
      ports:
        - 443

  rookCeph:
    enabled: false

  kubeSystem:
    enabled: true

  coredns:
    enabled: true
    externalDns:
      ips:
        - set-me
    serviceIp:
      ips:
        - "10.233.0.3/32"

  dnsAutoscaler:
    enabled: true

# Will be removed v0.31
kubernetesPSP: true
