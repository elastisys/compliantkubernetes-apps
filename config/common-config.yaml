## Common Kubernetes configuration options.
## This configuration applies to both service and workload clusters.

## Define resources requests and limits for single Pods.
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## resources: {}

## Node labels for Pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
## nodeSelector: {}

## Tolerations for Pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## tolerations: []

## Affinity for Pod assignment
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## affinity: {}

## Some common options used in various helm charts.
##
global:
  ## Compliantkubernetes-apps version.
  ## Use version number if you are exactly at a release tag.
  ## Otherwise use full commit hash of current commit.
  ## 'any', can be used to disable this validation.
  ck8sVersion: ${CK8S_VERSION}
  ck8sCloudProvider: ${CK8S_CLOUD_PROVIDER}
  ck8sEnvironmentName: ${CK8S_ENVIRONMENT_NAME}
  ck8sFlavor: ${CK8S_FLAVOR}
  ck8sK8sInstaller: ${CK8S_K8S_INSTALLER}

  ## Domain intended for ingress usage in the workload cluster
  ## and to reach user facing services such as Grafana, Harbor and OpenSearch Dashboards.
  ## E.g. with 'prod.domain.com', OpenSearch Dashboards is reached via 'opensearch.prod.domain.com'.
  baseDomain: set-me

  ## Domain intended for ingress usage in the service cluster and to reach
  ## non-user facing services such as Thanos and OpenSearch.
  ## E.g. with 'ops.prod.domain.com', OpenSearch is reached via 'opensearch.ops.prod.domain.com'.
  opsDomain: set-me

  ## Default cert-manager issuer to use for issuing certificates for ingresses.
  ## Normally one of 'letsencrypt-staging' or 'letsencrypt-prod'.
  issuer: letsencrypt-staging

  ## Verify ingress certificates
  verifyTls: true

  ## IP of the cluster DNS in kubernetes
  clusterDns: set-me

  ## Container runtime.
  ## Supported values are 'docker', 'containerd'
  containerRuntime: containerd

clusterApi:
  ## Set to true if kubernetes is installed with cluster-api
  enabled: set-me
  monitoring:
    enabled: set-me
  clusters:
  - sc
  - wc

## Configuration of storageclasses.
storageClasses:
  # Name of the default storageclass.
  # Normally one of 'cinder-csi' or "rook-ceph-block".
  default: set-me

## Object storage configuration for backups.
objectStorage:
  ## Options are 's3', 'gcs', 'azure' or 'none'
  ## If "none", remember to disable features that depend on object storage:
  ##   All backups (Velero, Harbor, OpenSearch), SC logs (Fluentd)
  ##   Long term metrics with Thanos.
  ##   Also set Harbor persistence to "filesystem" or "swift"
  ## Otherwise configure the features to match this type.
  type: set-me
  # azure:
  #  resourceGroup: set-me
  # gcs:
  #   project: set-me
  # s3:
  #   region: set-me
  #   # S3 endpoint for non-AWS implementation of S3. Make sure to prepend the protocol (i.e., https:// or http://) with the URL, e.g., https://s3.sto1.safedc.net
  #   regionEndpoint: set-me
  #   # Generally false when using AWS and Exoscale and true for other providers.
  #   forcePathStyle: set-me

  ## Buckets where each respctive application will store its backups.
  buckets:
    audit: ${CK8S_ENVIRONMENT_NAME}-audit
    velero: ${CK8S_ENVIRONMENT_NAME}-velero
    thanos: ${CK8S_ENVIRONMENT_NAME}-thanos

## User configuration.
user:
  ## User controlled alertmanager configuration.
  alertmanager:
    enabled: true

    ## Todo remove dependencies on alertmanager from service cluster
    ## Create basic-auth protected ingress to alertmanager
    ingress:
      enabled: false

## Falco configuration.
falco:
  ## Falco alerting configuration.
  alerts:
    enabled: true
    ## supported: 'alertmanager', 'slack', 'none'.
    type: alertmanager
    priority: notice
  enabled: true
  # Available rule files and addons can be found at: https://falcosecurity.github.io/falcoctl/index.yaml
  rulesFiles:
    default:
      enabled: true
      version: 3.0.1
    incubating:
      enabled: false
      version: 3.0.1
    sandbox:
      enabled: false
      version: 3.0.1

  artifact:
    install:
      # set to false in an air-gapped environment, unless artifacts are self-hosted
      enabled: true

  # Setting tty to "true" will immediately display Falco logs by flushing them as they are emitted
  tty: true

  ## configure syscall source
  ## ref: https://falco.org/docs/concepts/event-sources/kernel/
  driver:
    kind: kmod

    ebpf:
      # -- Path where the eBPF probe is located. It comes handy when the probe have been installed in the nodes using tools other than the init
      # container deployed with the chart.
      #path: ""
      # -- Needed to enable eBPF JIT at runtime for performance reasons.
      # Can be skipped if eBPF JIT is enabled from outside the container
      hostNetwork: false

    module:
      # override the URL used for downloading driver modules, e.g. to use a self hosted file server in an air-gapped environment
      repoURL: ""

  # use custom indexes for falcoctl downloads
  # ref: https://github.com/falcosecurity/charts/blob/falco-3.8.3/charts/falco/values.yaml#L390-L395
  customIndexes: []

  ## additional falco rules
  ## ref: https://falco.org/docs/rules/
  customRules: {}
  # my-rules-example.yaml : |-
  # - macro: <name of macro to add/change> # possible to use rules and lists as well
  #   append: <true/false> # if re-using an upstream macro/rule/list, append can be used to not overwrite.
  #   condition: <add condition here>
  #   ...

  resources:
    limits:
      cpu: 200m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 150Mi

  ## Run on master nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  affinity: {}
  nodeSelector: {}

  falcoSidekick:
    resources:
      limits:
        cpu: 20m
        memory: 250Mi
      requests:
        cpu: 10m
        memory: 25Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}

grafana:
  ops:
    subdomain: grafana
  user:
    subdomain: grafana

fluentd:
  audit:
    enabled: false
    # Add filter stages to capture audit logs, add the audit tag prefix
    filters: ""
      ## Example:
      # filters: |
      # # Set aside OpenSearch Master Audit events
      #   <match kubernetes.var.log.containers.opensearch-master-**>
      #     @type rewrite_tag_filter
      #     <rule>
      #       key message
      #       pattern /\[audit *\]/
      #       tag audit.${tag}
      #     </rule>
      #   </match>

  aggregator:
    # All buffer settings are available, keys are transformed to snake_case as required by fluentd config
    buffer:
      chunkLimitSize: 50MB
      totalLimitSize: 9GB

      flushMode: interval
      flushInterval: 15m
      flushThreadCount: 4
      flushThreadBurstInterval: 0.2

      retryType: exponential_backoff
      retryForever: true
      retryMaxInterval: 30

      timekey: 10m
      timekeyWait: 1m
      timekeyUseUtc: true

    persistence:
      storage: 10Gi

    resources:
      requests:
        cpu: 300m
        memory: 500Mi
      limits:
        cpu: 750m
        memory: 1000Mi

    tolerations: []

    nodeSelector: {}
    affinity: {}

  forwarder:
    buffer:
      totalLimitSize: 20GB

    requestTimeout: 60s

    resources:
      requests:
        cpu: 200m
        memory: 300Mi
      limits:
        cpu: 500m
        memory: 572Mi

    tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
        value: ""
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        value: ""

    nodeSelector: {}
    affinity: {}

    livenessThresholdSeconds: 900
    stuckThresholdSeconds: 1200

harbor:
  enabled: true
  subdomain: harbor

## Hierarchical namespace controller configuration.
hnc:
  enabled: true

## Prometheus configuration.
## Prometheus collects metrics and pushes it to Thanos.
prometheusBlackboxExporter:
  # Hostaliases allow to add additional DNS entries to be injected directly into pods.
  # This will take precedence over your implemented DNS solution
  hostAliases: []
  #  - ip: 192.168.1.1
  #    hostNames:
  #      - test.example.com
  #      - another.example.net
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 20m
      memory: 50Mi
  tolerations: []
  affinity: {}
  targets:
    rook: false
    nginx: true
    prometheus: true
prometheusOperator:
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 200Mi

  prometheusConfigReloader:
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        cpu: 50m
        memory: 50Mi

prometheusNodeExporter:
  resources:
    requests:
      cpu: 25m
      memory: 50Mi
    limits:
      cpu: 50m
      memory: 100Mi

kubeStateMetrics:
  resources:
    requests:
      cpu: 15m
      memory: 50Mi
    limits:
      cpu: 30m
      memory: 300Mi

openstackMonitoring:
  ## Enables openstack api metrics.
  enabled: false

prometheus:
  replicas: 1

  topologySpreadConstraints:
  - labelSelector:
      matchLabels:
        app.kubernetes.io/name: prometheus
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule

  ## Persistence for prometheus to store metrics and wal.
  storage:
    enabled: false
    size: 5Gi

  ## When prometheus should start to remove metrics from local storage.
  retention:
    size: 4GiB
    age: 3d

  resources:
    requests:
      memory: 1Gi
      cpu: 300m
    limits:
      memory: 2Gi
      cpu: "1" ## Must be a string (integers might be supported in newer versions)

  tolerations: []
  affinity: {}
  nodeSelector: {}
  webhookAlerts:
    enabled: true
  ## Predictive alert if the resource usage will hit the set percentage in 3 days
  capacityManagementAlerts:
    enabled: true
    disklimit: 75
    persistentVolume:
      enabled: true
      limit: 75
    predictUsage: false
    usagelimit: 95
    nodeGroupCpuLimit24h: 75
    nodeGroupMemoryLimit24h: 75
    nodeGroupCpuLimit1h: 95
    nodeGroupMemoryLimit1h: 85
    nodeCpuLimit1h: 95
    nodeMemoryLimit1h: 85
    ## for each cpu and memory add the node pattern for which you want to create an alert
    requestLimit:
      cpu: 80
      memory: 80
    nodeGroupRequestsExcludePattern: ""
  diskAlerts:
    storage:
      predictLinear:
        - hours: 24
          freeSpacePercentage: 5
          severity: warning
          for: 2h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
      space:
        - freeSpacePercentage: 20
          severity: warning
          for: 30m
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
        - freeSpacePercentage: 10
          severity: critical
          for: 30m
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
    inode:
      predictLinear:
        - hours: 24
          freeSpacePercentage: 40
          severity: warning
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
      space:
        - freeSpacePercentage: 5
          severity: warning
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""
        - freeSpacePercentage: 3
          severity: critical
          for: 1h
          pattern:
            include:
              node: ".*"
              disk: ".*"
            exclude:
              node: ""
              disk: ""

dex:
  subdomain: dex

thanos:
  # Enables Thanos components.
  # If this isn't set, no components will be deployed
  enabled: true

  receiver:
    # Enables the Thanos receiver with the additional components it needs.
    enabled: true

    # Subdomain that will be used for the receiver ingress
    subdomain: thanos-receiver

    # Username that will be used for metrics authentication
    basic_auth:
      username: thanos

opensearch:
  enabled: true
  subdomain: opensearch

  indexPerNamespace: false

  dashboards:
    subdomain: opensearch

## Set external traffic policy to: "Local" to preserve source IP on
## providers supporting it
## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer
externalTrafficPolicy:
  local: true

  ## Source IP range to allow.
  whitelistRange:
    global: 0.0.0.0/0

## Nginx ingress controller configuration
ingressNginx:
  controller:
    allowSnippetAnnotations: false
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 200m
        memory: 250Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    # Enable this if service type is loadbalancer and externalDns is enabled together with ingress
    enablepublishService: false

    ## Enable validation of Nginx annotations on Ingress resources.
    ## This is kept false by default due to the maturity of the feature,
    ## e.g. lack of documentation and disruptive bugs.
    ## TODO: Revisit this feature in future upgrade
    enableAnnotationValidations: false

    #  Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/cli-arguments.md
    extraArgs: {}
    # metrics-per-host: false

    config:
      ## If 'true', use PROXY protocol
      ## ref: https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
      useProxyProtocol: set-me

      ## Accepted values are 'Critical', 'High', 'Medium' and 'Low'
      ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#annotations-risk-level
      annotationsRiskLevel: Critical

    ## If 'true', nginx will use host ports 80 and 443
    useHostPort: set-me

    ## Kubernetes service configuration.
    service:
      enabled: set-me

      ## Type of service.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: set-me-if-(.ingressNginx.controller.service.enabled)

      ## Annotations to add to service
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      annotations: set-me-if-(.ingressNginx.controller.service.enabled)

      ## Enable node port allocation
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
      allocateLoadBalancerNodePorts: set-me

      ## Whitelist IP address for octavia loadbalancer
      ## ref: https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#restrict-access-for-loadbalancer-service
      loadBalancerSourceRanges: []

      # Set this to use an existing floating ip
      # If you want to change this then you need to recreate the service
      loadBalancerIP: ""

      # -- Represents the dual-stack-ness requested or required by this Service. Possible values are
      # SingleStack (default), PreferDualStack or RequireDualStack. When utilizing an internal loadbalancer service (ie MetalLB),
      # set this field to "RequireDualStack" if you want both IPv4 and IPv6 connectivity.
      # The ipFamilies and clusterIPs fields depend on the value of this field.
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/
      ipFamilyPolicy: ""
      # -- List of IP families (e.g. IPv4, IPv6) assigned to the service. Default is IPv4 only. When utilizing an internal loadbalancer service (ie MetalLB),
      # IPv6 would also need to be included in order for the ingress service to allocate an address in that family.
      ipFamilies: []

      ## type: NodePort
      nodePorts:
        http: 30080
        https: 30443

      clusterIP: ""

    ## Additional configuration options for Nginx
    ## ref: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
    additionalConfig: {}

    # If 'true', nginx will run in a 'chroot' in the controller container.
    # Note that this also allows the container to use the 'SYS_ADMIN' capability
    # as well as changing the ingress-nginx namespace to privileged.
    chroot: true

  defaultBackend:
    resources:
      requests:
        cpu: 5m
        memory: 10Mi
      limits:
        cpu: 10m
        memory: 20Mi
    tolerations: []
    affinity: {}
    nodeSelector: {}
    topologySpreadConstraints: []

## Configuration for Velero and node-agent.
## Check out https://compliantkubernetes.io/user-guide/backup/ to see what's included in backups.
velero:
  enabled: true
  tolerations: []
  nodeSelector: {}
  uploaderType: restic
  useVolumeSnapshots: false
  schedule: 0 0 * * * # once per day
  retentionPeriod: 720h0m0s
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 50m
      memory: 100Mi

  nodeAgent:
    tolerations: []
    resources:
      limits:
        cpu: 500m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 100Mi

## Configuration for cert-manager issuers.
issuers:
  ## Deploy let's encrypt ACME issuers
  ## "letsencrypt-prod" and "letsencrypt-staging".
  letsencrypt:
    enabled: true
    prod:
      ## Mail through which letsencrypt can contact you.
      email: set-me-if-(.issuers.letsencrypt.enabled)
      ## Solvers, sets a default http01 when empty.
      solvers: []
      # - selector:
      #     dnsZones:
      #     - example.org
      #   Set the required network policies
      #   dns01:
      #     route53:
      #       region: eu-north-1
      #       hostedZoneID: set-me
      #       accessKeyID: set-me
      #       secretAccessKeySecretRef:
      #         name: route53-credentials-secret # Can be created in secrets.yaml
      #         key: secretKey
    staging:
      ## Mail through which letsencrypt can contact you.
      email: set-me-if-(.issuers.letsencrypt.enabled)
      ## Solvers, sets a default http01 when empty.
      solvers: []

  ## Additional issuers to create.
  ## ref: https://cert-manager.io/docs/configuration/
  extraIssuers: []
  # - apiVersion: cert-manager.io/v1
  #   kind: Issuer
  #   metadata:
  #     name: selfsigned-issuer
  #     namespace: sandbox
  #   spec:
  #     selfSigned: {}

## Configuration for cert-manager and it's components.
certmanager:
  resources:
    requests:
      cpu: 25m
      memory: 100Mi
    limits:
      cpu: 250m
      memory: 250Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}

  ## if you need use cert-manager with HTTP01 challenge and a custom image registry in wc, please update the wc-config.yaml with the correct extraArgs
  extraArgs: []

  webhook:
    resources:
      requests:
        cpu: 25m
        memory: 25Mi
      limits:
        cpu: 250m
        memory: 100Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}

  cainjector:
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
      limits:
        cpu: 250m
        memory: 250Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}

    ## Extra args for cainjector, docs for available args: https://cert-manager.io/docs/cli/cainjector/
    extraArgs: []
    #- --enable-certificates-data-source=true

## Configuration for metric-server
metricsServer:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 100m
      memory: 100Mi
  tolerations: []
  affinity: {}

calicoAccountant:
  enabled: true
  # Backend for networkpolicy rules - Ubuntu 20.04 and older use iptables - Ubuntu 22.04 and newer use nftables
  backend: set-me
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

calicoFelixMetrics:
  enabled: true

clusterAdmin:
  users:
    - set-me
    - admin@example.com
  groups: []

## Open policy agent configuration
opa:
  mutations:
    enabled: true

    jobTTL:
      enabled: true
      ttlSeconds: 604800

    ndots:
      enabled: false
      ndotAmount: 3
      labelSelector:
        # None for clusterwide effect or one for each pod or podgroup to be effected by this mutation.
        matchLabels: {}
          # labelkey: labelvalue

  controllerManager:
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 400m
        memory: 400Mi
    affinity: {}
    tolerations: []
    nodeSelector: {kubernetes.io/os: linux}
    topologySpreadConstraints: []

  audit:
    resources:
      requests:
        cpu: 200m
        memory: 150Mi
      limits:
        cpu: 750m
        memory: 500Mi
    affinity: {}
    tolerations: []
    nodeSelector: {kubernetes.io/os: linux}

    # Enable for audit pod to use cache instead of disk
    writeToRAMDisk: false

  # How often gatekeeper audits kubernetes resources, in seconds
  auditIntervalSeconds: 600

  # A lower chunk size can reduce memory consumption of the auditing Pod
  # but can increase the number of requests to the Kubernetes API server
  auditChunkSize: 500

  # Enable audit informer cache instead of requesting resources from Kubernetes API
  auditFromCache: false

  # The maximum number of audit violations reported on a constraint
  constraintViolationsLimit: 20

  validatingWebhookTimeoutSeconds: 30
  mutatingWebhookTimeoutSeconds: 5

  ## Enable rule that requires pods to come from
  ## the image registry defined by "URL".
  ## "enforcement" can assume either "dryrun" or "deny".
  imageRegistry:
    enabled: true
    enforcement: warn
    URL:
      - set-me
      - harbor.example.com
      ## cert-manager-acmesolver resolves the ACME (LetsEncrypt) challenge for provisioning TLS certificates.
      ## It runs in the same namespace as the Ingress, hence, this needs to be added as an exception.
      - quay.io/jetstack/cert-manager-acmesolver

  ## Enable rule that requires pods to be targeted
  ## by at least one network policy.
  networkPolicies:
    enabled: true
    enforcement: warn

  ## Enable rule that requires pods to have resource requests.
  resourceRequests:
    enabled: true
    enforcement: deny

  ## It will not allow any image with the latest tag
  disallowedTags:
    enabled: true
    enforcement: deny
    tags:
      - latest

  ## Prevent the creation of load balancer services when unsupported
  rejectLoadBalancerService:
    enabled: set-me
    enforcement: deny

  ## Enable rule that warns about minimum replica number
  minimumDeploymentReplicas:
    enabled: true
    enforcement: warn

trivy:
  enabled: true
  # comma separated list of namespaces (or glob patterns) to be excluded from scanning
  excludeNamespaces: ""
  clusterComplianceEnabled: true
  configAuditScannerEnabled: true
  exposedSecretScannerEnabled: true
  infraAssessmentScannerEnabled: true
  rbacAssessmentScannerEnabled: true
  vulnerabilityScannerEnabled: true
  sbomGenerationEnabled: false
  # All durations must be specified in seconds, minutes or hours
  # Example 40s / 30m / 20h
  vulnerabilityScanner:
    scannerReportTTL: "720h"
    scanOnlyCurrentRevisions: true
  scanJobs:
    concurrentLimit: 1
    retryDelay: 1m
    timeout: 5m
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m  # needs more memory at startup
      memory: 1000Mi  # needs more memory at startup
  serviceMonitor:
  # enabled determines whether a serviceMonitor should be deployed
    enabled: true
    interval: 5m
  tolerations: []
  affinity: {}
  nodeCollector:
    tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
  # configurations for an offline / air-gapped environment
  # ref: https://github.com/aquasecurity/trivy-operator/tree/main/deploy/helm#values
  scanner:
    resources: {}
    timeout: 5m0s
    offlineScanEnabled: false
    dbRegistry: ""
    dbRepository: ""
    dbRepositoryInsecure: false
    javaDbRegistry: ""
    javaDbRepository: ""
    # if authorization is required for pulling from registry, create a pull
    # secret in the monitoring namespace and configure the secret name
    imagePullSecret:
      name: ""
    registry:
      mirror: {}
        # "docker.io": registry.example.org:5000
        # "gcr.io": registry.example.org:5000
        # "ghcr.io": registry.example.org:5000
        # "index.docker.io": registry.example.org:5000
        # "quay.io": registry.example.org:5000
        # "registry.k8s.io": registry.example.org:5000

kured:
  enabled: false
  # See options at https://github.com/weaveworks/kured/blob/1.9.1/charts/kured/values.yaml#L24
  configuration:
    lockReleaseDelay: 5m
  metrics:
    enabled: true
    interval: 60s
    labels: {}
  extraArgs: []
  extraEnvVars: {}
  notification:
    slack:
      enabled: false
      channel: ""
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 15m
      memory: 15Mi
    limits:
      cpu: 100m
      memory: 100Mi
  affinity: {}
  nodeSelector: {}
  dsAnnotations: {}

networkPolicies:
  enableAlerting: true
  # Default deny requires Calico as network plugin
  defaultDeny: false
  allowedNameSpaces: []
  additionalEgressPolicies: []
  additionalIngressPolicies: []

  additional: ""
    # |-
    # ---
    #   apiVersion: networking.k8s.io/v1
    #   kind: NetworkPolicy
    #   metadata:
    #     namespace: default
    #     name: example-np
    #   spec:
    #     podSelector:
    #       matchLabels:
    #         foo: bar
    #     policyTypes:
    #       - Ingress
    #       - Egress
    #     ingress: {}
    #     egress: {}
  global:
    objectStorage:
      # ips to s3 service
      ips:
      - set-me
      # ports to s3 service
      ports:
      - set-me
    scIngress:
      # ip(s) to sc loadbalancer if available, otherwise sc worker nodes
      ips:
        - set-me
    wcIngress:
      # ip(s) to wc loadbalancer if available, otherwise wc worker nodes
      ips:
        - set-me
    # only true if loadbalancer is not controlled by a kubernetes cloud controller
    externalLoadBalancer: set-me
    ingressUsingHostNetwork: set-me
    trivy:
      ips:
        - set-me-if-(.trivy.enabled)
      port: 443

  kured:
    enabled: true
    notificationSlack:
      ips:
        - set-me-if-(.kured.enabled and .kured.notification.slack.enabled)
      ports:
        - 443

  velero:
    enabled: true

  certManager:
    enabled: true
    # letsencrypt ip addresses
    letsencrypt:
      ips:
        - set-me-if-(.networkPolicies.certManager.enabled)
    # Configure this if HTTP-01 challenges need to be enabled in cert-manager for other endpoints than the ingress-controller
    http01:
      ips: []
    # Configure this if DNS-01 challenges are enabled in cert-manager
    dns01:
      ips: []

  ingressNginx:
    enabled: true
    ingressOverride:
      enabled: set-me
      ips:
        - set-me-if-(.networkPolicies.ingressNginx.ingressOverride.enabled)

  falco:
    enabled: true
    plugins:
      ips:
        - set-me-if-(.falco.enabled and .networkPolicies.falco.enabled)
      ports:
        - 443

  externalDns:
    enabled: false
    ips:
      - set-me-if-(.externalDns.enabled and .networkPolicies.externalDns.enabled)
    ports:
      - 443

  gatekeeper:
    enabled: true

  alertmanager:
    # alert receiver, e.g. slack or opsgenie
    alertReceivers:
      ips:
        - set-me
      ports:
        - 443

  rookCeph:
    enabled: false

  kubeSystem:
    enabled: true

  coredns:
    enabled: true
    externalDns:
      ips:
        - set-me-if-(.networkPolicies.coredns.enabled)
    serviceIp:
      ips:
        - set-me-if-(.networkPolicies.coredns.enabled)

  dnsAutoscaler:
    enabled: true

# Monitoring and Gatekeeper PSP for rook-ceph enabled/disabled
rookCeph:
  monitoring:
    enabled: false
  gatekeeperPsp:
    enabled: false

nodeLocalDns:
  # See docs for details about the config format and options:
  # https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configmap-options
  customConfig: "" #|-
  #  example.com:53 {
  #      errors
  #      cache 30
  #      reload
  #      loop
  #      forward . 127.0.0.1:9005
  #      }

  # Configure .53 host zone
  hostZone:
    extraConfig: "" #|
    #   template ANY ANY {
    #     rcode NXDOMAIN
    #   }
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 25m
      memory: 40Mi

externalDns:
  # Enable ExternalDNS to manage dns records from kubernetes objects.
  # Possible sources are CRD, Ingress and services.
  # Currently only aws route53 is supported within ck8s-apps.
  # Example: https://kubernetes-sigs.github.io/external-dns/v0.14.1/tutorials/aws/
  enabled: false
  provider: aws
  txtOwnerId: set-me-if-(.externalDns.enabled)
  sources:
    crd: false
    ingress: true
    service: true
  domains: []
  logLevel: info
  # AWS route53 can not handle CNAME records and TXT with the same name.
  # When --aws-prefer-cname is used, add a prefix to make it work.
  # txtPrefix: <cluster-name>
  txtPrefix: ""
  extraArgs: []
  # If loadbalancer as a service is used and it receives a hostname instead of a ip.
  # You should add the following to extraArgs.
  # This happens for openstack environments using proxy protocol (nip.io)
  # - --aws-prefer-cname
  resources:
    limits:
      cpu: 100m
      memory: 100Mi
    requests:
      cpu: 50m
      memory: 50Mi
  affinity: {}
  topologySpreadConstraints: []
  tolerations: []
  namespaced: false
  endpoints: []
  # If externalDns.sources.crd is enabled add endpoints for your dns endpoints
  #  - dnsName: kube
  #    recordTTL: 180
  #    recordType: A
  #    targets:
  #      - wc control-plane nodes
  #  - dnsName: kube.ops
  #    recordTTL: 180
  #    recordType: A
  #    targets:
  #      - sc control-plane nodes
