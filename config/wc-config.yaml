# This configuration applies to the workload cluster.
# It will override settings set in "defaults/common-config.yaml".
global:
  ## The cluster name.
  ## Used in logs and metrics as to separate these from other clusters.
  clusterName: ${CK8S_ENVIRONMENT_NAME}-wc

  ## If baseDomain for wc and sc are not the same, set the domain of the sc cluster.
  scDomain: ""
  scOpsDomain: ""

## User configuration.
user:
  ## This only controls if the namespaces should be created, user RBAC is always created.
  createNamespaces: true

  ## List of user namespaces to create.
  namespaces:
    - set-me
    - demo

  ## Any namespace listed in constraints are exempted from HNC managed namespaces
  ## This to override the Pod Security Admission level
  ## Example of constraint can be found here: helmfile/charts/gatekeeper/podsecuritypolicies/values.yaml
  ## The only extra label "psaLevel: <baseline/privileged>" is shown in the following example:
  ## <namespace>:
  ##  psaLevel: <baseline/privileged>
  ##  <service-name>:
  ##    ...
  constraints: {}

  ## List of users to create RBAC rules for.
  adminUsers:
    - set-me
    - admin@example.com

  ## List of serviceAccounts to create RBAC rules for, used for dev situations.
  serviceAccounts: []

  ## List of groups to create RBAC rules for.
  adminGroups:
    - set-me

  # Installs required cluster resources needed to install sealedSecrets
  # Requires that gatekeeper.allowUserCRDs.enabled is enabled.
  sealedSecrets:
    enabled: false
  # Installs required cluster resources needed to install mongodb
  # Requires that gatekeeper.allowUserCRDs.enabled is enabled.
  mongodb:
    enabled: false
  # Installs required cluster resources needed to install fluxv2
  # Requires that gatekeeper.allowUserCRDs.enabled is enabled.
  fluxv2:
    enabled: false
  # Installs required cluster resources needed to install kafka-operator
  # Requires that gatekeeper.allowUserCRDs.enabled is enabled.
  kafka:
    enabled: false

  # Create extra application developer RBAC
  extraRoles: {}
  ## Example:
  # extraRoles:
  #   example-role:
  #     rules:
  #     - apiGroups:
  #         - ""
  #       resources:
  #         - pods
  #       verbs:
  #         - get
  #         - list
  #         - watch

  extraRoleBindings: {}
  ## Example:
  # extraRoleBindings:
  #   example-rolebinding:
  #     roleRef:
  #       name: example-clusterrole
  #       kind: Role
  #     subjects:
  #     - kind: ServiceAccount
  #       name: example-serviceaccount
  #       namespace: example-namespace

  extraClusterRoles: {}
  ## Example:
  # extraClusterRoles:
  #   example-clusterrole:
  #     rules:
  #     - apiGroups:
  #         - ""
  #       resources:
  #         - pods
  #       verbs:
  #         - get
  #         - list
  #         - watch

  extraClusterRoleBindings: {}
  ## Example:
  # extraClusterRoleBindings:
  #   example-clusterrolebinding:
  #     roleRef:
  #       name: example-clusterrole
  #     subjects:
  #     - kind: ServiceAccount
  #       name: example-serviceaccount
  #       namespace: example-namespace

falco:
  alerts:
    hostPort: http://alertmanager-operated.alertmanager:9093

## Prometheus configuration.
## Prometheus collects metrics and pushes it to Thanos.
prometheus:
  ## Additional prometheus scrape config.
  ## ref: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
  additionalScrapeConfigs: []
  devAlertmanager:
     namespace: alertmanager
     ingressEnabled: false
     username: alertmanager
  alertmanagerSpec:
    groupBy:
    - alertname

velero:
  storagePrefix: workload-cluster

  ## Excluded namespaces
  excludedNamespaces:
    - calico-apiserver
    - calico-system
    - cert-manager
    - falco
    - fluentd
    - gatekeeper-system
    - gpu-operator
    - hnc-system
    - ingress-nginx
    - kube-node-lease
    - kube-public
    - kube-system
    - kured
    - kyverno
    - monitoring
    - openstack-system
    - rook-ceph
    - velero

  ## Extra excluded namespace, here we should add the namespaces that are more likely to change
  excludedExtraNamespaces: []

## Hierarchical namespace controller configuration, enable in common-config.yaml
hnc:
  ## Included namespaces, empty string will include all
  includedNamespacesRegex: ""

  ## Extra excluded namespace, here we should add the namespaces that are more likely to change
  excludedNamespaces: []

  ## Additional resources to enable opt-in propagation for.
  ## Objects that should be propagated must have one of the annotations listed here https://github.com/kubernetes-sigs/hierarchical-namespaces/blob/master/docs/user-guide/how-to.md#limit-the-propagation-of-an-object-to-descendant-namespaces
  additionalAllowPropagateResources: []
    ## examples:
    ## - resource: secrets
    ## - resource: networkpolicies
    ##   group: networking.k8s.io

  ## Annotations that will be stripped from propagated objects
  unpropagatedAnnotations: []

  ## Annotations that will be propagated to subnamespaces (allows regex)
  managedNamespaceAnnotations: []
  ## Labels that will be propagated to subnamespaces (allows regex)
  ## Labels in particular must also be configured in the HierarchyConfiguration object to be propagated
  managedNamespaceLabels:
    - pod-security.kubernetes.io/enforce
    - pod-security.kubernetes.io/audit
    - pod-security.kubernetes.io/warn

  ## Manager deployment configuration
  manager:
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 100m
        memory: 300Mi

    nodeSelector: {}
    tolerations: []
    affinity: {}

  ## Enables HA mode for hnc webhooks
  ha: true

  webhookMatchConditions: true

  ## Webhook deployment configuration, only used if hnc.ha is true
  webhook:
    replicaCount: 3
    resources:
      requests:
        cpu: 100m
        memory: 150Mi
      limits:
        cpu: 100m
        memory: 300Mi

    topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: hnc-webhook
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule

    nodeSelector: {}
    tolerations: []
    affinity: {}

  ## Service monitor configuration
  serviceMonitor:
    relabelings: []

## certmanager:
  ## when using cert-manager with HTTP01 challenge and a custom image registry the below parameter should be added in the extraArgs
  ## !! this works only with public repositories !! see https://github.com/jetstack/cert-manager/issues/2429
  ## update the image tag based on the version used in the helm chart
  ## - --acme-http01-solver-image=<harbor_server_name>/<proxy_project_name>/jetstack/cert-manager-acmesolver:v1.4.0
  ## extraArgs: []

## Configuration for fluentd.
## Fluentd ships logs to OpenSearch using the endpoint 'opensearch.subdomain' set in common-config.yaml.
## Consists of two different deployments, one for running on master nodes
## and and one for running on "user nodes".
fluentd:
  enabled: true
  ## Only run on master nodes.
  forwarder:
    buffer:
      chunkLimitSize: 8MB
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists

  ## Extra fluentd config to mount.
  extraConfigMaps: {}

  ## These pods collect logs from application developer worker nodes.
  user:
    resources:
      requests:
        cpu: 100m
        memory: 300Mi
      limits:
        cpu: 500m
        memory: 1000Mi

    tolerations: []
    nodeSelector: {}
    affinity: {}

externalTrafficPolicy:
  # Whitelisting requires externalTrafficPolicy.local to be true
  # local: true

  # Comma separated list of CIDRs, e.g. 172.16.0.0/24,172.24.0.0/24
  whitelistRange:
    # global: 0.0.0.0/0
    kubeapiMetrics: false
wcProbeIngress:
  enabled: true

prometheusBlackboxExporter:
  targets:
    gatekeeper: true
    falco: true
    sc: true

ingressNginx:
  subDomain: ingress-nginx
  controller:
    ## Kubernetes service configuration.
    service:
      internal:
        ## Type of service.
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
        # type: set-me

        ## Annotations to add to service
        ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
        ## Set the additional internal loadbalancer annotation if `type` is "LoadBalancer"
        ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/README.md#additional-internal-load-balancer
        ## For Azure use `service.beta.kubernetes.io/azure-load-balancer-internal: "true"`
        ## For Openstack use `service.beta.kubernetes.io/openstack-internal-load-balancer: "true"`
        annotations: {}

        ## Enable node port allocation
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
        allocateLoadBalancerNodePorts: true

        ## Whitelist IP address for octavia loadbalancer
        ## ref: https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#restrict-access-for-loadbalancer-service
        loadBalancerSourceRanges: []

        # Set this to use an existing floating ip
        # If you want to change this then you need to recreate the service
        loadBalancerIP: ""

        # -- Represents the dual-stack-ness requested or required by this Service. Possible values are
        # SingleStack (default), PreferDualStack or RequireDualStack. When utilizing an internal loadbalancer service (ie MetalLB),
        # set this field to "RequireDualStack" if you want both IPv4 and IPv6 connectivity.
        # The ipFamilies and clusterIPs fields depend on the value of this field.
        ## Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/
        ipFamilyPolicy: ""
        # -- List of IP families (e.g. IPv4, IPv6) assigned to the service. Default is IPv4 only. When utilizing an internal loadbalancer service (ie MetalLB),
        # IPv6 would also need to be included in order for the ingress service to allocate an address in that family.
        ipFamilies: []

        ## type: NodePort
        nodePorts:
          http: 30080
          https: 30443

        clusterIP: ""

# Network policies for workload cluster
networkPolicies:
  global:
    wcApiserver:
      # usually private ip of control-plane nodes
      ips:
        - set-me
      port: 6443
    wcNodes:
      # ip of all nodes in the cluster for internal communication
      ips:
        - set-me

  monitoring:
    enabled: true

  fluentd:
    enabled: true
    extraOutput:
      ips: []
      ports: []

  certManager:
    namespaces: []

  alertmanager:
    enabled: true

  prometheus:
    # This feature can be enabled to create Network Policies that allow internal traffic to Prometheus.
    # Access is granted to:
    # Pods with the label `elastisys.io/prometheus-access: allow` that belong to a namespace from a configurable list of namespaces
    internalAccess:
      enabled: false
      # A list of namespaces from which to allow traffic from pods that have the required label.
      namespaces: []

## Open Policy Agent Gatekeeper configuration
gatekeeper:
  allowUserCRDs:
    enabled: false
    enforcement: deny
    # The name of the user specified in /etc/kubernetes/admin.conf kubeconfig file of the control plane nodes
    # Necessary if Kubespray is used for managing the k8s cluster.
    adminConfUser: kubernetes-admin
    extraCRDs: []
    # Sealed secrets and MongoDB CRDs can be enabled via user.sealedsecrets/mongodb
    # - names:
    #     - sealedsecrets.bitnami.com
    #   group: "bitnami.com"
    extraServiceAccounts: []
    #  - namespace: "gatekeeper-system"
    #    name: "gatekeeper-admin-upgrade-crds"

gpu:
  enabled: false
