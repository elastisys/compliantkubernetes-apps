---
# Source: prometheus-alerts/templates/alerts/rook.yaml
# Based on https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/monitoring/prometheus-ceph-v14-rules.yaml
---
# Source: prometheus-alerts/templates/alerts/alertmanager.rules.yaml
# Note: These have been modified to use the cluster label, although it is currently templated to look for a specific alertmanager instance.
# Generated from 'alertmanager.rules' group from https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-rules.yaml
# Do not change in-place! In order to change this file first read following link:
# https://github.com/helm/charts/tree/master/stable//hack
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-alertmanager.rules
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      annotations:
        description: Alertmanager instances within the {{$labels.job}} cluster have different configurations.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent
        summary: Alertmanager instances within the same cluster have different configurations.
      expr: |-
        count by (cluster,service) (
          count_values by (cluster,service) ("config_hash", alertmanager_config_hash{job="kube-prometheus-stack-alertmanager",namespace="monitoring"})
        )
        != 1
      for: 20m
      labels:
        severity: critical
    - alert: AlertmanagerFailedReload
      annotations:
        description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
        summary: Reloading an Alertmanager configuration has failed.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(alertmanager_config_last_reload_successful{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
    - alert: AlertmanagerMembersInconsistent
      annotations:
        description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent
        summary: A member of an Alertmanager cluster has not found all other cluster members.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m])
        < on (cluster,service) group_left
          count by (cluster,service) (max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m]))
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/backup-status.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-backup-status
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: backup-status
    rules:
    - alert: HarborBackupHaveFailed24Hours
      annotations:
        description: The job daily backup job harbor-backup have failed over 24 hours.
        runbook_url: Missing runbook
        summary: The daily backup job harbor-backup have failed over 24 hours.
      expr: |-
        (
          (min((time()-kube_job_status_completion_time{job_name=~"harbor-backup-cronjob-.*", cluster=~".*"})/3600)) > 24
        )
      for: 1h
      labels:
        severity: warning
    - alert: HarborBackupHaveFailed48Hours
      annotations:
        description: The job daily backup job harbor-backup have failed over 48 hours.
        runbook_url: Missing runbook
        summary: The daily backup job harbor-backup have failed over 48 hours.
      expr: |-
        (
          (min((time()-kube_job_status_completion_time{job_name=~"harbor-backup-cronjob-.*", cluster=~".*"})/3600)) > 48
        )
      for: 1h
      labels:
        severity: warning
    - alert: VeleroBackupHaveFailed24Hours
      annotations:
        description: The job daily backup job velero-backup have failed over 24 hours.
        runbook_url: Missing runbook
        summary: The daily backup job velero-backup have failed over 24 hours.
      expr: |-
        (
          (time() - velero_backup_last_successful_timestamp{schedule="velero-daily-backup", cluster=~".*"}) / 60 / 60 > 24
        )
      for: 1h
      labels:
        severity: warning
    - alert: VeleroBackupHaveFailed48Hours
      annotations:
        description: The job daily backup job velero-backup have failed over 48 hours.
        runbook_url: Missing runbook
        summary: The daily backup job velero-backup have failed over 48 hours.
      expr: |-
        (
          (time() - velero_backup_last_successful_timestamp{schedule="velero-daily-backup", cluster=~".*"}) / 60 / 60 > 48
        )
      for: 1h
      labels:
        severity: warning
    - alert: OpenSearchSnapshotHaveFailed24Hours
      annotations:
        description: There have been no successful OpenSearch snapshots for over 24 hours.
        runbook_url: Missing runbook
        summary: OpenSearch snapshots have failed for over 24 hours.
      expr: |-
        (
          (time()-elasticsearch_snapshot_stats_snapshot_start_time_timestamp{state="SUCCESS", cluster=~".*"})/3600 > 24
        or
          (time()-elasticsearch_snapshot_stats_snapshot_start_time_timestamp{state="PARTIAL", cluster=~".*"})/3600 > 24
        )
      for: 1h
      labels:
        severity: warning
    - alert: OpenSearchSnapshotHaveFailed48Hours
      annotations:
        description: There have been no successful OpenSearch snapshots for over 48 hours.
        runbook_url: Missing runbook
        summary: OpenSearch snapshots have failed for over 48 hours.
      expr: |-
        (
          (time()-elasticsearch_snapshot_stats_snapshot_start_time_timestamp{state="SUCCESS", cluster=~".*"})/3600 > 48
        or
          (time()-elasticsearch_snapshot_stats_snapshot_start_time_timestamp{state="PARTIAL", cluster=~".*"})/3600 > 48
        )
      for: 1h
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/blackbox.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-blackbox
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
    - name: blackbox
      rules:
        - alert: EndpointDown
          expr: probe_success == 0
          for: 60s
          labels:
            severity: "critical"
          annotations:
            summary: "Endpoint {{ $labels.target }} at {{ $labels.instance }} down"
            runbook_url: Missing runbook
---
# Source: prometheus-alerts/templates/alerts/cert-manager-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-cert-manager-certificates
  namespace: "cert-manager"
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: cert-manager-certificate.rules
    rules:
    - alert: CertificateExpiringSoon
      annotations:
        message: The Certificate {{$labels.name}} is expiring within 20 days.
        runbook_url: Missing runbook
      # Fire the alert when there are less than 20 days = 60*60*24*20 seconds left
      expr: certmanager_certificate_expiration_timestamp_seconds - time() < 60*60*24*20
      for: 10m
      labels:
        severity: low
    - alert: CertificateNotReady
      annotations:
        message: The Certificate {{$labels.name}} is not ready!
        runbook_url: Missing runbook
      # Fire the alert when the Certificaet is not ready
      expr: certmanager_certificate_ready_status{condition="False"} > 0
      for: 10m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/cluster-api.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-cluster-api
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
    - name: cluster-api-cluster
      rules:
        - alert: ClusterApiClusterIsPaused
          annotations:
            description: |
              The cluster `{{ $labels.name }}` has been paused for 15 minutes
            summary: |
              The cluster `{{ $labels.name }}` has been paused for 15 minutes
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum(capi_cluster_spec_paused) by (name) != 0
          for: 15m
        - alert: ClusterApiClusterControlPlaneNotInitialized
          annotations:
            description: |
              The cluster `{{ $labels.name }}` has an uninitialized control plane
            summary: |
              The cluster `{{ $labels.name }}` has an uninitialized control plane
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum(capi_cluster_status_condition{type="ControlPlaneInitialized", status="True"}) by (name) != 1
          for: 15m
        - alert: ClusterApiClusterNotReady
          annotations:
            description: |
              The cluster `{{ $labels.name }}` is not ready
            summary: |
              The cluster `{{ $labels.name }}` is not ready
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: high
          expr: |
            sum(capi_cluster_status_condition{type="ControlPlaneReady", status="True"}) by (name) != 1
          for: 5m
        - alert: ClusterApiClusterInfrastructureNotReady
          annotations:
            description: |
              The cluster `{{ $labels.name }}` infrastructur is not ready
            summary: |
              The cluster `{{ $labels.name }}` infrastructur is not ready
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: high
          expr: |
            sum(capi_cluster_status_condition{type="InfrastructureReady", status="True"}) by (name) != 1
          for: 15m
        - alert: ClusterApiClusterNotProvisionedState
          annotations:
            description: |
              The cluster `{{ $labels.name }}` is not in a Provisioned state for more than 15 minutes, this can happen if the cluster is currently provisioning new nodes or replacing nodes.
            summary: |
              The cluster `{{ $labels.name }}` is not in a Provisioned state for more than 15 minutes
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            capi_cluster_status_phase{phase!="Provisioned"} == 1
          for: 15m
        - alert: ClusterApiClusterNotProvisionedState
          annotations:
            description: |
              The cluster `{{ $labels.name }}` is not in a Provisioned state for more than 1 hour, this can happen if the cluster is currently provisioning new nodes or replacing nodes.
            summary: |
              The cluster `{{ $labels.name }}` is not in a Provisioned state for more than 1 hour
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: high
          expr: |
            capi_cluster_status_phase{phase!="Provisioned"} == 1
          for: 60m
    - name: cluster-api-kubeadm-control-plane
      rules:
        - alert: ClusterApiKubeadmControlPlaneNotFullyReady
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` that has some not ready nodes.
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` that has some not ready nodes.
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum(capi_kubeadmcontrolplane_status_replicas_ready / capi_kubeadmcontrolplane_status_replicas) by (cluster_name, name) != 1
          for: 10m
        - alert: ClusterApiKubeadmControlPlaneCloseToMajorityNotReady
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` with one node from having majority of its nodes not ready.
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` with one node from having majority of its nodes not ready.
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum(capi_kubeadmcontrolplane_status_replicas_ready / capi_kubeadmcontrolplane_status_replicas) by (cluster_name, name) < 0.5
          for: 10m
        - alert: ClusterApiKubeadmControlPlaneMajorityNotReady
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` that have majority of its nodes not ready.
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a control plane `{{ $labels.name }}` that have majority of its nodes not ready.
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: high
          expr: |
            sum(capi_kubeadmcontrolplane_status_replicas_ready / capi_kubeadmcontrolplane_status_replicas) by (cluster_name, name) < 0.5
          for: 2m
    - name: cluster-api-machine-deployment
      rules:
        - alert: ClusterApiMachineDeploymentNotFullyReady
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a machine deployment `{{ $labels.name }}` that has some not ready nodes.
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a machine deployment `{{ $labels.name }}` that has some not ready nodes.
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum(capi_machinedeployment_status_replicas_ready / (capi_machinedeployment_status_replicas > 0)) by (cluster_name, name) != 1
          for: 15m
        - alert: ClusterApiMachineDeploymentMajorityNotReady
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a machine deployment `{{ $labels.name }}` that have majority of its nodes not ready.
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a machine deployment `{{ $labels.name }}` that have majority of its nodes not ready.
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: high
          expr: |
            sum(capi_machinedeployment_status_replicas_ready / capi_machinedeployment_status_replicas) by (cluster_name, name) < 0.5
          for: 2m
    - name: cluster-api-machine
      rules:
        - alert: ClusterApiMachineConditionNotTrue
          annotations:
            description: |
              The cluster `{{ $labels.cluster_name }}` has a machine `{{ $labels.node_name }}` that has the condition `{{ $labels.type }}` set to `{{ $labels.status }}`
            summary: |
              The cluster `{{ $labels.cluster_name }}` has a machine `{{ $labels.node_name }}` that has the condition `{{ $labels.type }}` set to `{{ $labels.status }}`
            runbook_url: Missing runbook
          labels:
            rulesgroup: cluster-api
            severity: warning
          expr: |
            sum((capi_machine_status_condition{status!="True"}) * on (name) group_left(node_name) sum(capi_machine_status_noderef) by (name, node_name)) by (cluster_name, node_name, type, status) > 0
          for: 15m
---
# Source: prometheus-alerts/templates/alerts/cluster-capacity-management-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-cluster-capacity-management-alerts
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: cluster-capacity-management-alerts
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 3 days.
      expr: |-
        (
          (node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100) < (100-75)
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 3*24*60*60) <= (node_filesystem_size_bytes*(1-75/100))
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: PersistentVolume75PercentInThreeDays
      annotations:
        message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} will go over 75 in three days.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
      expr: predict_linear(kubelet_volume_stats_available_bytes[24h], 3*24*60*60) <= (kubelet_volume_stats_capacity_bytes*(1-75/100))
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupCPU75PercentOver24h
      annotations:
        message: CPU usage has been over 75% on average over the span of 24h in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: avg by (label_elastisys_io_node_group,cluster) (sum by (instance) (rate(node_cpu_seconds_total{mode!='idle',cluster=~".*"}[24h])) / on (instance) instance:node_num_cpu:sum * on (instance) group_left (label_elastisys_io_node_group,cluster) label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)")) > 75/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupCPU95PercentOver1h
      annotations:
        message: CPU usage has been over 95% on average over the span of 1h in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: avg by (label_elastisys_io_node_group,cluster) (sum by (instance) (rate(node_cpu_seconds_total{mode!='idle',cluster=~".*"}[1h])) / on (instance) instance:node_num_cpu:sum * on (instance) group_left (label_elastisys_io_node_group,cluster) label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)")) > 95/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupMemory75PercentOver24h
      annotations:
        message: Memory usage has been over 75% on average over the span of 24h in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: avg by (label_elastisys_io_node_group,cluster) ((avg_over_time (instance:node_memory_utilisation:ratio{cluster=~".*"}[24h])) * on (instance) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)")) > 75/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupMemory85PercentOver1h
      annotations:
        message: Memory usage has been over 85% on average over the span of 1h in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: avg by (label_elastisys_io_node_group,cluster) ((avg_over_time (instance:node_memory_utilisation:ratio{cluster=~".*"}[1h])) * on (instance) group_left (label_elastisys_io_node_group) label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)")) > 85/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeCPU95PercentOver1h
      annotations:
        message: CPU usage has been over 95% on average over the span of 1h for the node {{ $labels.instance }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: sum by (instance) (rate(node_cpu_seconds_total{mode!='idle',cluster=~".*"}[1h])) / on (instance) instance:node_num_cpu:sum * on (instance) group_left (label_elastisys_io_node_group,cluster) label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)") > 95/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeMemory85PercentOver1h
      annotations:
        message: Memory usage has been over 85% on average over the span of 1h for the Node {{ $labels.instance }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: |-
        (
          avg_over_time (instance:node_memory_utilisation:ratio{cluster=~".*"}[1h]) * on (instance) group_left (label_elastisys_io_node_group)
          label_replace(kube_node_labels{label_elastisys_io_node_group!=""}, "instance", "$1", "node", "(.*)")
        ) > 85/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupCpuRequest80Percent
      annotations:
        message: Average CPU requests is over 80% in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: |-
        (
          avg by (label_elastisys_io_node_group,cluster) (sum by (node,cluster) (kube_pod_container_resource_requests{cluster=~".*",namespace=~".*",resource="cpu"}
        and
          on(pod, namespace, cluster) kube_pod_status_phase{cluster=~".*",namespace=~".*",phase="Running"} == 1)
        ) / (
          sum by(node,cluster) (kube_node_status_allocatable{cluster=~".*",resource="cpu"})) * on (node) group_left (label_elastisys_io_node_group)
          label_replace(kube_node_labels{label_elastisys_io_node_group!~''}, "instance", "$1", "node", "(.*)")
        ) >= 80/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeGroupMemoryRequest80Percent
      annotations:
        message: Average memory requests is over 80% in the Node Group {{ $labels.label_elastisys_io_node_group }} in Cluster {{ $labels.cluster }}.
        runbook_url: Missing runbook
      expr: |-
        (
          avg by (label_elastisys_io_node_group,cluster) (sum by (node,cluster) (kube_pod_container_resource_requests{cluster=~".*",namespace=~".*",resource="memory"}
        and
          on(pod, namespace, cluster) kube_pod_status_phase{cluster=~".*",namespace=~".*",phase="Running"} == 1)
        ) / (
          sum by(node,cluster) (kube_node_status_allocatable{cluster=~".*",resource="memory"})) * on (node) group_left (label_elastisys_io_node_group)
          label_replace(kube_node_labels{label_elastisys_io_node_group!~''}, "instance", "$1", "node", "(.*)")
        ) >= 80/100
      for: 5m
      labels:
        severity: warning
    - alert: NodeMissingElastisysNodeGroupLabel
      annotations:
        description: "The node {{ $labels.node }} in {{ $labels.cluster }}  is missing the 'elastisys.io/node-group' label. This might affect capacity management monitoring."
        summary: "Node {{ $labels.node }} in {{ $labels.cluster }} is missing the  'elastisys.io/node-group' label."
      expr: |
        kube_node_labels{label_elastisys_io_node_group=""}
      for: 60m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/config-reloaders.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-config-reloaders
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: config-reloaders
    rules:
    - alert: ConfigReloaderSidecarErrors
      annotations:
        description: 'Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
          As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors
        summary: config-reloader sidecar has not had a successful reload for 10m
      expr: max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
      for: 10m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/coredns.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-dns
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: dns
    rules:
    - alert: CorednsDown
      annotations:
        description: Coredns has disappeared from Prometheus target discovery.
        runbook_url: Missing runbook
        summary: Coredns has disappeared from Prometheus target discovery.
      expr: absent(up{job="coredns"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: CorednsLatencyHigh
      annotations:
        description: Coredns has 99th percentile latency of {{ $value }} seconds for server  {{ $labels.server }} zone  {{ $labels.zone }}.
        runbook_url: Missing runbook
        summary: Coredns is experiencing high 99th percentile latency.
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) without (instance,pod)) > 4
      for: 10m
      labels:
        severity: medium
    - alert: CorednsErrorsHigh
      annotations:
        description: Coredns is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.
        runbook_url: Missing runbook
        summary: Coredns is returning SERVFAIL.
      expr: |
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="coredns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="coredns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: medium
    - alert: CorednsErrorsHigh
      annotations:
        description: Coredns is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.
        runbook_url: Missing runbook
        summary: Coredns is returning SERVFAIL.
      expr: |
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="coredns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="coredns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: low
    - alert: CorednsForwardLatencyHigh
      annotations:
        description: Coredns has 99th percentile latency of {{ $value }} seconds forwarding requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Coredns is experiencing high latency forwarding requests.
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket{job="coredns"}[5m])) without (pod, instance, rcode)) > 4
      for: 10m
      labels:
        severity: medium
    - alert: CorednsForwardErrorsHigh
      annotations:
        description: Coredns is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Coredns is returning SERVFAIL for forward requests.
      expr: |
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="coredns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="coredns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: medium
    - alert: CorednsForwardErrorsHigh
      annotations:
        description: Coredns is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Coredns is returning SERVFAIL for forward requests.
      expr: |
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="coredns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="coredns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: low
    - alert: CorednsForwardHealthcheckFailureCount
      annotations:
        description: Coredns health checks have failed to upstream server  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Coredns health checks have failed to upstream server.
      expr: |
        sum without (pod, instance) (rate(coredns_forward_healthcheck_failures_total{job="coredns"}[5m])) > 0
      for: 10m
      labels:
        severity: low
    - alert: CorednsForwardHealthcheckBrokenCount
      annotations:
        description: Coredns health checks have failed for all upstream servers.
        runbook_url: Missing runbook
        summary: Coredns health checks have failed for all upstream servers.
      expr: |
        sum without (pod, instance) (rate(coredns_forward_healthcheck_broken_total{job="coredns"}[5m])) > 0
      for: 10m
      labels:
        severity: low
    - alert: CorednsPanicCount
      annotations:
        description: "Number of Coredns panics encountered VALUE = {{ $value }}  LABELS = {{ $labels }}"
        runbook_url: Missing runbook
        summary: Coredns Panic Count (instance {{ $labels.pod }})
      expr: increase(coredns_panics_total{job="coredns"}[1m]) > 0
      for: 1m
      labels:
        severity: low
    - alert: Node-Local-DnsDown
      annotations:
        description: Node-Local-Dns has disappeared from Prometheus target discovery.
        runbook_url: Missing runbook
        summary: Node-Local-Dns has disappeared from Prometheus target discovery.
      expr: absent(up{job="node-local-dns"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: Node-Local-DnsLatencyHigh
      annotations:
        description: Node-Local-Dns has 99th percentile latency of {{ $value }} seconds for server  {{ $labels.server }} zone  {{ $labels.zone }}.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is experiencing high 99th percentile latency.
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job="node-local-dns"}[5m])) without (instance,pod)) > 4
      for: 10m
      labels:
        severity: medium
    - alert: Node-Local-DnsErrorsHigh
      annotations:
        description: Node-Local-Dns is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is returning SERVFAIL.
      expr: |
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="node-local-dns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="node-local-dns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: medium
    - alert: Node-Local-DnsErrorsHigh
      annotations:
        description: Node-Local-Dns is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is returning SERVFAIL.
      expr: |
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="node-local-dns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, server, zone, view, rcode, plugin) (rate(coredns_dns_responses_total{job="node-local-dns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: low
    - alert: Node-Local-DnsForwardLatencyHigh
      annotations:
        description: Node-Local-Dns has 99th percentile latency of {{ $value }} seconds forwarding requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is experiencing high latency forwarding requests.
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket{job="node-local-dns"}[5m])) without (pod, instance, rcode)) > 4
      for: 10m
      labels:
        severity: medium
    - alert: Node-Local-DnsForwardErrorsHigh
      annotations:
        description: Node-Local-Dns is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is returning SERVFAIL for forward requests.
      expr: |
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="node-local-dns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="node-local-dns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: medium
    - alert: Node-Local-DnsForwardErrorsHigh
      annotations:
        description: Node-Local-Dns is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Node-Local-Dns is returning SERVFAIL for forward requests.
      expr: |
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="node-local-dns",rcode="SERVFAIL"}[5m]))
          /
        sum without (pod, instance, rcode) (rate(coredns_forward_responses_total{job="node-local-dns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: low
    - alert: Node-Local-DnsForwardHealthcheckFailureCount
      annotations:
        description: Node-Local-Dns health checks have failed to upstream server  {{ $labels.to }}.
        runbook_url: Missing runbook
        summary: Node-Local-Dns health checks have failed to upstream server.
      expr: |
        sum without (pod, instance) (rate(coredns_forward_healthcheck_failures_total{job="node-local-dns"}[5m])) > 0
      for: 10m
      labels:
        severity: low
    - alert: Node-Local-DnsForwardHealthcheckBrokenCount
      annotations:
        description: Node-Local-Dns health checks have failed for all upstream servers.
        runbook_url: Missing runbook
        summary: Node-Local-Dns health checks have failed for all upstream servers.
      expr: |
        sum without (pod, instance) (rate(coredns_forward_healthcheck_broken_total{job="node-local-dns"}[5m])) > 0
      for: 10m
      labels:
        severity: low
    - alert: Node-Local-DnsPanicCount
      annotations:
        description: "Number of Node-Local-Dns panics encountered VALUE = {{ $value }}  LABELS = {{ $labels }}"
        runbook_url: Missing runbook
        summary: Node-Local-Dns Panic Count (instance {{ $labels.pod }})
      expr: increase(coredns_panics_total{job="node-local-dns"}[1m]) > 0
      for: 1m
      labels:
        severity: low
---
# Source: prometheus-alerts/templates/alerts/daily-checks.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-daily-checks
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: daily-checks
    rules:
    - alert: Bucket36hActivityCheck
      annotations:
        message: The bucket {{ $labels.bucket }} haven't had any activity in 36h hours.
        runbook_url: Missing runbook
      expr: |-
        (
          sum(time() - s3_last_modified_object_date{bucket!~""}) by (bucket) / 3600 > 36
        )
      for: 1h
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/disk-perf.yaml
# Based on https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/monitoring/prometheus-ceph-v14-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-disk-perf
  namespace: "monitoring"
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  # The rules contain templates for prometheus, which breaks helm if trying
  # to render them. We have to either escape them or (as we do here) include
  # them without going through the template rendering.
  groups:
    - name: disk-performance
      rules:
        - alert: DiskReadWaitTimeHigh
          annotations:
            description: Disk {{ $labels.device }} Wait Time on {{ $labels.instance }} is {{ $value }}, check the workload
            summary: Disk {{ $labels.device }} Wait Time is high on {{ $labels.instance }}
            runbook_url: Missing runbook
            severity_level: warning
            storage_type: local
          expr: |
            (
            rate(node_disk_read_time_seconds_total{job="node-exporter"}[1m])
            /
            rate(node_disk_reads_completed_total{job="node-exporter"}[1m])
            ) > 0.01
          for: 10m
          labels:
            severity: warning
        - alert: DiskWriteWaitTimeHigh
          annotations:
            description: Disk {{ $labels.device }} Wait Time on {{ $labels.instance }} is {{ $value }}, check the workload
            summary: Disk {{ $labels.device }} Wait Time is high on {{ $labels.instance }}
            runbook_url: Missing runbook
            severity_level: warning
            storage_type: local
          expr: |
            (
            rate(node_disk_write_time_seconds_total{job="node-exporter"}[1m])
            /
            rate(node_disk_writes_completed_total{job="node-exporter"}[1m])
            ) > 0.1
          for: 10m
          labels:
            severity: warning
        - alert: DiskQueueSizeHigh
          annotations:
            description: Disk {{ $labels.device }} Queue Size on {{ $labels.instance }} is {{ $value }}, check the workload
            summary: Disk {{ $labels.device }} has very large Queue Size on {{ $labels.instance }}
            runbook_url: Missing runbook
            severity_level: warning
            storage_type: local
          expr: |
            (
            rate(node_disk_io_time_weighted_seconds_total{job="node-exporter"}[1m])
            ) > 0.5
          for: 10m
          labels:
            severity: warning
---
# Source: prometheus-alerts/templates/alerts/falco-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-falco-alert
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: falco-alert
    rules:
    - alert: FalcoAlert
      annotations:
        message: 'Pod: {{ $labels.pod }}, Rule: {{ $labels.rule }}'
        runbook_url: Missing runbook
      expr: |-
        falcosecurity_falcosidekick_falco_events_total != 0
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/fluentd.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-fluentd
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: fluentd
    rules:
    - alert: FluentdNodeDown
      expr: up{job=~"fluentd.*"} == 0
      for: 10m
      labels:
        service: fluentd
        severity: warning
      annotations:
        summary: fluentd cannot be scraped
        description: Prometheus could not scrape {{ $labels.job }} for more than 10 minutes
        runbook_url: Missing runbook
    - alert: FluentdNodeDown
      expr: up{job=~"fluentd.*"} == 0
      for: 30m
      labels:
        service: fluentd
        severity: critical
      annotations:
        summary: fluentd cannot be scraped
        description: Prometheus could not scrape {{ $labels.job }} for more than 30 minutes
        runbook_url: Missing runbook
    - alert: FluentdQueueLength
      expr: rate(fluentd_status_buffer_queue_length[15m]) > 0.3
      for: 5m
      labels:
        service: fluentd
        severity: warning
      annotations:
        summary: fluentd node are failing
        description: In the last 15 minutes, fluentd queues increased 30%. Current value is {{ $value }}
        runbook_url: Missing runbook
    - alert: FluentdQueueLength
      expr: rate(fluentd_status_buffer_queue_length[5m]) > 0.5
      for: 1m
      labels:
        service: fluentd
        severity: critical
      annotations:
        summary: fluentd node are critical
        description: In the last 5 minutes, fluentd queues increased 50%. Current value is {{ $value }}
        runbook_url: Missing runbook
    - alert: FluentdAvailableSpaceBuffer
      expr: sum(fluentd_output_status_buffer_available_space_ratio) by (pod, cluster, plugin_id) < 50
      for: 5m
      labels:
        service: fluentd
        severity: warning
      annotations:
        summary: fluentd available space in buffer is less than 50%
        description: For the last 5 minutes, the available buffer space for pod {{ $labels.pod }}, plugin-id {{ $labels.plugin_id }} in cluster {{ $labels.cluster }} is below 50%. Current value is {{ $value }}
        runbook_url: Missing runbook
    - alert: FluentdAvailableSpaceBuffer
      expr: sum(fluentd_output_status_buffer_available_space_ratio) by (pod, cluster, plugin_id) < 10
      for: 5m
      labels:
        service: fluentd
        severity: critical
      annotations:
        summary: fluentd available space in buffer is less than 90%
        description: For the last 5 minutes, the available buffer space for pod {{ $labels.pod }}, plugin-id {{ $labels.plugin_id }} in cluster {{ $labels.cluster }} is below 10%. Current value is {{ $value }}
        runbook_url: Missing runbook
    - alert: FluentdRecordsCountsHigh
      expr: >
        sum(rate(fluentd_output_status_emit_records{job="fluentd-metrics"}[5m]))
        BY (cluster, instance) >  (3 * sum(rate(fluentd_output_status_emit_records{job="fluentd-metrics"}[15m]))
        BY (cluster, instance))
      for: 10m
      labels:
        service: fluentd
        severity: warning
      annotations:
        summary: fluentd records count are critical
        description: In the last 5m, records counts increased 3 times, comparing to the latest 15 min.
        runbook_url: Missing runbook
    - alert: FluentdRetry
      expr: sum(increase(fluentd_status_retry_count[10m])) by (pod, cluster, service) > 0
      for: 30m
      labels:
        service: fluentd
        severity: warning
      annotations:
        description: Fluentd retry count has been  {{ $value }} for the last 10 minutes
        summary: Fluentd retry count has been  {{ $value }} for the last 10 minutes
        runbook_url: Missing runbook
    - alert: FluentdOutputError
      expr: sum(increase(fluentd_output_status_num_errors[10m])) by (pod, cluster, service) > 0
      for: 30m
      labels:
        service: fluentd
        severity: warning
      annotations:
        description: Fluentd output error count is {{ $value }} for the last 10 minutes
        summary: There have been Fluentd output error(s) for the last 10 minutes
        runbook_url: Missing runbook
---
# Source: prometheus-alerts/templates/alerts/general.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-general.rules
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDown
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
        summary: One or more targets are unreachable.
      expr: 100 * (count(up == 0) by (cluster, namespace, service, job) / count(up) by (cluster, namespace, service, job)) > 10
      for: 10m
      labels:
        severity: warning
    - alert: Watchdog
      annotations:
        description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.

          This alert is always firing, therefore it should always be firing in Alertmanager

          and always fire against a receiver. There are integrations with various notification

          mechanisms that send a notification when this alert is not firing. For example the

          "DeadMansSnitch" integration in PagerDuty.

          '
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
        summary: An alert that should always be firing to certify that Alertmanager is working properly.
      expr: vector(1)
      labels:
        severity: none
---
# Source: prometheus-alerts/templates/alerts/harbor.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-harbor
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: harbor
    rules:
    - alert: HarborCoreDown
      expr: |
        harbor_up{component="core"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor Core Is Down
        runbook_url: Missing runbook
    - alert: HarborDatabaseDown
      expr: |
        harbor_up{component="database"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor Database Is Down
        runbook_url: Missing runbook
    - alert: HarborRegistryDown
      expr: |
        harbor_up{component="registry"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor Registry Is Down
        runbook_url: Missing runbook
    - alert: HarborRedisDown
      expr: |
        harbor_up{component="redis"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor Redis Is Down
        runbook_url: Missing runbook
    - alert: HarborTrivyDown
      expr: |
        harbor_up{component="trivy"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor Trivy Is Down
        runbook_url: Missing runbook
    - alert: HarborJobServiceDown
      expr: |
        harbor_up{component="jobservice"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: Harbor JobService Is Down
        runbook_url: Missing runbook
    - alert: HarborStorageUsageAboveThreshold
      expr: |
        sum(harbor_project_quota_usage_byte) / (1024^3) > 1500
      for: 5m
      labels:
        severity: medium
      annotations:
        description: Total used storage for Harbor is high (above the threshold of 1500GB). This indicates that users have not set up artifact retention. If the total size of artifacts continues to grow, then Harbor will likely consume more resources and slow down. This might also cause issues with object storage quota at the infrastructure provider.
        runbook_url: Missing runbook
    - alert: HarborP99LatencyHigherThan10Seconds
      expr: |
        histogram_quantile(0.99,  sum  (rate(registry_http_request_duration_seconds_bucket[30m]) ) by (le)) > 10
      for: 5m
      labels:
        severity: low
      annotations:
        description: Harbor p99 latency is higher than 10 seconds
        runbook_url: Missing runbook
    - alert: HarborErrorRateHigh
      expr: |
        sum(rate(registry_http_requests_total{code=~"4..|5.."}[5m]))/sum(rate(registry_http_requests_total[5m])) > 0.15
      for: 5m
      labels:
        severity: low
      annotations:
        description: Harbor Error Rate is High
        runbook_url: Missing runbook
    - alert: HarborTotalNumberOfArtifactsAboveThreshold
      expr: |
        sum(harbor_project_artifact_total) > 3000
      for: 5m
      labels:
        severity: low
      annotations:
        description: Total number of artifacts is high (above alert threshold of 3000). This indicates that users have not set up artifact retention. If the number of artifacts continues to grow, then Harbor will likely consume more resources and slow down.
        runbook_url: Missing runbook
---
# Source: prometheus-alerts/templates/alerts/hnc.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-hnc
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
    - name: hnc-alert
      rules:
        - alert: HierarchicalNamespaceControllerNamespaceCondition
          annotations:
            description: |
              The Hierarchical Namespace Controller is reporting `{{ $value }}` Namespace condition(s) `{{ $labels.Condition }}` due to `{{ $labels.Reason }}` for longer than one minute.
            summary: |
              The Hierarchical Namespace Controller is reporting `{{ $value }}` Namespace condition(s) `{{ $labels.Condition }}` due to `{{ $labels.Reason }}` for longer than one minute.
            runbook_url: Missing runbook
          labels:
            rulesgroup: hnc
            severity: warning
          expr: |
            hnc_namespace_conditions > 0
          for: 1m
---
# Source: prometheus-alerts/templates/alerts/kube-state-metrics.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kube-state-metrics
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kube-state-metrics
    rules:
    - alert: KubeStateMetricsListErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors
        summary: kube-state-metrics is experiencing errors in list operations.
      expr: |-
        (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        severity: critical
    - alert: KubeStateMetricsWatchErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors
        summary: kube-state-metrics is experiencing errors in watch operations.
      expr: |-
        (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        severity: critical
    - alert: KubeStateMetricsShardingMismatch
      annotations:
        description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
        summary: kube-state-metrics sharding is misconfigured.
      expr: stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) != 0
      for: 15m
      labels:
        severity: critical
    - alert: KubeStateMetricsShardsMissing
      annotations:
        description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
        summary: kube-state-metrics shards are missing.
      expr: |-
        2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) - 1
          -
        sum( 2 ^ max by (shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"})) by (cluster)
        != 0
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/kubernetes-apps.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-apps
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
        summary: Pod is crash looping.
      expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
      for: 15m
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
        summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: |-
        sum by (cluster, namespace, pod) (
          max by(cluster, namespace, pod) (
            kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown"}
          ) * on(cluster, namespace, pod) group_left(owner_kind) topk by(cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeContainerOOMKilled
      annotations:
        description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} from {{ $labels.cluster }} has been OOMKilled {{ $value }} times in the last 30 minutes.
        runbook_url: Missing runbook
        summary: Kubernetes container OOMKilled.
      expr: increase(kube_pod_container_status_restarts_total[30m]) >= 2 and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[30m]) == 1
      for: 0m
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
        summary: Deployment generation mismatch due to possible roll-back
      expr: |-
        kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
            >
          kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
        ) and (
          changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
        summary: Deployment has not matched the expected number of replicas.
      expr: |-
        (
          kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |-
        kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
        summary: StatefulSet update has not been rolled out.
      expr: |-
        (
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
        summary: DaemonSet rollout is stuck.
      expr: |-
        (
          (
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          ) or (
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            0
          ) or (
            kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          ) or (
            kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
              !=
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          )
        ) and (
          changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeContainerWaiting
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}} has been in waiting state for longer than 1 hour.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
        summary: Pod container waiting longer than 1 hour
      expr: sum by (cluster, namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~".*"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
        summary: DaemonSet pods are not scheduled.
      expr: |-
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
        summary: DaemonSet pods are misscheduled.
      expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"} > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeJobNotCompleted
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
        summary: Job did not complete in time
      expr: |-
        time() - max by(cluster, namespace, job_name) (kube_job_status_start_time{job="kube-state-metrics", namespace=~".*"}
          and
        kube_job_status_active{job="kube-state-metrics", namespace=~".*"} > 0) > 43200
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
        summary: Job failed to complete.
      expr: kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeFailedEvictedPods
      annotations:
        description: '{{ $value }} Failed Evicted pods in {{ $labels.namespace }} namespace {{ $labels.cluster }} cluster'
        summary: Kubernetes failed evicted pods
        runbook_url: Missing runbook
      expr: sum by (namespace, cluster) (kube_pod_status_phase{phase="Failed"} > 0 and on(namespace, cluster) kube_pod_status_reason{reason="Evicted"} > 0) > 0
      for: 10m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/kubernetes-resources.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-resources
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
        summary: Cluster has overcommitted CPU resource requests.
      expr: |-
        sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{resource="cpu"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{resource="cpu"}) by (cluster)) > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeMemoryOvercommit
      annotations:
        description: Cluster has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
        summary: Cluster has overcommitted memory resource requests.
      expr: |-
        sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory"}) by (cluster) - max(kube_node_status_allocatable{resource="memory"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{resource="memory"}) by (cluster) - max(kube_node_status_allocatable{resource="memory"}) by (cluster)) > 0
      for: 10m
      labels:
        severity: warning
    - alert: CPUThrottlingHigh
      annotations:
        description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
        summary: Processes experience elevated CPU throttling.
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (cluster, container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (cluster, container, pod, namespace)
          > ( 25 / 100 )
      for: 15m
      labels:
        severity: info
---
# Source: prometheus-alerts/templates/alerts/kubernetes-storage.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-storage
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
        summary: PersistentVolume is filling up.
      expr: |-
        (
          kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      for: 1m
      labels:
        severity: critical
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
        summary: PersistentVolume is filling up.
      expr: |-
        (
          kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
      for: 1h
      labels:
        severity: warning
    - alert: KubePersistentVolumeErrors
      annotations:
        description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
        summary: PersistentVolume is having issues with provisioning.
      expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/kubernetes-system-apiserver.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-system-apiserver
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-system-apiserver
    rules:
    - alert: KubeClientCertificateExpiration
      annotations:
        description: A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
        summary: Client certificate is about to expire.
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
      labels:
        severity: warning
    - alert: KubeClientCertificateExpiration
      annotations:
        description: A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
        summary: Client certificate is about to expire.
      expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
      labels:
        severity: critical
    - alert: KubeAggregatedAPIErrors
      annotations:
        description: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors
        summary: An aggregated API has reported errors.
      expr: sum by(cluster, name, namespace)(increase(aggregator_unavailable_apiservice_total[10m])) > 4
      labels:
        severity: warning
    - alert: KubeAggregatedAPIDown
      annotations:
        description: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown
        summary: An aggregated API is down.
      expr: (1 - max by(cluster, name, namespace)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85
      for: 5m
      labels:
        severity: warning
    - alert: KubeAPIDown
      annotations:
        description: KubeAPI has disappeared from Prometheus target discovery.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown
        summary: Target disappeared from Prometheus target discovery.
      expr: absent(up{job="apiserver"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: KubeAPITerminatedRequests
      annotations:
        description: The apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests
        summary: The apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
      expr: sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m])) by (cluster)  / (  sum(rate(apiserver_request_total{job="apiserver"}[10m])) by (cluster) + sum(rate(apiserver_request_terminations_total{job="apiserver"}[10m])) by (cluster) ) > 0.20
      for: 5m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/kubernetes-system-kube-proxy.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kube-proxy
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-system-kube-proxy
    rules:
    - alert: KubeProxyDown
      annotations:
        description: KubeProxy has disappeared from Prometheus target discovery.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown
        summary: Target disappeared from Prometheus target discovery.
      expr: absent(up{job="kube-proxy"} == 1)
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/kubernetes-system-kubelet.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-system-kubelet
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-system-kubelet
    rules:
    - alert: KubeNodeNotReady
      annotations:
        description: '{{ $labels.node }} has been unready for more than 15 minutes.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
        summary: Node is not ready.
      expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeNodeUnreachable
      annotations:
        description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
        summary: Node is unreachable.
      expr: (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeletTooManyPods
      annotations:
        description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
        summary: Kubelet is running at capacity.
      expr: |-
        count by(cluster,node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        )
        /
        max by(cluster,node) (
          kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
        ) > 0.95
      for: 15m
      labels:
        severity: warning
    - alert: KubeNodeReadinessFlapping
      annotations:
        description: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
        summary: Node readiness status is flapping.
      expr: sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (cluster,node) > 2
      for: 15m
      labels:
        severity: warning
    - alert: KubeletPlegDurationHigh
      annotations:
        description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        severity: warning
    - alert: KubeletPodStartUpLatencyHigh
      annotations:
        description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
        summary: Kubelet Pod startup latency is too high.
      expr: histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster,instance,le)) * on(cluster,instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      for: 15m
      labels:
        severity: warning
    - alert: KubeletClientCertificateExpiration
      annotations:
        description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
        summary: Kubelet client certificate is about to expire.
      expr: kubelet_certificate_manager_client_ttl_seconds < 604800
      labels:
        severity: warning
    - alert: KubeletClientCertificateExpiration
      annotations:
        description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
        summary: Kubelet client certificate is about to expire.
      expr: kubelet_certificate_manager_client_ttl_seconds < 86400
      labels:
        severity: critical
    - alert: KubeletServerCertificateExpiration
      annotations:
        description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
        summary: Kubelet server certificate is about to expire.
      expr: kubelet_certificate_manager_server_ttl_seconds < 604800
      labels:
        severity: warning
    - alert: KubeletServerCertificateExpiration
      annotations:
        description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
        summary: Kubelet server certificate is about to expire.
      expr: kubelet_certificate_manager_server_ttl_seconds < 86400
      labels:
        severity: critical
    - alert: KubeletClientCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
        summary: Kubelet has failed to renew its client certificate.
      expr: increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeletServerCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
        summary: Kubelet has failed to renew its server certificate.
      expr: increase(kubelet_server_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeletDown
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        summary: Target disappeared from Prometheus target discovery.
      expr: absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      for: 15m
      labels:
        severity: critical
    - alert: KubletDownForAutoscaledNodeFor15m
      annotations:
        description: The kubelet job of an existing autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 15m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        summary: The kubelet job of an existing autoscaled cluster {{ $labels.node }} in cluster {{ $labels.node }} is down for the last 15m.
      expr: |-
          sum by (node, cluster) (kube_node_info and on (node) up{job="kubelet",metrics_path="/metrics"} == 0)
          and on (node)
          kube_node_labels{ label_node_restriction_kubernetes_io_autoscaled_node_type!="" }
      for: 15m
      labels:
        severity: warning
    - alert: KubletDownForAutoscaledNodeFor30m
      annotations:
        description: The kubelet job of an existing autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 30m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        summary: The kubelet job of an existing autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 30m.
      expr: |-
          sum by (node, cluster) (kube_node_info and on (node) up{job="kubelet",metrics_path="/metrics"} == 0)
          and on (node)
          kube_node_labels{ label_node_restriction_kubernetes_io_autoscaled_node_type!="" }
      for: 30m
      labels:
        severity: critical
    - alert: KubletDownForNonAutoscaledNodeFor5m
      annotations:
        description: The kubelet job of an existing non-autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 5m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        summary: The kubelet job of an existing non-autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 5m.
      expr: |-
          sum by (node, cluster) (kube_node_info and on (node) up{job="kubelet",metrics_path="/metrics"} == 0)
          unless on (node)
          kube_node_labels{ label_node_restriction_kubernetes_io_autoscaled_node_type!="" }
      for: 5m
      labels:
        severity: warning
    - alert: KubletDownForNonAutoscaledNodeFor15m
      annotations:
        description: The kubelet job of an existing non-autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 15m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
        summary: The kubelet job of an existing non-autoscaled node {{ $labels.node }} in cluster {{ $labels.cluster }} is down for the last 15m.
      expr: |-
          sum by (node, cluster) (kube_node_info and on (node) up{job="kubelet",metrics_path="/metrics"} == 0)
          unless on (node)
          kube_node_labels{ label_node_restriction_kubernetes_io_autoscaled_node_type!="" }
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/kubernetes-system.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubernetes-system
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: kubernetes-system
    rules:
    - alert: KubeVersionMismatch
      annotations:
        description: There are {{ $value }} different semantic versions of Kubernetes components running.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
        summary: Different semantic versions of Kubernetes components running.
      expr: count(count by (cluster, git_version) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns|openstack-monitoring"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) by (cluster) > 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeClientErrors
      annotations:
        description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors
        summary: Kubernetes API server client is experiencing errors.
      expr: |-
        (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (cluster, instance, job, namespace)
          /
        sum(rate(rest_client_requests_total[5m])) by (cluster, instance, job, namespace))
        > 0.01
      for: 15m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/missing-metrics-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
  name: sc-alerts-prometheus-alerts-missing-metrics
  namespace: "monitoring"
spec:
  groups:
    - name: metrics-from-cluster-is-missing
      rules:
      - alert: MetricsFromWcClusterIsMissing
        annotations:
          description: Metrics from the worker cluster is missing
          summary: Metrics from the worker cluster is not being received.
          runbook_url: Missing runbook
        expr: |
          absent(prometheus_tsdb_head_series{job="kube-prometheus-stack-prometheus",tenant_id!~".*-sc"}) > 0 or prometheus_tsdb_head_series{job="kube-prometheus-stack-prometheus",tenant_id!~".*-sc"} == 0
        for: 5m
        labels:
          severity: critical
      - alert: MetricsFromScClusterIsMissing
        annotations:
          description: Metrics from the service cluster is missing
          summary: Metrics from the service cluster is not being received.
          runbook_url: Missing runbook
        expr: |
          absent(prometheus_tsdb_head_series{job="kube-prometheus-stack-prometheus",tenant_id!~".*-wc"}) > 0 or prometheus_tsdb_head_series{job="kube-prometheus-stack-prometheus",tenant_id!~".*-wc"} == 0
        for: 15m
        labels:
          severity: critical
---
# Source: prometheus-alerts/templates/alerts/node-exporter.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-node-exporter
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: node-exporter
    rules:
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        summary: Filesystem has less than 20% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 20
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 30m
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        summary: Filesystem has less than 10% space left.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 10
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 30m
      labels:
        severity: critical
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |-
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 5
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 2h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        summary: Filesystem has less than 5% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
        summary: Filesystem has less than 3% inodes left.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} / node_filesystem_files{job="node-exporter",fstype!="",device=~".*",device!~"",instance=~".*",instance!~""} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!="",device=~".*",device!~"",instance=~".*",instance!~""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
        summary: Network interface is reporting many receive errors.
      expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
        summary: Network interface is reporting many transmit errors.
      expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
    - alert: NodeHighNumberConntrackEntriesUsed
      annotations:
        description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
        summary: Number of conntrack are getting close to the limit.
      expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
    - alert: NodeTextFileCollectorScrapeError
      annotations:
        description: Node Exporter text file collector failed to scrape.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
        summary: Node Exporter text file collector failed to scrape.
      expr: node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
    - alert: NodeClockSkewDetected
      annotations:
        description: Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
        summary: Clock skew detected.
      expr: |-
        (
          node_timex_offset_seconds{job="node-exporter"} > 0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds{job="node-exporter"} < -0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
        )
      for: 10m
      labels:
        severity: warning
    - alert: NodeClockNotSynchronising
      annotations:
        description: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
        summary: Clock not synchronising.
      expr: |-
        min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
        and
        node_timex_maxerror_seconds{job="node-exporter"} >= 16
      for: 10m
      labels:
        severity: warning
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |-
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
        )
      for: 15m
      labels:
        severity: warning
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |-
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
        )
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/node-network.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-node-network
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: node-network
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        description: Network interface "{{ $labels.device }}" changing it's up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
        summary: Network interface is often changing it's status
      expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/opensearch.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-opensearch-alerts
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: opensearch-alerts
    rules:
    - alert: OpenSearchTooFewNodesRunning
      expr: elasticsearch_cluster_health_number_of_nodes{namespace="opensearch-system"} < 3
      for: 15m
      labels:
        severity: critical
      annotations:
        description: There are only {{ $value }}  OpenSearch nodes running
        summary: OpenSearch running on less than 3 nodes
        runbook_url: Missing runbook
    - alert: OpenSearchHeapTooHigh
      expr: elasticsearch_jvm_memory_used_bytes{namespace="opensearch-system",area="heap"} / elasticsearch_jvm_memory_max_bytes{namespace="opensearch-system",area="heap"} > 0.9
      for: 15m
      labels:
        severity: critical
      annotations:
        description: The heap usage is over 90% for 15m
        summary: OpenSearch node {{ $labels.node}} heap usage is high
        runbook_url: Missing runbook
    - alert: OpenSearchFieldLimit
      expr: (sum(max_over_time(elasticsearch_indices_mappings_stats_fields{namespace="opensearch-system"}[5m])) by (index) / sum(max_over_time(elasticsearch_indices_settings_total_fields{namespace="opensearch-system"}[5m])) by (index)) * 100 > 80
      for: 15m
      labels:
        severity: warning
      annotations:
        description: Index {{ $labels.index }} is using {{ $value }} percent of max field limit
        summary: Index {{ $labels.index }} is using {{ $value }} percent of max field limit
        runbook_url: Missing runbook
    - alert: OpenSearchFieldLimit
      expr: (sum(max_over_time(elasticsearch_indices_mappings_stats_fields{namespace="opensearch-system"}[5m])) by (index) / sum(max_over_time(elasticsearch_indices_settings_total_fields{namespace="opensearch-system"}[5m])) by (index)) * 100 > 95
      for: 15m
      labels:
        severity: critical
      annotations:
        description: Index {{ $labels.index }} is using {{ $value }} percent of max field limit
        summary: Index {{ $labels.index }} is using {{ $value }} percent of max field limit
        runbook_url: Missing runbook
    - alert: OpensearchClusterYellow
      expr: elasticsearch_cluster_health_status{namespace="opensearch-system",color="yellow"} == 1
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: Opensearch Cluster Yellow (instance {{ $labels.instance }})
        description: Opensearch Cluster is in a Yellow status VALUE = {{ $value }}  LABELS = {{ $labels }}
        runbook_url: Missing runbook
    - alert: OpensearchClusterRed
      expr: elasticsearch_cluster_health_status{namespace="opensearch-system",color="red"} == 1
      for: 15m
      labels:
        severity: critical
      annotations:
        summary: Opensearch Cluster Red (instance {{ $labels.instance }})
        description: Opensearch Cluster is in a Red status VALUE = {{ $value }}  LABELS = {{ $labels }}
        runbook_url: Missing runbook
    - alert: OpenSearchKubernetes-DefaultIndexSizeOverLimit
      expr: (elasticsearch_indices_store_size_bytes_primary{namespace="opensearch-system", index=~"kubernetes-default.+"} / (1024 ^ 2) > 5500) * clamp(rate(elasticsearch_indices_docs_primary{namespace="opensearch-system", index=~"kubernetes-default.+"}[4h]) > 0, 1, 1)
      for: 15m
      labels:
        severity: warning
      annotations:
        message: Active primary shard size for index {{ $labels.index }} has increased over the limit of 5500MB, current size is {{ $value | printf "%.0f" }}MB
        description: This indicates a problem with index state management preventing the alias to roll over to a new index.
        runbook_url: Missing runbook
    - alert: OpenSearchKubeaudit-DefaultIndexSizeOverLimit
      expr: (elasticsearch_indices_store_size_bytes_primary{namespace="opensearch-system", index=~"kubeaudit-default.+"} / (1024 ^ 2) > 5500) * clamp(rate(elasticsearch_indices_docs_primary{namespace="opensearch-system", index=~"kubeaudit-default.+"}[4h]) > 0, 1, 1)
      for: 15m
      labels:
        severity: warning
      annotations:
        message: Active primary shard size for index {{ $labels.index }} has increased over the limit of 5500MB, current size is {{ $value | printf "%.0f" }}MB
        description: This indicates a problem with index state management preventing the alias to roll over to a new index.
        runbook_url: Missing runbook
    - alert: OpenSearchOther-DefaultIndexSizeOverLimit
      expr: (elasticsearch_indices_store_size_bytes_primary{namespace="opensearch-system", index=~"other-default.+"} / (1024 ^ 2) > 400) * clamp(rate(elasticsearch_indices_docs_primary{namespace="opensearch-system", index=~"other-default.+"}[4h]) > 0, 1, 1)
      for: 15m
      labels:
        severity: warning
      annotations:
        message: Active primary shard size for index {{ $labels.index }} has increased over the limit of 400MB, current size is {{ $value | printf "%.0f" }}MB
        description: This indicates a problem with index state management preventing the alias to roll over to a new index.
        runbook_url: Missing runbook
    - alert: OpenSearchAuthlog-DefaultIndexSizeOverLimit
      expr: (elasticsearch_indices_store_size_bytes_primary{namespace="opensearch-system", index=~"authlog-default.+"} / (1024 ^ 2) > 2) * clamp(rate(elasticsearch_indices_docs_primary{namespace="opensearch-system", index=~"authlog-default.+"}[4h]) > 0, 1, 1)
      for: 15m
      labels:
        severity: warning
      annotations:
        message: Active primary shard size for index {{ $labels.index }} has increased over the limit of 2MB, current size is {{ $value | printf "%.0f" }}MB
        description: This indicates a problem with index state management preventing the alias to roll over to a new index.
        runbook_url: Missing runbook
---
# Source: prometheus-alerts/templates/alerts/openstack.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-openstack
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: openstack
    rules:
    - alert: OpenStackCloudControllerDown
      annotations:
        summary: OpenStack cloud controller manager has disappeared.
        runbook_url: Missing runbook
      expr: (group(present_over_time(up{job="openstack-monitoring"}[6h])) by (cluster) unless group(up{job="openstack-monitoring"}) by (cluster)) or (absent(up{job="openstack-monitoring"} == 1))
      for: 10m
      labels:
        severity: critical
    - alert: OpenStackApiRequestFailed
      annotations:
        summary: Failed OpenStack API call.
        runbook_url: Missing runbook
      expr: rate(openstack_api_request_errors_total[5m]) > 0.01
      for: 5m
      labels:
        severity: high
    - alert: OpenStackApiRequestDuration
      annotations:
        summary: OpenStack API has taken longer than 15 seconds.
        runbook_url: Missing runbook
      expr: rate(openstack_api_request_duration_seconds_sum[5m]) / rate(openstack_api_request_duration_seconds_count[5m]) > 15
      for: 5m
      labels:
        severity: high
    - alert: OpenStackApiRequestTotal
      annotations:
        summary: Too high amount of OpenStack API calls.
        runbook_url: Missing runbook
      expr: (delta(openstack_api_requests_total[5m]))/5 > 20
      for: 5m
      labels:
        severity: high
    - alert: OpenStackReconcileFailed
      annotations:
        summary: Increased reconciliation errors.
        runbook_url: Missing runbook
      expr: rate(cloudprovider_openstack_reconcile_errors_total[5m]) > 0
      for: 10m
      labels:
        severity: high
    - alert: OpenStackReconcileDuration
      annotations:
        summary: Reconciliation has taken longer than 10 minutes.
        runbook_url: Missing runbook
      expr: rate(cloudprovider_openstack_reconcile_duration_seconds_sum[5m]) / rate(cloudprovider_openstack_reconcile_duration_seconds_count[5m]) > 600
      for: 10m
      labels:
        severity: high
---
# Source: prometheus-alerts/templates/alerts/packets-dropped.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-packets-dropped
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: packets-dropped
    rules:
    - alert: FrequentPacketsDroppedToWorkload
      annotations:
        description: Number of packets dropped to workload {{ $labels.exported_pod }} in namespace {{ $labels.pod_namespace }}, because no policies matched them
        summary: Number of packets dropped to workload in {{ $labels.cluster }} cluster
        runbook_url: Missing runbook
      expr: |
        increase(no_policy_drop_counter{type="tw"}[15m]) > 0
      for: 5m
      labels:
        severity: warning
    - alert: ScarcePacketsDroppedToWorkload
      annotations:
        description: Number of packets dropped to workload {{ $labels.exported_pod }} in namespace {{ $labels.pod_namespace }}, because no policies matched them
        summary: Number of packets dropped to workload in {{ $labels.cluster }} cluster
        runbook_url: Missing runbook
      expr: |
        increase(no_policy_drop_counter{type="tw"}[3h]) > 20
      for: 6h
      labels:
        severity: info
    - alert: FrequentPacketsDroppedFromWorkload
      annotations:
        description: Number of packets dropped from workload {{ $labels.exported_pod }} in namespace {{ $labels.pod_namespace }}, because no policies matched them
        summary: Number of packets dropped from workload in {{ $labels.cluster }} cluster
        runbook_url: Missing runbook
      expr: |
        increase(no_policy_drop_counter{type="fw"}[15m]) > 0
      for: 5m
      labels:
        severity: warning
    - alert: ScarcePacketsDroppedFromWorkload
      annotations:
        description: Number of packets dropped from workload {{ $labels.exported_pod }} in namespace {{ $labels.pod_namespace }}, because no policies matched them
        summary: Number of packets dropped from workload in {{ $labels.cluster }} cluster
        runbook_url: Missing runbook
      expr: |
        increase(no_policy_drop_counter{type="fw"}[3h]) > 20
      for: 6h
      labels:
        severity: info
---
# Source: prometheus-alerts/templates/alerts/prometheus-operator.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-prometheus-operator
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorListErrors
      annotations:
        description: Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors
        summary: Errors while performing list operations in controller.
      expr: (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorWatchErrors
      annotations:
        description: Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors
        summary: Errors while performing watch operations in controller.
      expr: (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorSyncFailed
      annotations:
        description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed
        summary: Last controller reconciliation failed
      expr: min_over_time(prometheus_operator_syncs{status="failed",job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        description: '{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
        summary: Errors while reconciling controller.
      expr: (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
        summary: Errors while reconciling Prometheus.
      expr: rate(prometheus_operator_node_address_lookup_errors_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNotReady
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready
        summary: Prometheus operator not ready
      expr: min by(cluster, namespace, controller) (max_over_time(prometheus_operator_ready{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) == 0)
      for: 5m
      labels:
        severity: warning
    - alert: PrometheusOperatorRejectedResources
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources
        summary: Resources rejected by Prometheus operator
      expr: min_over_time(prometheus_operator_managed_resources{state="rejected",job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
---
# Source: prometheus-alerts/templates/alerts/prometheus.yaml
# We should consider making this more generic so we can pick up any prometheus instance
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-prometheus
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
  - name: prometheus
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig
        summary: Failed Prometheus configuration reload.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull
        summary: Prometheus alert notification queue predicted to run full in less than 30m.
      expr: |-
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
        summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      expr: |-
        (
          rate(prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        /
          rate(prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) < 1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing
        summary: Prometheus has issues reloading blocks from disk.
      expr: increase(prometheus_tsdb_reloads_failures_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing
        summary: Prometheus has issues compacting blocks.
      expr: increase(prometheus_tsdb_compactions_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples
        summary: Prometheus is not ingesting samples.
      expr: |-
        (
          rate(prometheus_tsdb_head_samples_appended_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) <= 0
        and
          (
            sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="kube-prometheus-stack-prometheus",namespace="monitoring"}) > 0
          or
            sum without(rule_group) (prometheus_rule_group_rules{job="kube-prometheus-stack-prometheus",namespace="monitoring"}) > 0
          )
        )
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: rate(prometheus_target_scrapes_sample_out_of_order_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusRemoteStorageFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures
        summary: Prometheus fails to send samples to remote storage.
      expr: |-
        (
          (rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]))
        /
          (
            (rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]))
          +
            (rate(prometheus_remote_storage_succeeded_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]))
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteBehind
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind
        summary: Prometheus remote write is behind.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        - ignoring(remote_name, url) group_right
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        )
        > 120
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="kube-prometheus-stack-prometheus",namespace="monitoring"}` $labels.instance | query | first | value }}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards
        summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
      expr: |-
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures
        summary: Prometheus is failing rule evaluations.
      expr: increase(prometheus_rule_evaluation_failures_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: increase(prometheus_rule_group_iterations_missed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusTargetLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded the configured target_limit.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit
        summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
      expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusLabelLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit
        summary: Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
      expr: increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusScrapeBodySizeLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit
        summary: Prometheus has dropped some targets that exceeded body size limit.
      expr: increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusScrapeSampleLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit
        summary: Prometheus has failed scrapes that have exceeded the configured sample limit.
      expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusTargetSyncFailure
      annotations:
        description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure
        summary: Prometheus has failed to sync targets.
      expr: increase(prometheus_target_sync_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[30m]) > 0
      for: 5m
      labels:
        severity: critical
    - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
      annotations:
        description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      expr: |-
        min without (alertmanager) (
          rate(prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus",namespace="monitoring",alertmanager!=""}[5m])
        /
          rate(prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus",namespace="monitoring",alertmanager!=""}[5m])
        )
        * 100
        > 3
      for: 15m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/thanos-ruler.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-thanos-ruler
  namespace: "thanos"
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  # Found from https://github.com/thanos-io/thanos/blob/main/examples/alerts/alerts.yaml
  # But edited to match our names for jobs, thanos-ruler section
  groups:
  - name: thanos-ruler
    rules:
    - alert: ThanosRuleIsDown
      annotations:
        description: ThanosRule has disappeared. Prometheus target for the component
          cannot be discovered.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-component-absent
        summary: Thanos component has disappeared.
      expr: |
        absent(up{job=~".*thanos-receiver-ruler.*"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ThanosRuleQueueIsDroppingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} is failing to queue alerts.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is failing to queue alerts.
      expr: |
        sum by (cluster, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job=~".*thanos-receiver-ruler.*"}[5m])) > 0
      for: 5m
      labels:
        severity: critical
    - alert: ThanosRuleSenderIsFailingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} is failing to send alerts to alertmanager.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is failing to send alerts to alertmanager.
      expr: |
        sum by (cluster, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job=~".*thanos-receiver-ruler.*"}[5m])) > 0
      for: 5m
      labels:
        severity: critical
    - alert: ThanosRuleHighRuleEvaluationFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} is failing to evaluate rules.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is failing to evaluate rules.
      expr: |
        (
          sum by (cluster, job, instance) (rate(prometheus_rule_evaluation_failures_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        /
          sum by (cluster, job, instance) (rate(prometheus_rule_evaluations_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: critical
    - alert: ThanosRuleHighRuleEvaluationWarnings
      annotations:
        description: Thanos Rule {{$labels.instance}} has high number of evaluation
          warnings.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule has high number of evaluation warnings.
      expr: |
        sum by (cluster, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job=~".*thanos-receiver-ruler.*"}[5m])) > 0
      for: 15m
      labels:
        severity: info
    - alert: ThanosRuleRuleEvaluationLatencyHigh
      annotations:
        description: Thanos Rule {{$labels.instance}} has higher evaluation latency
          than interval for {{$labels.rule_group}}.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule has high rule evaluation latency.
      expr: |
        (
          sum by (cluster, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job=~".*thanos-receiver-ruler.*"})
        >
          sum by (cluster, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job=~".*thanos-receiver-ruler.*"})
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleGrpcErrorRate
      annotations:
        description: Thanos Rule {{$labels.job}} is failing to handle {{$value | humanize}}%
          of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is failing to handle grpc requests.
      expr: |
        (
          sum by (cluster, job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-receiver-ruler.*"}[5m]))
        /
          sum by (cluster, job, instance) (rate(grpc_server_started_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosRuleConfigReloadFailure
      annotations:
        description: Thanos Rule {{$labels.job}} has not been able to reload its configuration.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule has not been able to reload configuration.
      expr: avg by (cluster, job, instance) (thanos_rule_config_last_reload_successful{job=~".*thanos-receiver-ruler.*"})
        != 1
      for: 5m
      labels:
        severity: info
    - alert: ThanosRuleQueryHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.job}} has {{$value | humanize}}% of failing
          DNS queries for query endpoints.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (cluster, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        /
          sum by (cluster, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosRuleAlertmanagerHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} has {{$value | humanize}}% of
          failing DNS queries for Alertmanager endpoints.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (cluster, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        /
          sum by (cluster, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job=~".*thanos-receiver-ruler.*"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosRuleNoEvaluationFor10Intervals
      annotations:
        description: Thanos Rule {{$labels.job}} has {{$value | humanize}}% rule groups
          that did not evaluate for at least 10x of their expected interval.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
      expr: |
        time() -  max by (cluster, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job=~".*thanos-receiver-ruler.*"})
        >
        10 * max by (cluster, job, instance, group) (prometheus_rule_group_interval_seconds{job=~".*thanos-receiver-ruler.*"})
      for: 5m
      labels:
        severity: info
    - alert: ThanosNoRuleEvaluations
      annotations:
        description: Thanos Rule {{$labels.instance}} did not perform any rule evaluations
          in the past 10 minutes.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-rule
        summary: Thanos Rule did not perform any rule evaluations.
      expr: |
        sum by (cluster, job, instance) (rate(prometheus_rule_evaluations_total{job=~".*thanos-receiver-ruler.*"}[5m])) <= 0
          and
        sum by (cluster, job, instance) (thanos_rule_loaded_rules{job=~".*thanos-receiver-ruler.*"}) > 0
      for: 5m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/thanos.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-thanos
  namespace: "thanos"
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  # Found from https://github.com/thanos-io/thanos/blob/main/examples/alerts/alerts.yaml
  # But edited to match our names for jobs, all but thanos-ruler section
  groups:
  - name: thanos-compact
    rules:
    - alert: ThanosCompactMultipleRunning
      annotations:
        description: No more than one Thanos Compact instance should be running at once.
          There are {{$value}} instances running.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-compact
        summary: Thanos Compact has multiple instances running.
      expr: sum by (cluster, job) (up{job=~".*thanos-receiver-compact.*"}) > 1
      for: 5m
      labels:
        severity: warning
    - alert: ThanosCompactHalted
      annotations:
        description: Thanos Compact {{$labels.job}} has failed to run and now is halted.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-compact
        summary: Thanos Compact has failed to run and is now halted.
      expr: thanos_compact_halted{job=~".*thanos-receiver-compact.*"} == 1
      for: 5m
      labels:
        severity: warning
    - alert: ThanosCompactHighCompactionFailures
      annotations:
        description: Thanos Compact {{$labels.job}} is failing to execute {{$value | humanize}}% of compactions.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-compact
        summary: Thanos Compact is failing to execute compactions.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_compact_group_compactions_failures_total{job=~".*thanos-receiver-compact.*"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_compact_group_compactions_total{job=~".*thanos-receiver-compact.*"}[5m]))
        * 100 > 5
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosCompactBucketHighOperationFailures
      annotations:
        description: Thanos Compact {{$labels.job}} Bucket is failing to execute {{$value | humanize}}% of operations.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-compact
        summary: Thanos Compact Bucket is having a high number of operation failures.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-receiver-compact.*"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_objstore_bucket_operations_total{job=~".*thanos-receiver-compact.*"}[5m]))
        * 100 > 5
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosCompactHasNotRun
      annotations:
        description: Thanos Compact {{$labels.job}} has not uploaded anything for 24
          hours.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-compact
        summary: Thanos Compact has not uploaded anything for last 24 hours.
      expr: (time() - max by (cluster, job) (max_over_time(thanos_objstore_bucket_last_successful_upload_time{job=~".*thanos-receiver-compact.*"}[24h])))
        / 60 / 60 > 24
      labels:
        severity: warning
  - name: thanos-query
    rules:
    - alert: ThanosQueryHttpRequestQueryErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of "query" requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (cluster, job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query-query", handler="query"}[5m]))
        /
          sum by (cluster, job) (rate(http_requests_total{job=~".*thanos-query-query", handler="query"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
    - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of "query_range" requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (cluster, job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query-query", handler="query_range"}[5m]))
        /
          sum by (cluster, job) (rate(http_requests_total{job=~".*thanos-query-query", handler="query_range"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
    - alert: ThanosQueryGrpcServerErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}%
          of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (cluster, job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query-query"}[5m]))
        /
          sum by (cluster, job) (rate(grpc_server_started_total{job=~".*thanos-query-query"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosQueryGrpcClientErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} is failing to send {{$value | humanize}}%
          of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query is failing to send requests.
      expr: |
        (
          sum by (cluster, job) (rate(grpc_client_handled_total{grpc_code!="OK", job=~".*thanos-query-query"}[5m]))
        /
          sum by (cluster, job) (rate(grpc_client_started_total{job=~".*thanos-query-query"}[5m]))
        ) * 100 > 5
      for: 5m
      labels:
        severity: warning
    - alert: ThanosQueryHighDNSFailures
      annotations:
        description: Thanos Query {{$labels.job}} have {{$value | humanize}}% of failing
          DNS queries for store endpoints.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query is having high number of DNS failures.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_query_store_apis_dns_failures_total{job=~".*thanos-query-query"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_query_store_apis_dns_lookups_total{job=~".*thanos-query-query"}[5m]))
        ) * 100 > 1
      for: 15m
      labels:
        severity: warning
    - alert: ThanosQueryInstantLatencyHigh
      annotations:
        description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for instant queries.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query has high latency for queries.
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query-query", handler="query"}[5m]))) > 40
        and
          sum by (cluster, job) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query-query", handler="query"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
    - alert: ThanosQueryRangeLatencyHigh
      annotations:
        description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for range queries.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-query
        summary: Thanos Query has high latency for queries.
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query-query", handler="query_range"}[5m]))) > 90
        and
          sum by (cluster, job) (rate(http_request_duration_seconds_count{job=~".*thanos-query-query", handler="query_range"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
  - name: thanos-receive
    rules:
    - alert: ThanosReceiveHttpRequestErrorRateHigh
      annotations:
        description: Thanos Receive {{$labels.job}} is failing to handle {{$value | humanize}}% of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive is failing to handle requests.
      expr: |
        (
          sum by (cluster, job) (rate(http_requests_total{code=~"5..", job=~".*thanos-receiver-receive.*", handler="receive"}[5m]))
        /
          sum by (cluster, job) (rate(http_requests_total{job=~".*thanos-receiver-receive.*", handler="receive"}[5m]))
        ) * 100 > 5
      for: 20m
      labels:
        severity: critical
    - alert: ThanosReceiveHttpRequestLatencyHigh
      annotations:
        description: Thanos Receive {{$labels.job}} has a 99th percentile latency of
          {{ $value }} seconds for requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive has high HTTP requests latency.
      expr: |
        (
          histogram_quantile(0.99, sum by (cluster, job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-receiver-receive.*", handler="receive"}[5m]))) > 10
        and
          sum by (cluster, job) (rate(http_request_duration_seconds_count{job=~".*thanos-receiver-receive.*", handler="receive"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: critical
    - alert: ThanosReceiveHighReplicationFailures
      annotations:
        description: Thanos Receive {{$labels.job}} is failing to replicate {{$value | humanize}}% of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive is having high number of replication failures.
      expr: |
        thanos_receive_replication_factor > 1
          and
        (
          (
            sum by (cluster, job) (rate(thanos_receive_replications_total{result="error", job=~".*thanos-receiver-receive.*"}[5m]))
          /
            sum by (cluster, job) (rate(thanos_receive_replications_total{job=~".*thanos-receiver-receive.*"}[5m]))
          )
          >
          (
            max by (cluster, job) (floor((thanos_receive_replication_factor{job=~".*thanos-receiver-receive.*"}+1) / 2))
          /
            max by (cluster, job) (thanos_receive_hashring_nodes{job=~".*thanos-receiver-receive.*"})
          )
        ) * 100
      for: 5m
      labels:
        severity: warning
    - alert: ThanosReceiveHighForwardRequestFailures
      annotations:
        description: Thanos Receive {{$labels.job}} is failing to forward {{$value | humanize}}% of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive is failing to forward requests.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_receive_forward_requests_total{result="error", job=~".*thanos-receiver-receive.*"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_receive_forward_requests_total{job=~".*thanos-receiver-receive.*"}[5m]))
        ) * 100 > 20
      for: 5m
      labels:
        severity: info
    - alert: ThanosReceiveHighHashringFileRefreshFailures
      annotations:
        description: Thanos Receive {{$labels.job}} is failing to refresh hashring file,
          {{$value | humanize}} of attempts failed.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive is failing to refresh hasring file.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_receive_hashrings_file_errors_total{job=~".*thanos-receiver-receive.*"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_receive_hashrings_file_refreshes_total{job=~".*thanos-receiver-receive.*"}[5m]))
        > 0
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosReceiveConfigReloadFailure
      annotations:
        description: Thanos Receive {{$labels.job}} has not been able to reload hashring
          configurations.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive has not been able to reload configuration.
      expr: avg by (cluster, job) (thanos_receive_config_last_reload_successful{job=~".*thanos-receiver-receive.*"})
        != 1
      for: 5m
      labels:
        severity: warning
    - alert: ThanosReceiveNoUpload
      annotations:
        description: Thanos Receive {{$labels.instance}} has not uploaded latest data
          to object storage.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive has not uploaded latest data to object storage.
      expr: |
        (up{job=~".*thanos-receiver-receive.*"} - 1)
        + on (cluster, job, instance) # filters to only alert on current instance last 3h
        (sum by (cluster, job, instance) (increase(thanos_shipper_uploads_total{job=~".*thanos-receiver-receive.*"}[3h])) == 0)
      for: 3h
      labels:
        severity: critical
    - alert: ThanosReceiveTrafficBelowThreshold
      annotations:
        description: At Thanos Receive {{$labels.job}} in {{$labels.namespace}} , the
          average 1-hr avg. metrics ingestion rate  is {{$value | humanize}}% of 12-hr
          avg. ingestion rate.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-receive
        summary: Thanos Receive is experiencing low avg. 1-hr ingestion rate relative
          to avg. 12-hr ingestion rate.
      expr: |
        (
          avg_over_time(rate(http_requests_total{job=~".*thanos-receiver-receive.*", code=~"2..", handler="receive"}[5m])[1h:5m])
        /
          avg_over_time(rate(http_requests_total{job=~".*thanos-receiver-receive.*", code=~"2..", handler="receive"}[5m])[12h:5m])
        ) * 100 < 50
      for: 1h
      labels:
        severity: warning
  - name: thanos-store
    rules:
    - alert: ThanosStoreGrpcErrorRate
      annotations:
        description: Thanos Store {{$labels.job}} is failing to handle {{$value | humanize}}%
          of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-store
        summary: Thanos Store is failing to handle qrpcd requests.
      expr: |
        (
          sum by (cluster, job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-receiver-store.*"}[5m]))
        /
          sum by (cluster, job) (rate(grpc_server_started_total{job=~".*thanos-receiver-store.*"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
    - alert: ThanosStoreSeriesGateLatencyHigh
      annotations:
        description: Thanos Store {{$labels.job}} has a 99th percentile latency of {{$value}}
          seconds for store series gate requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-store
        summary: Thanos Store has high latency for store series gate requests.
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(thanos_bucket_store_series_gate_duration_seconds_bucket{job=~".*thanos-receiver-store.*"}[5m]))) > 2
        and
          sum by (cluster, job) (rate(thanos_bucket_store_series_gate_duration_seconds_count{job=~".*thanos-receiver-store.*"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: warning
    - alert: ThanosStoreBucketHighOperationFailures
      annotations:
        description: Thanos Store {{$labels.job}} Bucket is failing to execute {{$value | humanize}}% of operations.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-store
        summary: Thanos Store Bucket is failing to execute operations.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-receiver-store.*"}[5m]))
        /
          sum by (cluster, job) (rate(thanos_objstore_bucket_operations_total{job=~".*thanos-receiver-store.*"}[5m]))
        * 100 > 5
        )
      for: 15m
      labels:
        severity: warning
    - alert: ThanosStoreObjstoreOperationLatencyHigh
      annotations:
        description: Thanos Store {{$labels.job}} Bucket has a 99th percentile latency
          of {{$value}} seconds for the bucket operations.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-store
        summary: Thanos Store is having high latency for bucket operations.
      expr: |
        (
          histogram_quantile(0.99, sum by (job, le) (rate(thanos_objstore_bucket_operation_duration_seconds_bucket{job=~".*thanos-receiver-store.*"}[5m]))) > 10
        and
          sum by (cluster, job) (rate(thanos_objstore_bucket_operation_duration_seconds_count{job=~".*thanos-receiver-store.*"}[5m])) > 0
        )
      for: 10m
      labels:
        severity: warning
  - name: thanos-bucket-replicate
    rules:
    - alert: ThanosBucketReplicateErrorRate
      annotations:
        description: Thanos Replicate is failing to run, {{$value | humanize}}% of attempts
          failed.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-bucket-replicate
        summary: Thanos Replicate is failing to run.
      expr: |
        (
          sum by (cluster, job) (rate(thanos_replicate_replication_runs_total{result="error", job=~".*thanos-receiver-bucket-replicate.*"}[5m]))
        / on (cluster, job) group_left
          sum by (cluster, job) (rate(thanos_replicate_replication_runs_total{job=~".*thanos-receiver-bucket-replicate.*"}[5m]))
        ) * 100 >= 10
      for: 5m
      labels:
        severity: critical
    - alert: ThanosBucketReplicateRunLatency
      annotations:
        description: Thanos Replicate {{$labels.job}} has a 99th percentile latency
          of {{$value}} seconds for the replicate operations.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-bucket-replicate
        summary: Thanos Replicate has a high latency for replicate operations.
      expr: |
        (
          histogram_quantile(0.99, sum by (cluster, job) (rate(thanos_replicate_replication_run_duration_seconds_bucket{job=~".*thanos-receiver-bucket-replicate.*"}[5m]))) > 20
        and
          sum by (cluster, job) (rate(thanos_replicate_replication_run_duration_seconds_bucket{job=~".*thanos-receiver-bucket-replicate.*"}[5m])) > 0
        )
      for: 5m
      labels:
        severity: critical
  - name: thanos-component-absent
    rules:
    - alert: ThanosCompactIsDown
      annotations:
        description: ThanosCompact has disappeared. Prometheus target for the component
          cannot be discovered.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-component-absent
        summary: Thanos component has disappeared.
      expr: |
        absent(up{job=~".*thanos-receiver-compact.*"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ThanosQueryIsDown
      annotations:
        description: ThanosQuery has disappeared. Prometheus target for the component
          cannot be discovered.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-component-absent
        summary: Thanos component has disappeared.
      expr: |
        absent(up{job=~".*thanos-query-query"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ThanosReceiveIsDown
      annotations:
        description: ThanosReceive has disappeared. Prometheus target for the component
          cannot be discovered.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-component-absent
        summary: Thanos component has disappeared.
      expr: |
        absent(up{job=~".*thanos-receiver-receive.*"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ThanosStoreIsDown
      annotations:
        description: ThanosStore has disappeared. Prometheus target for the component
          cannot be discovered.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#thanos-component-absent
        summary: Thanos component has disappeared.
      expr: |
        absent(up{job=~".*thanos-receiver-store.*"} == 1)
      for: 5m
      labels:
        severity: critical
---
# Source: prometheus-alerts/templates/alerts/webhooks.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-webhooks
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_thanos: "1"
spec:
  groups:
    - name: webhook-failures
      rules:
      - alert: WebhookFailing
        annotations:
          description: Webhooks have been failing during the last 10m
          summary: Webhooks requests are failing
          runbook_url: Missing runbook
        expr: rate(apiserver_admission_webhook_rejection_count{error_type!="no_error"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
---
# Source: prometheus-alerts/templates/records/k8s.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-k8s.rules
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  groups:
  - name: k8s.rules
    rules:
    - expr: |-
        sum by (cluster, namespace, pod, container) (
          irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
        ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
          1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
    - expr: |-
        container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_working_set_bytes
    - expr: |-
        container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_rss
    - expr: |-
        container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_cache
    - expr: |-
        container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_swap
    - expr: |-
        kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (cluster, namespace, pod)
        group_left() max by (cluster, namespace, pod) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
    - expr: |-
        sum by (cluster, namespace) (
            sum by (cluster, namespace, pod) (
                max by (cluster, namespace, pod, container) (
                  kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                ) * on(cluster, namespace, pod) group_left() max by (cluster, namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_requests:sum
    - expr: |-
        kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (cluster, namespace, pod)
        group_left() max by (cluster, namespace, pod) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
    - expr: |-
        sum by (cluster, namespace) (
            sum by (cluster, namespace, pod) (
                max by (cluster, namespace, pod, container) (
                  kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                ) * on(cluster, namespace, pod) group_left() max by (cluster, namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_requests:sum
    - expr: |-
        kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (cluster, namespace, pod)
        group_left() max by (cluster, namespace, pod) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
    - expr: |-
        sum by (cluster, namespace) (
            sum by (cluster, namespace, pod) (
                max by (cluster, namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                ) * on(cluster, namespace, pod) group_left() max by (cluster, namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_limits:sum
    - expr: |-
        kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (cluster, namespace, pod)
        group_left() max by (cluster, namespace, pod) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
    - expr: |-
        sum by (cluster, namespace) (
            sum by (cluster, namespace, pod) (
                max by (cluster, namespace, pod, container) (
                  kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                ) * on(cluster, namespace, pod) group_left() max by (cluster, namespace, pod) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_limits:sum
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(cluster, namespace, replicaset) group_left(owner_name) topk by(cluster, namespace, replicaset) (
              1, max by (cluster, namespace, replicaset, owner_name) (
                kube_replicaset_owner{job="kube-state-metrics"}
              )
            ),
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: deployment
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: daemonset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: statefulset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |-
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: job
      record: namespace_workload_pod:kube_pod_owner:relabel
---
# Source: prometheus-alerts/templates/records/kubelet.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-kubelet.rules
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  groups:
  - name: kubelet.rules
    rules:
    - expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.99'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.9'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: '0.5'
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
---
# Source: prometheus-alerts/templates/records/node-exporter.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-node-exporter.rules
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  groups:
  - name: node-exporter.rules
    rules:
    - expr: |-
        count without (cpu, mode, pod) (
          node_cpu_seconds_total{job="node-exporter",mode="idle"}
        )
      record: instance:node_num_cpu:sum
    - expr: |-
        1 - avg without (cpu) (
          sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[5m]))
        )
      record: instance:node_cpu_utilisation:rate5m
    - expr: |-
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |-
        sum by (instance) (
          1 - (
            (
              node_memory_MemAvailable_bytes{job="node-exporter"}
              or
              (
                node_memory_Buffers_bytes{job="node-exporter"}
                +
                node_memory_Cached_bytes{job="node-exporter"}
                +
                node_memory_MemFree_bytes{job="node-exporter"}
                +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            )
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        )
      record: instance:node_memory_utilisation:ratio
    - expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[5m])
      record: instance:node_vmstat_pgmajfault:rate5m
    - expr: rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      record: instance_device:node_disk_io_time_seconds:rate5m
    - expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate5m
    - expr: |-
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[5m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate5m
    - expr: |-
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[5m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate5m
    - expr: |-
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[5m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate5m
    - expr: |-
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[5m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate5m
---
# Source: prometheus-alerts/templates/records/node.rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-node.rules
  labels:
    app: prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  groups:
  - name: node.rules
    rules:
    - expr: |-
        topk by(cluster, namespace, pod) (1,
          max by (cluster, node, namespace, pod) (
            label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
        ))
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |-
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
    - expr: |-
        avg by (cluster, node) (
          sum without (mode) (
            rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
          )
        )
      record: node:node_cpu_utilization:ratio_rate5m
    - expr: |-
        avg by (cluster) (
          node:node_cpu_utilization:ratio_rate5m
        )
      record: cluster:node_cpu:ratio_rate5m
---
# Source: prometheus-alerts/templates/records/opensearch.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sc-alerts-prometheus-alerts-opensearch-records
  labels:
    app: sc-alerts-prometheus-alerts
    app.kubernetes.io/name: prometheus-alerts
    helm.sh/chart: prometheus-alerts-0.1.1
    app.kubernetes.io/instance: sc-alerts
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
    evaluate_prometheus: "1"
spec:
  groups:
  - name: opensearch-records
    rules:
    - record: elasticsearch_filesystem_data_used_percent
      expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes)
        / elasticsearch_filesystem_data_size_bytes

